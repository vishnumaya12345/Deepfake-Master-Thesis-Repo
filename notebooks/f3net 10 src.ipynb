{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO6lkQHLHNxS17G/Nxt0GcF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"dd94a18967ed452c9dc76ef4a970a3a4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_708ab317a50f4773813b6d1ad4aa3944","IPY_MODEL_04c4cb2211e145d29e114dccbe69b065","IPY_MODEL_5ebe75f879d6466b870ae13e0914a356"],"layout":"IPY_MODEL_af42a35e0af746f998d4d1865b7a8221"}},"708ab317a50f4773813b6d1ad4aa3944":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aa705227b3e149df80452bd230f332d3","placeholder":"​","style":"IPY_MODEL_55619026b23143e6ad08355dac66e3d6","value":"model.safetensors: 100%"}},"04c4cb2211e145d29e114dccbe69b065":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b2f0c7486aa84d71955e96522fb64caf","max":108392156,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ff5d94002da54c85bb9cbcf2056f2946","value":108392156}},"5ebe75f879d6466b870ae13e0914a356":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_92a38a2d9dd14af99adebfba70f27b95","placeholder":"​","style":"IPY_MODEL_c79cdc7fe03d4459871007ec9c9b97df","value":" 108M/108M [00:01&lt;00:00, 101MB/s]"}},"af42a35e0af746f998d4d1865b7a8221":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aa705227b3e149df80452bd230f332d3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"55619026b23143e6ad08355dac66e3d6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b2f0c7486aa84d71955e96522fb64caf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ff5d94002da54c85bb9cbcf2056f2946":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"92a38a2d9dd14af99adebfba70f27b95":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c79cdc7fe03d4459871007ec9c9b97df":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["# === F3Net on frames_cropped_faces_10src — print ONLY AUC, EER, AP ===\n","import os, re, glob, io, contextlib, warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=False)\n","\n","import numpy as np\n","import torch, torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from PIL import Image\n","from sklearn.metrics import roc_auc_score, average_precision_score, roc_curve\n","import timm\n","\n","# --- Paths ---\n","DRIVE_ROOT = \"/content/drive/MyDrive\" if os.path.exists(\"/content/drive/MyDrive\") else \"/content/drive/My Drive\"\n","DATA_ROOT  = os.path.join(DRIVE_ROOT, \"frames_cropped_faces_10src\")   # {real,fake}\n","WEIGHT_PATH= os.path.join(DRIVE_ROOT, \"DeepfakeBench_weights\", \"f3net_best.pth\")\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","IMG_SIZE = 299\n","IMAGENET_MEAN = torch.tensor([0.485, 0.456, 0.406]).view(1,3,1,1)\n","IMAGENET_STD  = torch.tensor([0.229, 0.224, 0.225]).view(1,3,1,1)\n","\n","def pil_to_tensor(img: Image.Image, size=IMG_SIZE):\n","    if img.mode != \"RGB\":\n","        img = img.convert(\"RGB\")\n","    if img.size != (size, size):\n","        img = img.resize((size, size), Image.BILINEAR)\n","    arr = np.asarray(img, dtype=np.float32) / 255.0   # HWC\n","    arr = np.transpose(arr, (2,0,1))                  # CHW\n","    t = torch.from_numpy(arr).unsqueeze(0)            # 1x3xHxW\n","    t = (t - IMAGENET_MEAN) / IMAGENET_STD\n","    return t.squeeze(0)  # 3xHxW\n","\n","# ---------- F3Net’s FAD head ----------\n","def dct_matrix(size: int) -> torch.Tensor:\n","    i = torch.arange(size, dtype=torch.float32)\n","    j = torch.arange(size, dtype=torch.float32)\n","    jj, ii = torch.meshgrid(j, i, indexing='xy')\n","    mat = torch.cos((jj + 0.5) * torch.pi * ii / size)\n","    mat[0, :] = mat[0, :] * (1.0 / torch.sqrt(torch.tensor(size, dtype=torch.float32)))\n","    mat[1:, :] = mat[1:, :] * torch.sqrt(2.0 / torch.tensor(size, dtype=torch.float32))\n","    return mat.t()\n","\n","def make_filter_mask(size, start, end):\n","    i = np.arange(size)\n","    j = np.arange(size)\n","    ii, jj = np.meshgrid(i, j, indexing='ij')\n","    s = ii + jj\n","    return ((s >= start) & (s <= end)).astype(np.float32)\n","\n","class LearnableFilter(nn.Module):\n","    def __init__(self, size, band_start, band_end, learnable=True, normalize=False):\n","        super().__init__()\n","        self.base = nn.Parameter(torch.tensor(make_filter_mask(size, band_start, band_end)), requires_grad=False)\n","        self.learn = nn.Parameter(torch.randn(size, size) * 0.1, requires_grad=learnable)\n","        self.normalize = normalize\n","        if normalize:\n","            self.ft_num = nn.Parameter(torch.tensor(float(self.base.sum())), requires_grad=False)\n","    def forward(self, X):\n","        filt = self.base.to(X.device)\n","        if self.learn.requires_grad:\n","            filt = filt + (2.0 * torch.sigmoid(self.learn.to(X.device)) - 1.0)\n","        return X * (filt / self.ft_num if self.normalize else filt)\n","\n","class FADHead(nn.Module):\n","    def __init__(self, size=IMG_SIZE):\n","        super().__init__()\n","        D = dct_matrix(size)\n","        self.D = nn.Parameter(D, requires_grad=False)\n","        self.DT = nn.Parameter(D.t(), requires_grad=False)\n","        low   = LearnableFilter(size, 0, int(size // 2.82))\n","        mid   = LearnableFilter(size, int(size // 2.82), size // 2)\n","        high  = LearnableFilter(size, size // 2, size * 2)\n","        allf  = LearnableFilter(size, 0, size * 2)\n","        self.filters = nn.ModuleList([low, mid, high, allf])\n","    def _dct2(self, x):   # (B,C,H,W)\n","        D, DT = self.D.to(x.device), self.DT.to(x.device)\n","        xh = torch.einsum('ih, b c h w -> b c i w', D, x)\n","        xw = torch.einsum('jw, b c i w -> b c i j', D, xh)\n","        return xw\n","    def _idct2(self, X):\n","        D, DT = self.D.to(X.device), self.DT.to(X.device)\n","        xw = torch.einsum('wj, b c i j -> b c i w', DT, X)\n","        xh = torch.einsum('hi, b c i w -> b c h w', DT, xw)\n","        return xh\n","    def forward(self, x):  # 3xHxW\n","        x = x.unsqueeze(0)\n","        X = self._dct2(x)\n","        outs = []\n","        for f in self.filters:\n","            Xp = f(X)\n","            yp = self._idct2(Xp)\n","            outs.append(yp)\n","        y = torch.cat(outs, dim=1)  # 1x12xHxW\n","        return y.squeeze(0)\n","\n","class F3NetModel(nn.Module):\n","    def __init__(self, img_size=IMG_SIZE, num_classes=2):\n","        super().__init__()\n","        self.fad = FADHead(img_size)\n","        with contextlib.redirect_stdout(io.StringIO()), contextlib.redirect_stderr(io.StringIO()):\n","            self.backbone = timm.create_model(\"xception41\", pretrained=True, num_classes=num_classes, in_chans=12)\n","        self.softmax = nn.Softmax(dim=1)\n","    def forward(self, x3):              # x3: (B,3,H,W)\n","        fad_feats = torch.stack([self.fad(x3[i]) for i in range(x3.size(0))], dim=0)  # (B,12,H,W)\n","        logits = self.backbone(fad_feats)  # (B,2)\n","        return logits\n","\n","def try_load_weights(model, path):\n","    if not os.path.isfile(path):\n","        return False\n","    try:\n","        sd = torch.load(path, map_location=\"cpu\")\n","        if isinstance(sd, dict) and \"state_dict\" in sd: sd = sd[\"state_dict\"]\n","        new_sd = {}\n","        for k,v in (sd.items() if isinstance(sd, dict) else []):\n","            nk = k\n","            for pref in (\"module.\",\"model.\",\"net.\",\"backbone.\"):\n","                if nk.startswith(pref): nk = nk[len(pref):]\n","            new_sd[nk] = v\n","        model.load_state_dict(new_sd, strict=False)\n","        return True\n","    except Exception:\n","        return False\n","\n","# ---------- Data (no torchvision) ----------\n","FRAME_KEY_RE = re.compile(r\"^(.*?)(?:[_-]frames?[_-]?\\d+|[_-]frame[_-]?\\d+)$\", re.IGNORECASE)\n","def get_video_key(basename):\n","    base = os.path.splitext(basename)[0]\n","    m = FRAME_KEY_RE.match(base)\n","    return m.group(1) if m else base.split(\"_\")[0]\n","\n","class FramesDataset(Dataset):\n","    def __init__(self, root):\n","        exts = {\".jpg\",\".jpeg\",\".png\",\".bmp\",\".webp\",\".tif\",\".tiff\",\".JPG\",\".JPEG\",\".PNG\"}\n","        self.samples=[]\n","        for cls,y in ((\"real\",0),(\"fake\",1)):\n","            d = os.path.join(root, cls)\n","            if not os.path.isdir(d): continue\n","            for p in glob.glob(os.path.join(d, \"*\")):\n","                if os.path.splitext(p)[1] in exts:\n","                    self.samples.append((p, y, get_video_key(os.path.basename(p))))\n","        self.samples.sort(key=lambda x:(x[1], x[2], x[0]))\n","    def __len__(self): return len(self.samples)\n","    def __getitem__(self, i):\n","        p,y,v = self.samples[i]\n","        with Image.open(p) as im:\n","            x = pil_to_tensor(im, IMG_SIZE)  # 3xHxW\n","        return x, y, v\n","\n","def aggregate_by_video(vkeys, probs, labels, how=\"median\"):\n","    vids={}\n","    for v,p,y in zip(vkeys, probs, labels):\n","        if v not in vids: vids[v]={\"p\":[], \"y\":y}\n","        vids[v][\"p\"].append(float(p))\n","    P=[]; Y=[]\n","    for v in vids:\n","        arr = np.array(vids[v][\"p\"], dtype=np.float32)\n","        P.append(float(np.median(arr)) if how==\"median\" else float(np.mean(arr)))\n","        Y.append(int(vids[v][\"y\"]))\n","    return np.array(P, dtype=np.float32), np.array(Y, dtype=np.int64)\n","\n","def metrics_auc_eer_ap(y_true, y_score):\n","    auc = roc_auc_score(y_true, y_score)\n","    ap  = average_precision_score(y_true, y_score)\n","    fpr, tpr, thr = roc_curve(y_true, y_score)\n","    fnr = 1 - tpr\n","    idx = int(np.nanargmin(np.abs(fpr - fnr)))\n","    eer = float((fpr[idx] + fnr[idx]) / 2.0)\n","    return float(auc), float(eer), float(ap)\n","\n","# --------------------- Run ---------------------\n","ds = FramesDataset(DATA_ROOT)\n","if len(ds)==0:\n","    raise RuntimeError(f\"No images under {DATA_ROOT}/{{real,fake}}\")\n","\n","loader = DataLoader(ds, batch_size=16, shuffle=False, num_workers=2, pin_memory=True)\n","\n","model = F3NetModel(img_size=IMG_SIZE).to(device).eval()\n","_ = try_load_weights(model, WEIGHT_PATH)\n","softmax = nn.Softmax(dim=1)\n","\n","all_probs, all_labels, all_vkeys = [], [], []\n","with torch.no_grad():\n","    for xb, yb, vks in loader:\n","        xb = xb.to(device, dtype=torch.float32, non_blocking=True)\n","        logits = model(xb)\n","        probs = softmax(logits)[:,1].detach().cpu().numpy()\n","        all_probs.extend(probs.tolist())\n","        all_labels.extend(yb.numpy().tolist())\n","        all_vkeys.extend(list(vks))\n","\n","all_probs  = np.asarray(all_probs, dtype=np.float32)\n","all_labels = np.asarray(all_labels, dtype=np.int64)\n","all_vkeys  = np.asarray(all_vkeys)\n","\n","vp, vy = aggregate_by_video(all_vkeys, all_probs, all_labels, \"median\")\n","auc1, eer1, ap1 = metrics_auc_eer_ap(vy, vp)\n","auc2, eer2, ap2 = metrics_auc_eer_ap(vy, 1.0 - vp)\n","auc, eer, ap = (auc2, eer2, ap2) if auc2 > auc1 else (auc1, eer1, ap1)\n","\n","print(f\"AUC: {auc:.4f}\")\n","print(f\"EER: {eer:.4f}\")\n","print(f\"AP : {ap:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":118,"referenced_widgets":["dd94a18967ed452c9dc76ef4a970a3a4","708ab317a50f4773813b6d1ad4aa3944","04c4cb2211e145d29e114dccbe69b065","5ebe75f879d6466b870ae13e0914a356","af42a35e0af746f998d4d1865b7a8221","aa705227b3e149df80452bd230f332d3","55619026b23143e6ad08355dac66e3d6","b2f0c7486aa84d71955e96522fb64caf","ff5d94002da54c85bb9cbcf2056f2946","92a38a2d9dd14af99adebfba70f27b95","c79cdc7fe03d4459871007ec9c9b97df"]},"id":"6HY-e6IpjZW7","executionInfo":{"status":"ok","timestamp":1761063592904,"user_tz":-120,"elapsed":272426,"user":{"displayName":"Vishnumaya","userId":"01919615312035119785"}},"outputId":"79653f5c-cbbe-4cd8-fa8c-69b05ff9dc67"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/108M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd94a18967ed452c9dc76ef4a970a3a4"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["AUC: 0.5280\n","EER: 0.4900\n","AP : 0.5843\n"]}]},{"cell_type":"code","source":["# =============== F3Net LARGE TABLE (frames_cropped_faces_10src) =================\n","# Columns:\n","# dataset, detector, video_name, true_label, n_frames, n_correct_frames, n_wrong_frames,\n","# frame_accuracy, avg_prob_fake, std_prob_fake, video_pred_by_avg, video_correct_by_avg,\n","# video_pred_by_majority, video_correct_by_majority\n","\n","import os, re, glob, io, contextlib, warnings, sys, subprocess, math\n","warnings.filterwarnings(\"ignore\")\n","\n","# Mount Drive\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=False)\n","\n","# --- Imports (no torchvision) ---\n","import numpy as np\n","import pandas as pd\n","import torch, torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from PIL import Image\n","from sklearn.metrics import roc_auc_score, average_precision_score, roc_curve\n","import timm\n","\n","# ---- Paths / names ----\n","DRIVE_ROOT   = \"/content/drive/MyDrive\" if os.path.exists(\"/content/drive/MyDrive\") else \"/content/drive/My Drive\"\n","DATASET      = \"frames_cropped_faces_10src\"\n","DATA_ROOT    = os.path.join(DRIVE_ROOT, DATASET)            # {real,fake}\n","WEIGHT_PATH  = os.path.join(DRIVE_ROOT, \"DeepfakeBench_weights\", \"f3net_best.pth\")\n","DETECTOR     = \"F3Net\"\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","IMG_SIZE = 299\n","IMAGENET_MEAN = torch.tensor([0.485, 0.456, 0.406]).view(1,3,1,1)\n","IMAGENET_STD  = torch.tensor([0.229, 0.224, 0.225]).view(1,3,1,1)\n","\n","# ---- Tiny image pipeline (no torchvision) ----\n","def pil_to_tensor(img: Image.Image, size=IMG_SIZE):\n","    if img.mode != \"RGB\":\n","        img = img.convert(\"RGB\")\n","    if img.size != (size, size):\n","        img = img.resize((size, size), Image.BILINEAR)\n","    arr = np.asarray(img, dtype=np.float32) / 255.0   # HWC -> [0,1]\n","    arr = np.transpose(arr, (2,0,1))                  # CHW\n","    t = torch.from_numpy(arr).unsqueeze(0)            # 1x3xHxW\n","    t = (t - IMAGENET_MEAN) / IMAGENET_STD\n","    return t.squeeze(0)  # 3xHxW\n","\n","# ====================== F3Net’s FAD head (DCT + 4 bands) ======================\n","def dct_matrix(size: int) -> torch.Tensor:\n","    i = torch.arange(size, dtype=torch.float32)\n","    j = torch.arange(size, dtype=torch.float32)\n","    jj, ii = torch.meshgrid(j, i, indexing='xy')\n","    mat = torch.cos((jj + 0.5) * torch.pi * ii / size)\n","    mat[0, :]  *= (1.0 / torch.sqrt(torch.tensor(size, dtype=torch.float32)))\n","    mat[1:, :] *=  torch.sqrt(2.0 / torch.tensor(size, dtype=torch.float32))\n","    return mat.t()\n","\n","def make_filter_mask(size, start, end):\n","    i = np.arange(size); j = np.arange(size)\n","    ii, jj = np.meshgrid(i, j, indexing='ij')\n","    s = ii + jj\n","    return ((s >= start) & (s <= end)).astype(np.float32)\n","\n","class LearnableFilter(nn.Module):\n","    def __init__(self, size, band_start, band_end, learnable=True, normalize=False):\n","        super().__init__()\n","        self.base = nn.Parameter(torch.tensor(make_filter_mask(size, band_start, band_end)), requires_grad=False)\n","        self.learn = nn.Parameter(torch.randn(size, size) * 0.1, requires_grad=learnable)\n","        self.normalize = normalize\n","        if normalize:\n","            self.ft_num = nn.Parameter(torch.tensor(float(self.base.sum())), requires_grad=False)\n","    def forward(self, X):\n","        filt = self.base.to(X.device)\n","        if self.learn.requires_grad:\n","            filt = filt + (2.0 * torch.sigmoid(self.learn.to(X.device)) - 1.0)\n","        return X * (filt / self.ft_num if self.normalize else filt)\n","\n","class FADHead(nn.Module):\n","    def __init__(self, size=IMG_SIZE):\n","        super().__init__()\n","        D = dct_matrix(size)\n","        self.D = nn.Parameter(D, requires_grad=False)\n","        self.DT = nn.Parameter(D.t(), requires_grad=False)\n","        low   = LearnableFilter(size, 0, int(size // 2.82))\n","        mid   = LearnableFilter(size, int(size // 2.82), size // 2)\n","        high  = LearnableFilter(size, size // 2, size * 2)\n","        allf  = LearnableFilter(size, 0, size * 2)\n","        self.filters = nn.ModuleList([low, mid, high, allf])\n","    def _dct2(self, x):   # (B,C,H,W)\n","        D, DT = self.D.to(x.device), self.DT.to(x.device)\n","        xh = torch.einsum('ih, b c h w -> b c i w', D, x)\n","        xw = torch.einsum('jw, b c i w -> b c i j', D, xh)\n","        return xw\n","    def _idct2(self, X):\n","        D, DT = self.D.to(X.device), self.DT.to(X.device)\n","        xw = torch.einsum('wj, b c i j -> b c i w', DT, X)\n","        xh = torch.einsum('hi, b c i w -> b c h w', DT, xw)\n","        return xh\n","    def forward(self, x):  # 3xHxW\n","        x = x.unsqueeze(0)\n","        X = self._dct2(x)\n","        outs = []\n","        for f in self.filters:\n","            Xp = f(X)\n","            yp = self._idct2(Xp)\n","            outs.append(yp)\n","        y = torch.cat(outs, dim=1)  # 1x12xHxW\n","        return y.squeeze(0)\n","\n","# ====================== Backbone (timm xception, 12 in-channels) ======================\n","class F3NetModel(nn.Module):\n","    def __init__(self, img_size=IMG_SIZE, num_classes=2):\n","        super().__init__()\n","        self.fad = FADHead(img_size)\n","        with contextlib.redirect_stdout(io.StringIO()), contextlib.redirect_stderr(io.StringIO()):\n","            self.backbone = timm.create_model(\"xception41\", pretrained=True, num_classes=num_classes, in_chans=12)\n","        self.softmax = nn.Softmax(dim=1)\n","    def forward(self, x3):              # x3: (B,3,H,W)\n","        fad_feats = torch.stack([self.fad(x3[i]) for i in range(x3.size(0))], dim=0)  # (B,12,H,W)\n","        logits = self.backbone(fad_feats)  # (B,2)\n","        return logits\n","\n","def try_load_weights(model, path):\n","    if not os.path.isfile(path): return False\n","    try:\n","        sd = torch.load(path, map_location=\"cpu\")\n","        if isinstance(sd, dict) and \"state_dict\" in sd: sd = sd[\"state_dict\"]\n","        new_sd = {}\n","        for k,v in (sd.items() if isinstance(sd, dict) else []):\n","            nk=k\n","            for pref in (\"module.\",\"model.\",\"net.\",\"backbone.\"):\n","                if nk.startswith(pref): nk = nk[len(pref):]\n","            new_sd[nk]=v\n","        model.load_state_dict(new_sd, strict=False)\n","        return True\n","    except Exception:\n","        return False\n","\n","# ====================== Data (group frames by video key) ======================\n","FRAME_KEY_RE = re.compile(r\"^(.*?)(?:[_-]frames?[_-]?\\d+|[_-]frame[_-]?\\d+)$\", re.IGNORECASE)\n","def get_video_key(basename):\n","    base = os.path.splitext(basename)[0]\n","    m = FRAME_KEY_RE.match(base)\n","    return m.group(1) if m else base.split(\"_\")[0]\n","\n","class FramesDataset(Dataset):\n","    def __init__(self, root):\n","        exts = {\".jpg\",\".jpeg\",\".png\",\".bmp\",\".webp\",\".tif\",\".tiff\",\".JPG\",\".JPEG\",\".PNG\"}\n","        self.samples=[]\n","        for cls,y in ((\"real\",0),(\"fake\",1)):\n","            d = os.path.join(root, cls)\n","            if not os.path.isdir(d): continue\n","            for p in glob.glob(os.path.join(d, \"*\")):\n","                if os.path.splitext(p)[1] in exts:\n","                    self.samples.append((p, y, get_video_key(os.path.basename(p))))\n","        self.samples.sort(key=lambda x:(x[1], x[2], x[0]))\n","    def __len__(self): return len(self.samples)\n","    def __getitem__(self, i):\n","        p,y,v = self.samples[i]\n","        with Image.open(p) as im:\n","            x = pil_to_tensor(im, IMG_SIZE)  # 3xHxW\n","        return x, y, v\n","\n","# ====================== Metrics & thresholds ======================\n","def agg_by_video(vkeys, probs, labels, fn=\"median\"):\n","    vids={}\n","    for v,p,y in zip(vkeys, probs, labels):\n","        if v not in vids: vids[v]={\"p\":[], \"y\":y}\n","        vids[v][\"p\"].append(float(p))\n","    names = sorted(vids.keys())\n","    P = np.array([np.median(vids[n][\"p\"]) if fn==\"median\" else np.mean(vids[n][\"p\"]) for n in names], dtype=np.float32)\n","    Y = np.array([vids[n][\"y\"] for n in names], dtype=np.int64)\n","    return names, P, Y\n","\n","def youden_threshold(y_true, y_score):\n","    fpr, tpr, thr = roc_curve(y_true, y_score)\n","    j = tpr - fpr\n","    return float(thr[np.nanargmax(j)])\n","\n","def lab2str(y): return \"real\" if int(y)==0 else \"fake\"\n","\n","# ====================== Run inference ======================\n","ds = FramesDataset(DATA_ROOT)\n","if len(ds)==0:\n","    raise RuntimeError(f\"No images under {DATA_ROOT}/{{real,fake}}\")\n","\n","loader = DataLoader(ds, batch_size=16, shuffle=False, num_workers=2, pin_memory=True)\n","\n","model = F3NetModel(img_size=IMG_SIZE).to(device).eval()\n","_ = try_load_weights(model, WEIGHT_PATH)\n","softmax = nn.Softmax(dim=1)\n","\n","frame_probs, frame_labels, frame_vkeys = [], [], []\n","with torch.no_grad():\n","    for xb, yb, vks in loader:\n","        xb = xb.to(device, dtype=torch.float32, non_blocking=True)\n","        logits = model(xb)\n","        p = softmax(logits)[:,1].detach().cpu().numpy()\n","        frame_probs.extend(p.tolist())\n","        frame_labels.extend(yb.numpy().tolist())\n","        frame_vkeys.extend(list(vks))\n","\n","frame_probs  = np.asarray(frame_probs, dtype=np.float32)\n","frame_labels = np.asarray(frame_labels, dtype=np.int64)\n","frame_vkeys  = np.asarray(frame_vkeys)\n","\n","# Auto orientation flip (choose p or 1-p maximizing VIDEO-level AUC under MEDIAN aggregation)\n","_, P_med, Y_vid = agg_by_video(frame_vkeys, frame_probs, frame_labels, \"median\")\n","auc_p  = roc_auc_score(Y_vid, P_med)\n","auc_1p = roc_auc_score(Y_vid, 1.0 - P_med)\n","if auc_1p > auc_p:\n","    frame_probs = 1.0 - frame_probs\n","\n","# Thresholds:\n","thr_frame = youden_threshold(frame_labels, frame_probs)   # for per-frame + majority\n","names_avg, P_avg, Y_avg = agg_by_video(frame_vkeys, frame_probs, frame_labels, \"mean\")\n","thr_video_avg = youden_threshold(Y_avg, P_avg)            # for video_pred_by_avg\n","\n","# Build per-video rows\n","rows = []\n","video_dict = {}\n","for v,p,y in zip(frame_vkeys, frame_probs, frame_labels):\n","    if v not in video_dict: video_dict[v] = {\"probs\": [], \"label\": int(y)}\n","    video_dict[v][\"probs\"].append(float(p))\n","\n","for v in sorted(video_dict.keys()):\n","    probs = np.array(video_dict[v][\"probs\"], dtype=np.float32)\n","    y_int  = int(video_dict[v][\"label\"])\n","    y_str  = lab2str(y_int)\n","    n_frames = probs.size\n","\n","    yhat_frames = (probs >= thr_frame).astype(int)\n","    n_correct_frames = int((yhat_frames == y_int).sum())\n","    n_wrong_frames   = int(n_frames - n_correct_frames)\n","    frame_accuracy   = n_correct_frames / float(n_frames) if n_frames > 0 else 0.0\n","\n","    avg_prob_fake = float(np.mean(probs))\n","    std_prob_fake = float(np.std(probs))\n","\n","    pred_avg_int = int(avg_prob_fake >= thr_video_avg)\n","    pred_avg_str = lab2str(pred_avg_int)\n","    video_correct_by_avg = int(pred_avg_int == y_int)\n","\n","    pred_maj_int = int((yhat_frames.sum() >= math.ceil(n_frames/2)))\n","    pred_maj_str = lab2str(pred_maj_int)\n","    video_correct_by_majority = int(pred_maj_int == y_int)\n","\n","    rows.append({\n","        \"dataset\": DATASET,\n","        \"detector\": DETECTOR,\n","        \"video_name\": v,\n","        \"true_label\": y_str,\n","        \"n_frames\": n_frames,\n","        \"n_correct_frames\": n_correct_frames,\n","        \"n_wrong_frames\": n_wrong_frames,\n","        \"frame_accuracy\": round(frame_accuracy, 4),\n","        \"avg_prob_fake\": round(avg_prob_fake, 6),\n","        \"std_prob_fake\": round(std_prob_fake, 6),\n","        \"video_pred_by_avg\": pred_avg_str,\n","        \"video_correct_by_avg\": video_correct_by_avg,\n","        \"video_pred_by_majority\": pred_maj_str,\n","        \"video_correct_by_majority\": video_correct_by_majority,\n","    })\n","\n","df = pd.DataFrame(rows)\n","\n","# Print full table (no truncation / no column breaks)\n","pd.set_option(\"display.max_rows\", 500)\n","pd.set_option(\"display.max_columns\", None)\n","pd.set_option(\"display.width\", 0)\n","print(df.to_string(index=False))\n","# ===============================================================================\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f7fODIwFmgl6","executionInfo":{"status":"ok","timestamp":1761064164277,"user_tz":-120,"elapsed":32269,"user":{"displayName":"Vishnumaya","userId":"01919615312035119785"}},"outputId":"be966a99-2a8c-498a-e899-89448b09f2ca"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","                   dataset detector video_name true_label  n_frames  n_correct_frames  n_wrong_frames  frame_accuracy  avg_prob_fake  std_prob_fake video_pred_by_avg  video_correct_by_avg video_pred_by_majority  video_correct_by_majority\n","frames_cropped_faces_10src    F3Net       10_1       fake        20                 4              16            0.20       0.656579       0.022117              fake                     1                   real                          0\n","frames_cropped_faces_10src    F3Net      10_10       fake        20                12               8            0.60       0.680252       0.018214              fake                     1                   fake                          1\n","frames_cropped_faces_10src    F3Net      10_11       fake        20                 9              11            0.45       0.673700       0.016679              fake                     1                   real                          0\n","frames_cropped_faces_10src    F3Net      10_12       fake        20                12               8            0.60       0.689437       0.026400              fake                     1                   fake                          1\n","frames_cropped_faces_10src    F3Net      10_13       fake        20                 0              20            0.00       0.630682       0.022602              real                     0                   real                          0\n","frames_cropped_faces_10src    F3Net      10_14       fake        20                 4              16            0.20       0.657642       0.021044              fake                     1                   real                          0\n","frames_cropped_faces_10src    F3Net      10_15       fake        20                 1              19            0.05       0.644194       0.022697              real                     0                   real                          0\n","frames_cropped_faces_10src    F3Net      10_16       fake        20                11               9            0.55       0.682132       0.024719              fake                     1                   fake                          1\n","frames_cropped_faces_10src    F3Net      10_17       fake        20                20               0            1.00       0.756579       0.017637              fake                     1                   fake                          1\n","frames_cropped_faces_10src    F3Net      10_18       fake        20                 7              13            0.35       0.665815       0.037489              fake                     1                   real                          0\n","frames_cropped_faces_10src    F3Net      10_19       fake        20                20               0            1.00       0.767544       0.038156              fake                     1                   fake                          1\n","frames_cropped_faces_10src    F3Net       10_2       fake        20                18               2            0.90       0.728111       0.031047              fake                     1                   fake                          1\n","frames_cropped_faces_10src    F3Net      10_20       fake        20                 2              18            0.10       0.656314       0.023593              fake                     1                   real                          0\n","frames_cropped_faces_10src    F3Net      10_21       fake        20                 3              17            0.15       0.652665       0.023786              real                     0                   real                          0\n","frames_cropped_faces_10src    F3Net      10_22       fake        20                 2              18            0.10       0.628817       0.041081              real                     0                   real                          0\n","frames_cropped_faces_10src    F3Net      10_23       fake        20                20               0            1.00       0.715636       0.016139              fake                     1                   fake                          1\n","frames_cropped_faces_10src    F3Net      10_24       fake        20                 2              18            0.10       0.633880       0.031441              real                     0                   real                          0\n","frames_cropped_faces_10src    F3Net      10_25       fake        20                17               3            0.85       0.700420       0.019653              fake                     1                   fake                          1\n","frames_cropped_faces_10src    F3Net      10_26       fake        20                 7              13            0.35       0.672550       0.021063              fake                     1                   real                          0\n","frames_cropped_faces_10src    F3Net      10_27       fake        20                 5              15            0.25       0.670820       0.013556              fake                     1                   real                          0\n","frames_cropped_faces_10src    F3Net      10_28       fake        20                20               0            1.00       0.712255       0.014776              fake                     1                   fake                          1\n","frames_cropped_faces_10src    F3Net      10_29       fake        20                 5              15            0.25       0.654289       0.038544              real                     0                   real                          0\n","frames_cropped_faces_10src    F3Net       10_3       fake        20                 1              19            0.05       0.633238       0.024813              real                     0                   real                          0\n","frames_cropped_faces_10src    F3Net      10_30       fake        20                 4              16            0.20       0.659571       0.022070              fake                     1                   real                          0\n","frames_cropped_faces_10src    F3Net      10_31       fake        20                 2              18            0.10       0.654305       0.018237              fake                     1                   real                          0\n","frames_cropped_faces_10src    F3Net      10_32       fake        20                 4              16            0.20       0.655725       0.025725              fake                     1                   real                          0\n","frames_cropped_faces_10src    F3Net      10_33       fake        20                14               6            0.70       0.692577       0.023915              fake                     1                   fake                          1\n","frames_cropped_faces_10src    F3Net      10_34       fake        20                14               6            0.70       0.688585       0.019479              fake                     1                   fake                          1\n","frames_cropped_faces_10src    F3Net      10_35       fake        20                 8              12            0.40       0.666490       0.021896              fake                     1                   real                          0\n","frames_cropped_faces_10src    F3Net      10_36       fake        20                13               7            0.65       0.685708       0.032398              fake                     1                   fake                          1\n","frames_cropped_faces_10src    F3Net      10_37       fake        20                13               7            0.65       0.690703       0.028361              fake                     1                   fake                          1\n","frames_cropped_faces_10src    F3Net      10_38       fake        20                15               5            0.75       0.693394       0.025310              fake                     1                   fake                          1\n","frames_cropped_faces_10src    F3Net      10_39       fake        20                 4              16            0.20       0.657145       0.028346              fake                     1                   real                          0\n","frames_cropped_faces_10src    F3Net       10_4       fake        20                15               5            0.75       0.699539       0.061664              fake                     1                   fake                          1\n","frames_cropped_faces_10src    F3Net      10_40       fake        20                17               3            0.85       0.718716       0.030025              fake                     1                   fake                          1\n","frames_cropped_faces_10src    F3Net      10_41       fake        20                15               5            0.75       0.692616       0.018019              fake                     1                   fake                          1\n","frames_cropped_faces_10src    F3Net      10_42       fake        20                11               9            0.55       0.690474       0.029645              fake                     1                   fake                          1\n","frames_cropped_faces_10src    F3Net      10_43       fake        20                18               2            0.90       0.699654       0.015786              fake                     1                   fake                          1\n","frames_cropped_faces_10src    F3Net      10_44       fake        20                 0              20            0.00       0.644487       0.015279              real                     0                   real                          0\n","frames_cropped_faces_10src    F3Net      10_45       fake        20                 0              20            0.00       0.613038       0.021849              real                     0                   real                          0\n","frames_cropped_faces_10src    F3Net      10_46       fake        20                 5              15            0.25       0.667053       0.017504              fake                     1                   real                          0\n","frames_cropped_faces_10src    F3Net      10_47       fake        20                19               1            0.95       0.727090       0.028485              fake                     1                   fake                          1\n","frames_cropped_faces_10src    F3Net      10_48       fake        20                20               0            1.00       0.734601       0.015600              fake                     1                   fake                          1\n","frames_cropped_faces_10src    F3Net      10_49       fake        20                 8              12            0.40       0.672500       0.022904              fake                     1                   real                          0\n","frames_cropped_faces_10src    F3Net       10_5       fake        20                16               4            0.80       0.706565       0.032951              fake                     1                   fake                          1\n","frames_cropped_faces_10src    F3Net      10_50       fake        20                 8              12            0.40       0.672517       0.025283              fake                     1                   real                          0\n","frames_cropped_faces_10src    F3Net       10_6       fake        20                 7              13            0.35       0.665755       0.032358              fake                     1                   real                          0\n","frames_cropped_faces_10src    F3Net       10_7       fake        20                 0              20            0.00       0.636369       0.017853              real                     0                   real                          0\n","frames_cropped_faces_10src    F3Net       10_8       fake        20                12               8            0.60       0.682663       0.026353              fake                     1                   fake                          1\n","frames_cropped_faces_10src    F3Net       10_9       fake        20                15               5            0.75       0.692862       0.015793              fake                     1                   fake                          1\n","frames_cropped_faces_10src    F3Net        Ali       real        20                20               0            1.00       0.633056       0.018817              real                     1                   real                          1\n","frames_cropped_faces_10src    F3Net  Elizebeth       real        20                 7              13            0.35       0.686805       0.020723              fake                     0                   fake                          0\n","frames_cropped_faces_10src    F3Net     Ganesh       real        20                 3              17            0.15       0.704871       0.030816              fake                     0                   fake                          0\n","frames_cropped_faces_10src    F3Net        aji       real        20                20               0            1.00       0.631051       0.015088              real                     1                   real                          1\n","frames_cropped_faces_10src    F3Net      akbar       real        20                19               1            0.95       0.643292       0.024650              real                     1                   real                          1\n","frames_cropped_faces_10src    F3Net      akhil       real        20                14               6            0.70       0.677399       0.016998              fake                     0                   real                          1\n","frames_cropped_faces_10src    F3Net     akshay       real        20                 6              14            0.30       0.699436       0.031174              fake                     0                   fake                          0\n","frames_cropped_faces_10src    F3Net       alib       real        20                14               6            0.70       0.671565       0.013763              fake                     0                   real                          1\n","frames_cropped_faces_10src    F3Net      ameen       real        20                 2              18            0.10       0.699022       0.023489              fake                     0                   fake                          0\n","frames_cropped_faces_10src    F3Net       ammu       real        20                12               8            0.60       0.670344       0.038809              fake                     0                   real                          1\n","frames_cropped_faces_10src    F3Net     anandu       real        20                 7              13            0.35       0.686195       0.015559              fake                     0                   fake                          0\n","frames_cropped_faces_10src    F3Net      anish       real        20                11               9            0.55       0.673184       0.028235              fake                     0                   real                          1\n","frames_cropped_faces_10src    F3Net       ansu       real        20                12               8            0.60       0.671900       0.018431              fake                     0                   real                          1\n","frames_cropped_faces_10src    F3Net     arnold       real        20                16               4            0.80       0.658809       0.022975              fake                     0                   real                          1\n","frames_cropped_faces_10src    F3Net     assif_       real        20                20               0            1.00       0.587768       0.046547              real                     1                   real                          1\n","frames_cropped_faces_10src    F3Net    baptist       real        20                20               0            1.00       0.631854       0.023081              real                     1                   real                          1\n","frames_cropped_faces_10src    F3Net   binisha_       real        20                16               4            0.80       0.663229       0.018240              fake                     0                   real                          1\n","frames_cropped_faces_10src    F3Net    chettai       real        20                12               8            0.60       0.658534       0.038004              fake                     0                   real                          1\n","frames_cropped_faces_10src    F3Net      chris       real        20                19               1            0.95       0.654983       0.015916              fake                     0                   real                          1\n","frames_cropped_faces_10src    F3Net  christian       real        20                18               2            0.90       0.647873       0.024570              real                     1                   real                          1\n","frames_cropped_faces_10src    F3Net        col       real        20                17               3            0.85       0.652788       0.029159              real                     1                   real                          1\n","frames_cropped_faces_10src    F3Net    darwish       real        20                18               2            0.90       0.649432       0.024788              real                     1                   real                          1\n","frames_cropped_faces_10src    F3Net      deeps       real        20                20               0            1.00       0.638014       0.015253              real                     1                   real                          1\n","frames_cropped_faces_10src    F3Net      denna       real        20                10              10            0.50       0.681269       0.030093              fake                     0                   fake                          0\n","frames_cropped_faces_10src    F3Net    fathima       real        20                20               0            1.00       0.635234       0.018219              real                     1                   real                          1\n","frames_cropped_faces_10src    F3Net     jelvin       real        20                19               1            0.95       0.641818       0.020727              real                     1                   real                          1\n","frames_cropped_faces_10src    F3Net   jennifer       real        20                17               3            0.85       0.635955       0.033519              real                     1                   real                          1\n","frames_cropped_faces_10src    F3Net      jissa       real        20                 8              12            0.40       0.686581       0.014894              fake                     0                   fake                          0\n","frames_cropped_faces_10src    F3Net      kevin       real        20                 4              16            0.20       0.715825       0.032495              fake                     0                   fake                          0\n","frames_cropped_faces_10src    F3Net       lena       real        20                 6              14            0.30       0.694605       0.024315              fake                     0                   fake                          0\n","frames_cropped_faces_10src    F3Net       liya       real        20                19               1            0.95       0.652677       0.021770              real                     1                   real                          1\n","frames_cropped_faces_10src    F3Net    liyamom       real        20                18               2            0.90       0.639242       0.035930              real                     1                   real                          1\n","frames_cropped_faces_10src    F3Net       malu       real        20                20               0            1.00       0.636217       0.026554              real                     1                   real                          1\n","frames_cropped_faces_10src    F3Net      nevin       real        20                17               3            0.85       0.648162       0.031845              real                     1                   real                          1\n","frames_cropped_faces_10src    F3Net   niranjan       real        20                20               0            1.00       0.643141       0.014758              real                     1                   real                          1\n","frames_cropped_faces_10src    F3Net    praveen       real        20                18               2            0.90       0.650465       0.019223              real                     1                   real                          1\n","frames_cropped_faces_10src    F3Net    pushpan       real        20                 0              20            0.00       0.731211       0.017759              fake                     0                   fake                          0\n","frames_cropped_faces_10src    F3Net      rahul       real        20                18               2            0.90       0.649527       0.021789              real                     1                   real                          1\n","frames_cropped_faces_10src    F3Net       raju       real        20                 6              14            0.30       0.682499       0.019970              fake                     0                   fake                          0\n","frames_cropped_faces_10src    F3Net      rasee       real        20                14               6            0.70       0.666734       0.021157              fake                     0                   real                          1\n","frames_cropped_faces_10src    F3Net     roshan       real        20                 8              12            0.40       0.681191       0.019399              fake                     0                   fake                          0\n","frames_cropped_faces_10src    F3Net     sachin       real        20                 8              12            0.40       0.689210       0.029977              fake                     0                   fake                          0\n","frames_cropped_faces_10src    F3Net      salim       real        20                17               3            0.85       0.656472       0.023647              fake                     0                   real                          1\n","frames_cropped_faces_10src    F3Net     seethu       real        20                 5              15            0.25       0.700984       0.041362              fake                     0                   fake                          0\n","frames_cropped_faces_10src    F3Net     shanty       real        20                 3              17            0.15       0.780703       0.070446              fake                     0                   fake                          0\n","frames_cropped_faces_10src    F3Net       subu       real        20                12               8            0.60       0.667221       0.030299              fake                     0                   real                          1\n","frames_cropped_faces_10src    F3Net      teggy       real        20                 6              14            0.30       0.697991       0.042515              fake                     0                   fake                          0\n","frames_cropped_faces_10src    F3Net     thomas       real        20                16               4            0.80       0.665884       0.021215              fake                     0                   real                          1\n","frames_cropped_faces_10src    F3Net      umesh       real        20                18               2            0.90       0.662839       0.022261              fake                     0                   real                          1\n","frames_cropped_faces_10src    F3Net        yad       real        20                20               0            1.00       0.619296       0.028465              real                     1                   real                          1\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=False)\n","\n","import os, time\n","SAVE_DIR = \"/content/drive/MyDrive/f3net results 10 src\"\n","os.makedirs(SAVE_DIR, exist_ok=True)\n","\n","CSV_PATH = os.path.join(SAVE_DIR, f\"f3net_large_table_10src_{time.strftime('%Y%m%d-%H%M%S')}.csv\")\n","df.to_csv(CSV_PATH, index=False)\n","\n","print(\"Saved to:\", CSV_PATH)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QPuoFGwrm9-G","executionInfo":{"status":"ok","timestamp":1761064255479,"user_tz":-120,"elapsed":2399,"user":{"displayName":"Vishnumaya","userId":"01919615312035119785"}},"outputId":"3f281c4f-d3a0-4e16-f466-42579b3e1e37"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Saved to: /content/drive/MyDrive/f3net results 10 src/f3net_large_table_10src_20251021-163056.csv\n"]}]},{"cell_type":"code","source":["# === Small table from LARGE table `df` (use majority decision) ===\n","# correctly_predicted = \"yes\" if video_pred_by_majority == true_label else \"no\"\n","\n","import pandas as pd\n","import numpy as np\n","\n","# Normalize to lowercase strings just in case\n","tl = df[\"true_label\"].astype(str).str.lower()\n","vp = df[\"video_pred_by_majority\"].astype(str).str.lower()\n","\n","small_df = df[[\"dataset\",\"detector\",\"video_name\",\"true_label\"]].copy()\n","small_df[\"correctly_predicted\"] = np.where(vp == tl, \"yes\", \"no\")\n","\n","# Print all rows with no column breaks\n","pd.set_option(\"display.max_rows\", 500)\n","pd.set_option(\"display.max_columns\", None)\n","pd.set_option(\"display.width\", 0)\n","print(small_df.to_string(index=False))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1JWWMnURqwid","executionInfo":{"status":"ok","timestamp":1761065245139,"user_tz":-120,"elapsed":80,"user":{"displayName":"Vishnumaya","userId":"01919615312035119785"}},"outputId":"3c8093b8-7315-4651-f2ef-a40fe09e0986"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["                   dataset detector video_name true_label correctly_predicted\n","frames_cropped_faces_10src    F3Net       10_1       fake                  no\n","frames_cropped_faces_10src    F3Net      10_10       fake                 yes\n","frames_cropped_faces_10src    F3Net      10_11       fake                  no\n","frames_cropped_faces_10src    F3Net      10_12       fake                 yes\n","frames_cropped_faces_10src    F3Net      10_13       fake                  no\n","frames_cropped_faces_10src    F3Net      10_14       fake                  no\n","frames_cropped_faces_10src    F3Net      10_15       fake                  no\n","frames_cropped_faces_10src    F3Net      10_16       fake                 yes\n","frames_cropped_faces_10src    F3Net      10_17       fake                 yes\n","frames_cropped_faces_10src    F3Net      10_18       fake                  no\n","frames_cropped_faces_10src    F3Net      10_19       fake                 yes\n","frames_cropped_faces_10src    F3Net       10_2       fake                 yes\n","frames_cropped_faces_10src    F3Net      10_20       fake                  no\n","frames_cropped_faces_10src    F3Net      10_21       fake                  no\n","frames_cropped_faces_10src    F3Net      10_22       fake                  no\n","frames_cropped_faces_10src    F3Net      10_23       fake                 yes\n","frames_cropped_faces_10src    F3Net      10_24       fake                  no\n","frames_cropped_faces_10src    F3Net      10_25       fake                 yes\n","frames_cropped_faces_10src    F3Net      10_26       fake                  no\n","frames_cropped_faces_10src    F3Net      10_27       fake                  no\n","frames_cropped_faces_10src    F3Net      10_28       fake                 yes\n","frames_cropped_faces_10src    F3Net      10_29       fake                  no\n","frames_cropped_faces_10src    F3Net       10_3       fake                  no\n","frames_cropped_faces_10src    F3Net      10_30       fake                  no\n","frames_cropped_faces_10src    F3Net      10_31       fake                  no\n","frames_cropped_faces_10src    F3Net      10_32       fake                  no\n","frames_cropped_faces_10src    F3Net      10_33       fake                 yes\n","frames_cropped_faces_10src    F3Net      10_34       fake                 yes\n","frames_cropped_faces_10src    F3Net      10_35       fake                  no\n","frames_cropped_faces_10src    F3Net      10_36       fake                 yes\n","frames_cropped_faces_10src    F3Net      10_37       fake                 yes\n","frames_cropped_faces_10src    F3Net      10_38       fake                 yes\n","frames_cropped_faces_10src    F3Net      10_39       fake                  no\n","frames_cropped_faces_10src    F3Net       10_4       fake                 yes\n","frames_cropped_faces_10src    F3Net      10_40       fake                 yes\n","frames_cropped_faces_10src    F3Net      10_41       fake                 yes\n","frames_cropped_faces_10src    F3Net      10_42       fake                 yes\n","frames_cropped_faces_10src    F3Net      10_43       fake                 yes\n","frames_cropped_faces_10src    F3Net      10_44       fake                  no\n","frames_cropped_faces_10src    F3Net      10_45       fake                  no\n","frames_cropped_faces_10src    F3Net      10_46       fake                  no\n","frames_cropped_faces_10src    F3Net      10_47       fake                 yes\n","frames_cropped_faces_10src    F3Net      10_48       fake                 yes\n","frames_cropped_faces_10src    F3Net      10_49       fake                  no\n","frames_cropped_faces_10src    F3Net       10_5       fake                 yes\n","frames_cropped_faces_10src    F3Net      10_50       fake                  no\n","frames_cropped_faces_10src    F3Net       10_6       fake                  no\n","frames_cropped_faces_10src    F3Net       10_7       fake                  no\n","frames_cropped_faces_10src    F3Net       10_8       fake                 yes\n","frames_cropped_faces_10src    F3Net       10_9       fake                 yes\n","frames_cropped_faces_10src    F3Net        Ali       real                 yes\n","frames_cropped_faces_10src    F3Net  Elizebeth       real                  no\n","frames_cropped_faces_10src    F3Net     Ganesh       real                  no\n","frames_cropped_faces_10src    F3Net        aji       real                 yes\n","frames_cropped_faces_10src    F3Net      akbar       real                 yes\n","frames_cropped_faces_10src    F3Net      akhil       real                 yes\n","frames_cropped_faces_10src    F3Net     akshay       real                  no\n","frames_cropped_faces_10src    F3Net       alib       real                 yes\n","frames_cropped_faces_10src    F3Net      ameen       real                  no\n","frames_cropped_faces_10src    F3Net       ammu       real                 yes\n","frames_cropped_faces_10src    F3Net     anandu       real                  no\n","frames_cropped_faces_10src    F3Net      anish       real                 yes\n","frames_cropped_faces_10src    F3Net       ansu       real                 yes\n","frames_cropped_faces_10src    F3Net     arnold       real                 yes\n","frames_cropped_faces_10src    F3Net     assif_       real                 yes\n","frames_cropped_faces_10src    F3Net    baptist       real                 yes\n","frames_cropped_faces_10src    F3Net   binisha_       real                 yes\n","frames_cropped_faces_10src    F3Net    chettai       real                 yes\n","frames_cropped_faces_10src    F3Net      chris       real                 yes\n","frames_cropped_faces_10src    F3Net  christian       real                 yes\n","frames_cropped_faces_10src    F3Net        col       real                 yes\n","frames_cropped_faces_10src    F3Net    darwish       real                 yes\n","frames_cropped_faces_10src    F3Net      deeps       real                 yes\n","frames_cropped_faces_10src    F3Net      denna       real                  no\n","frames_cropped_faces_10src    F3Net    fathima       real                 yes\n","frames_cropped_faces_10src    F3Net     jelvin       real                 yes\n","frames_cropped_faces_10src    F3Net   jennifer       real                 yes\n","frames_cropped_faces_10src    F3Net      jissa       real                  no\n","frames_cropped_faces_10src    F3Net      kevin       real                  no\n","frames_cropped_faces_10src    F3Net       lena       real                  no\n","frames_cropped_faces_10src    F3Net       liya       real                 yes\n","frames_cropped_faces_10src    F3Net    liyamom       real                 yes\n","frames_cropped_faces_10src    F3Net       malu       real                 yes\n","frames_cropped_faces_10src    F3Net      nevin       real                 yes\n","frames_cropped_faces_10src    F3Net   niranjan       real                 yes\n","frames_cropped_faces_10src    F3Net    praveen       real                 yes\n","frames_cropped_faces_10src    F3Net    pushpan       real                  no\n","frames_cropped_faces_10src    F3Net      rahul       real                 yes\n","frames_cropped_faces_10src    F3Net       raju       real                  no\n","frames_cropped_faces_10src    F3Net      rasee       real                 yes\n","frames_cropped_faces_10src    F3Net     roshan       real                  no\n","frames_cropped_faces_10src    F3Net     sachin       real                  no\n","frames_cropped_faces_10src    F3Net      salim       real                 yes\n","frames_cropped_faces_10src    F3Net     seethu       real                  no\n","frames_cropped_faces_10src    F3Net     shanty       real                  no\n","frames_cropped_faces_10src    F3Net       subu       real                 yes\n","frames_cropped_faces_10src    F3Net      teggy       real                  no\n","frames_cropped_faces_10src    F3Net     thomas       real                 yes\n","frames_cropped_faces_10src    F3Net      umesh       real                 yes\n","frames_cropped_faces_10src    F3Net        yad       real                 yes\n"]}]},{"cell_type":"code","source":["import os, time\n","SAVE_DIR = \"/content/drive/MyDrive/f3net results 10 src\"\n","os.makedirs(SAVE_DIR, exist_ok=True)\n","small_df.to_csv(os.path.join(SAVE_DIR, f\"f3net_small_table_majority_10src_{time.strftime('%Y%m%d-%H%M%S')}.csv\"), index=False)\n"],"metadata":{"id":"oere9SH8q-A7"},"execution_count":null,"outputs":[]}]}