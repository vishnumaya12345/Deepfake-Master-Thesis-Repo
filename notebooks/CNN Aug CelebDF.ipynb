{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOur6oDG7/xWc18eRQ+Usgr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AKd7zOUJsb5Y","executionInfo":{"status":"ok","timestamp":1756033468185,"user_tz":-120,"elapsed":62493,"user":{"displayName":"Vishnumaya","userId":"01919615312035119785"}},"outputId":"46ae7c09-cb27-42cf-eab2-c432d0adf381"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","CUDA: True | device: cuda\n","Using source: face crops\n","  REAL: /content/drive/My Drive/frames_xception_faces/real  | images: 1000\n","  FAKE: /content/drive/My Drive/frames_xception_faces/fake  | images: 1000\n","Loaded weights: /content/drive/My Drive/DeepfakeBench_weights/effnb4_best.pth\n","Matched tensors: 704/706 (~100%)\n","✅ Model loaded: CNN-Aug (EffNet-B4, 2-class) on cuda | dummy output shape: (1, 2)\n"]}],"source":["# STEP 1 — Setup + find frames/crops + LOAD CNN-Aug (EfficientNet-B4, 2-class) and print status.\n","\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=False)\n","\n","import os, sys, fnmatch, subprocess, numpy as np\n","import torch, torch.nn as nn\n","from PIL import Image\n","\n","# --- quiet install\n","def _pip(*pkgs):\n","    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", *pkgs],\n","                   stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=True)\n","try:\n","    from efficientnet_pytorch import EfficientNet\n","except Exception:\n","    _pip(\"efficientnet-pytorch==0.7.1\"); from efficientnet_pytorch import EfficientNet\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","torch.backends.cudnn.benchmark = True\n","\n","# ---------- paths (edit if needed) ----------\n","CROPS_REAL  = \"/content/drive/My Drive/frames_xception_faces/real\"\n","CROPS_FAKE  = \"/content/drive/My Drive/frames_xception_faces/fake\"\n","FRAMES_REAL = \"/content/drive/My Drive/frames/celebdf_effb4/real\"\n","FRAMES_FAKE = \"/content/drive/My Drive/frames/celebdf_effb4/fake\"\n","\n","IMG_EXTS = (\".jpg\",\".jpeg\",\".png\",\".bmp\",\".webp\")\n","def is_img(p): return p.lower().endswith(IMG_EXTS)\n","def count_imgs(d):\n","    try: return sum(is_img(os.path.join(d,f)) for f in os.listdir(d))\n","    except: return 0\n","\n","use_crops = count_imgs(CROPS_REAL)>0 and count_imgs(CROPS_FAKE)>0\n","SRC_REAL, SRC_FAKE = (CROPS_REAL,CROPS_FAKE) if use_crops else (FRAMES_REAL,FRAMES_FAKE)\n","\n","# ---------- model defs ----------\n","class EffB4_Flat(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.backbone = EfficientNet.from_name(\"efficientnet-b4\")\n","        self.backbone._fc = nn.Identity()\n","        self.head = nn.Linear(1792, 2)\n","    def forward(self, x): return self.head(self.backbone(x))\n","\n","class EffB4_Nested(nn.Module):\n","    # some checkpoints use backbone.efficientnet + backbone.last_layer\n","    def __init__(self):\n","        super().__init__()\n","        self.backbone = nn.Module()\n","        self.backbone.efficientnet = EfficientNet.from_name(\"efficientnet-b4\")\n","        self.backbone.efficientnet._fc = nn.Identity()\n","        self.backbone.last_layer = nn.Linear(1792, 2)\n","    def forward(self, x):\n","        x = self.backbone.efficientnet(x)\n","        return self.backbone.last_layer(x)\n","\n","def load_ckpt_dict(path):\n","    ck = torch.load(path, map_location=\"cpu\")\n","    if isinstance(ck, dict):\n","        for k in (\"state_dict\",\"model\",\"net\",\"weights\",\"model_state\",\"ema_state_dict\"):\n","            if k in ck and isinstance(ck[k], dict): ck = ck[k]\n","    if not isinstance(ck, dict): raise ValueError(\"Checkpoint not a state-dict.\")\n","    clean={}\n","    for k,v in ck.items():\n","        if not isinstance(k,str): continue\n","        k2=k\n","        for pref in (\"module.\",\"model.\",\"net.\"):\n","            if k2.startswith(pref): k2=k2[len(pref):]\n","        clean[k2]=v\n","    return clean\n","\n","def remap_for_flat(sd):\n","    out={}\n","    for k,v in sd.items():\n","        k2=k\n","        if k2.startswith(\"backbone.efficientnet.\"):\n","            k2 = k2.replace(\"backbone.efficientnet.\",\"backbone.\")\n","        if k2.endswith(\"_fc.weight\"): out[\"head.weight\"]=v; continue\n","        if k2.endswith(\"_fc.bias\"):   out[\"head.bias\"]=v;   continue\n","        if k2.startswith(\"last_layer.\"): out[k2.replace(\"last_layer.\",\"head.\")]=v; continue\n","        out[k2]=v\n","    return out\n","\n","def remap_for_nested(sd):\n","    out={}\n","    for k,v in sd.items():\n","        k2=k\n","        if k2.startswith(\"backbone.\") and not k2.startswith(\"backbone.efficientnet.\"):\n","            k2 = k2.replace(\"backbone.\",\"backbone.efficientnet.\",1)\n","        if k2.endswith(\"_fc.weight\"): out[\"backbone.last_layer.weight\"]=v; continue\n","        if k2.endswith(\"_fc.bias\"):   out[\"backbone.last_layer.bias\"]=v;   continue\n","        if k2.startswith(\"head.\"): out[k2.replace(\"head.\",\"backbone.last_layer.\")]=v; continue\n","        out[k2]=v\n","    return out\n","\n","def coverage(model, sd):\n","    m = model.state_dict()\n","    matched = sum(1 for k,w in sd.items() if k in m and m[k].shape==w.shape)\n","    total   = len(m)\n","    return matched, total\n","\n","def try_load(ctor, sd_raw, remap):\n","    m = ctor().to(device)\n","    sd = remap(sd_raw)\n","    match, total = coverage(m, sd)\n","    if match == 0: return None, 0, total\n","    m.load_state_dict(sd, strict=False); m.eval()\n","    return m, match, total\n","\n","def find_weights():\n","    roots = [\"/content/drive/My Drive\", \"/content/drive/MyDrive\", \"/content/drive/Shareddrives\"]\n","    pats  = [\"cnnaug*best*.pth\",\"cnnaug*.pth\",\"effnb4*best*.pth\",\"effnb4*.pth\"]\n","    hits=[]\n","    for root in roots:\n","        if not os.path.isdir(root): continue\n","        for dp,_,fs in os.walk(root):\n","            for f in fs:\n","                low=f.lower()\n","                if any(fnmatch.fnmatch(low, p) for p in pats):\n","                    p=os.path.join(dp,f)\n","                    try: hits.append((p, os.path.getsize(p), os.path.getmtime(p)))\n","                    except: hits.append((p, 0, 0))\n","    if not hits: return None\n","    hits.sort(key=lambda x:(0 if \"cnnaug\" in os.path.basename(x[0]).lower() else 1,\n","                            0 if \"best\"   in os.path.basename(x[0]).lower() else 1,\n","                            -x[1], -x[2]))\n","    return [h[0] for h in hits]\n","\n","# ---------- load best-matching CNN-Aug/EffB4 weights ----------\n","candidates = find_weights()\n","if not candidates:\n","    raise FileNotFoundError(\"No cnnaug/effnb4 weights found in Drive. Place e.g. 'cnnaug_best.pth' in My Drive.\")\n","\n","best = None  # (coverage, model, path, matched, total)\n","for w in candidates:\n","    try:\n","        sd = load_ckpt_dict(w)\n","    except Exception:\n","        continue\n","    for ctor, remap in ((EffB4_Flat, remap_for_flat), (EffB4_Nested, remap_for_nested)):\n","        try:\n","            m, matched, total = try_load(ctor, sd, remap)\n","        except Exception:\n","            m=None; matched=0; total=1\n","        if m is None: continue\n","        cov = matched/total if total>0 else 0.0\n","        if (best is None) or (cov > best[0]):\n","            best = (cov, m, w, matched, total)\n","\n","if best is None or best[0] < 0.30:\n","    raise RuntimeError(\"Could not match weights to an EfficientNet-B4 2-class head. \"\n","                       \"Ensure the checkpoint is CNN-Aug/EffB4 (e.g., cnnaug_best.pth).\")\n","\n","coverage_ratio, model, WEIGHTS_PATH, matched, total = best\n","model.eval()\n","\n","# sanity forward to confirm shape\n","IMG_SIZE = 380\n","with torch.no_grad():\n","    x = torch.zeros(1,3,IMG_SIZE,IMG_SIZE, device=device)\n","    y = model(x)\n","out_shape = tuple(y.shape)\n","\n","# ---------- summary prints ----------\n","print(f\"CUDA: {torch.cuda.is_available()} | device: {device.type}\")\n","print(f\"Using source: {'face crops' if use_crops else 'raw frames'}\")\n","print(f\"  REAL: {SRC_REAL}  | images: {count_imgs(SRC_REAL)}\")\n","print(f\"  FAKE: {SRC_FAKE}  | images: {count_imgs(SRC_FAKE)}\")\n","print(f\"Loaded weights: {WEIGHTS_PATH}\")\n","print(f\"Matched tensors: {matched}/{total} (~{coverage_ratio*100:.0f}%)\")\n","print(f\"✅ Model loaded: CNN-Aug (EffNet-B4, 2-class) on {device.type} | dummy output shape: {out_shape}\")\n","\n","# Ready for Step 2 (evaluation)\n"]},{"cell_type":"code","source":["# Force-load CNN-Aug (EfficientNet-B4, 2-class) from your exact path\n","\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=False)\n","\n","import os, torch, torch.nn as nn\n","from efficientnet_pytorch import EfficientNet\n","\n","CNN_AUG_PATH = \"/content/drive/My Drive/DeepfakeBench_weights/cnnaug_best.pth\"  # ← your file\n","\n","# 1) sanity that Colab sees the file\n","print(\"exists:\", os.path.exists(CNN_AUG_PATH), \"| size:\", os.path.getsize(CNN_AUG_PATH) if os.path.exists(CNN_AUG_PATH) else \"—\")\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# 2) model arch (EffNet-B4 with 2-class head)\n","class EffB4_Flat(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.backbone = EfficientNet.from_name(\"efficientnet-b4\")\n","        self.backbone._fc = nn.Identity()\n","        self.head = nn.Linear(1792, 2)\n","    def forward(self, x):\n","        return self.head(self.backbone(x))\n","\n","class EffB4_Nested(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.backbone = nn.Module()\n","        self.backbone.efficientnet = EfficientNet.from_name(\"efficientnet-b4\")\n","        self.backbone.efficientnet._fc = nn.Identity()\n","        self.backbone.last_layer = nn.Linear(1792, 2)\n","    def forward(self, x):\n","        x = self.backbone.efficientnet(x)\n","        return self.backbone.last_layer(x)\n","\n","# 3) load checkpoint and remap common key patterns\n","def load_ckpt_dict(path):\n","    ck = torch.load(path, map_location=\"cpu\")\n","    if isinstance(ck, dict):\n","        for k in (\"state_dict\",\"model\",\"net\",\"weights\",\"model_state\",\"ema_state_dict\"):\n","            if k in ck and isinstance(ck[k], dict):\n","                ck = ck[k]\n","    if not isinstance(ck, dict):\n","        raise ValueError(\"Checkpoint is not a state-dict.\")\n","    clean = {}\n","    for k,v in ck.items():\n","        if not isinstance(k,str): continue\n","        k2 = k\n","        for pref in (\"module.\",\"model.\",\"net.\"):\n","            if k2.startswith(pref): k2 = k2[len(pref):]\n","        # heads commonly seen in DeepfakeBench\n","        if k2.endswith(\"_fc.weight\"): k2=\"head.weight\"\n","        if k2.endswith(\"_fc.bias\"):   k2=\"head.bias\"\n","        if k2.startswith(\"last_layer.\"): k2 = k2.replace(\"last_layer.\",\"head.\")\n","        # flatten nested efficientnet path\n","        if k2.startswith(\"backbone.efficientnet.\"):\n","            k2 = k2.replace(\"backbone.efficientnet.\",\"backbone.\")\n","        clean[k2] = v\n","    return clean\n","\n","def coverage(model, sd):\n","    m = model.state_dict()\n","    matched = sum(1 for k,w in sd.items() if k in m and m[k].shape==w.shape)\n","    total   = len(m)\n","    return matched, total\n","\n","sd = load_ckpt_dict(CNN_AUG_PATH)\n","\n","# try both flat & nested layouts and pick the better match\n","m_flat   = EffB4_Flat().to(device)\n","m_nested = EffB4_Nested().to(device)\n","\n","# map for flat\n","sd_flat = sd.copy()\n","# map for nested (send any 'head.*' to 'backbone.last_layer.*')\n","sd_nested = {}\n","for k,v in sd.items():\n","    k2=k\n","    if k2.startswith(\"head.\"): k2 = k2.replace(\"head.\",\"backbone.last_layer.\")\n","    sd_nested[k2]=v\n","\n","match_f, tot_f = coverage(m_flat, sd_flat)\n","match_n, tot_n = coverage(m_nested, sd_nested)\n","\n","if match_f/tot_f >= match_n/tot_n:\n","    model, sd_use, matched, total, layout = m_flat, sd_flat, match_f, tot_f, \"flat\"\n","else:\n","    model, sd_use, matched, total, layout = m_nested, sd_nested, match_n, tot_n, \"nested\"\n","\n","# load and sanity forward\n","model.load_state_dict(sd_use, strict=False)\n","model.eval()\n","with torch.no_grad():\n","    y = model(torch.zeros(1,3,380,380, device=device))\n","\n","print(f\"✅ Loaded EXACT file: {os.path.basename(CNN_AUG_PATH)} | layout={layout} | matched {matched}/{total} ({matched/total:.0%}) | device={device.type} | out={tuple(y.shape)}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cXfa9kF5ynE4","executionInfo":{"status":"ok","timestamp":1756034140924,"user_tz":-120,"elapsed":3364,"user":{"displayName":"Vishnumaya","userId":"01919615312035119785"}},"outputId":"dd34a618-5bfe-4622-e3f4-e7911e712c0e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","exists: True | size: 85285901\n","✅ Loaded EXACT file: cnnaug_best.pth | layout=flat | matched 0/706 (0%) | device=cuda | out=(1, 2)\n"]}]},{"cell_type":"code","source":["# CNN-AUG (EfficientNet-B4, 2-class) — force-load from cnnaug_best.pth and evaluate strongly.\n","# Output: (1) model loaded line, (2) FINAL: AUC | EER | AP\n","\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=False)\n","\n","import os, re, fnmatch, sys, subprocess, shutil, numpy as np, pandas as pd, cv2, warnings\n","from PIL import Image, ImageFilter\n","import torch, torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","from torchvision.transforms import functional as TF\n","from sklearn.metrics import roc_auc_score, average_precision_score, roc_curve\n","\n","warnings.filterwarnings(\"ignore\", category=FutureWarning)\n","\n","# ---------- deps ----------\n","def _pip(*pkgs):\n","    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", *pkgs],\n","                   stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=True)\n","try:\n","    from efficientnet_pytorch import EfficientNet\n","except Exception:\n","    _pip(\"efficientnet-pytorch==0.7.1\"); from efficientnet_pytorch import EfficientNet\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","torch.backends.cudnn.benchmark = True\n","\n","# ---------- YOUR PATHS (edit if needed) ----------\n","CROPS_REAL = \"/content/drive/My Drive/frames_xception_faces/real\"\n","CROPS_FAKE = \"/content/drive/My Drive/frames_xception_faces/fake\"\n","CNN_AUG_PATH = \"/content/drive/My Drive/DeepfakeBench_weights/cnnaug_best.pth\"\n","\n","# ---------- check data ----------\n","IMG_EXTS = (\".jpg\",\".jpeg\",\".png\",\".bmp\",\".webp\")\n","def is_img(p): return p.lower().endswith(IMG_EXTS)\n","def has_imgs(d):\n","    try: return os.path.isdir(d) and any(is_img(os.path.join(d,f)) for f in os.listdir(d))\n","    except: return False\n","assert has_imgs(CROPS_REAL) and has_imgs(CROPS_FAKE), \"Face crops not found. Update CROPS_REAL/CROPS_FAKE.\"\n","\n","# ---------- model (EffNet-B4 2-class) ----------\n","class EffB4_Flat(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.backbone = EfficientNet.from_name(\"efficientnet-b4\")\n","        self.backbone._fc = nn.Identity()\n","        self.head = nn.Linear(1792, 2)\n","    def forward(self, x): return self.head(self.backbone(x))\n","\n","def load_ckpt_dict(path):\n","    ck = torch.load(path, map_location=\"cpu\")\n","    if isinstance(ck, dict):\n","        for k in (\"state_dict\",\"model\",\"net\",\"weights\",\"model_state\",\"ema_state_dict\"):\n","            if k in ck and isinstance(ck[k], dict):\n","                ck = ck[k]\n","    if not isinstance(ck, dict):\n","        raise ValueError(\"Checkpoint is not a state-dict.\")\n","    clean={}\n","    for k,v in ck.items():\n","        if not isinstance(k,str): continue\n","        k2=k\n","        for pref in (\"module.\",\"model.\",\"net.\"):\n","            if k2.startswith(pref): k2=k2[len(pref):]\n","        # common head remaps\n","        if k2.endswith(\"_fc.weight\"): k2=\"head.weight\"\n","        if k2.endswith(\"_fc.bias\"):   k2=\"head.bias\"\n","        if k2.startswith(\"last_layer.\"): k2=k2.replace(\"last_layer.\",\"head.\")\n","        if k2.startswith(\"backbone.efficientnet.\"): k2=k2.replace(\"backbone.efficientnet.\",\"backbone.\")\n","        clean[k2]=v\n","    return clean\n","\n","assert os.path.isfile(CNN_AUG_PATH), f\"Missing weights: {CNN_AUG_PATH}\"\n","sd = load_ckpt_dict(CNN_AUG_PATH)\n","\n","model = EffB4_Flat().to(device)\n","mstate = model.state_dict()\n","matched = sum(1 for k,w in sd.items() if k in mstate and mstate[k].shape==w.shape)\n","total   = len(mstate)\n","coverage = matched/total\n","# Require good match; otherwise you're not evaluating CNN-Aug EffB4 weights.\n","assert coverage >= 0.60, (\n","    f\"'{os.path.basename(CNN_AUG_PATH)}' does not look like a CNN-Aug EfficientNet-B4 (2-class) checkpoint.\\n\"\n","    f\"Matched {matched}/{total} ({coverage:.0%}). Please provide the correct CNN-Aug weights.\"\n",")\n","model.load_state_dict(sd, strict=False); model.eval()\n","\n","# sanity forward\n","with torch.no_grad():\n","    _ = model(torch.zeros(1,3,380,380, device=device))\n","print(f\"✅ CNN-Aug loaded: {os.path.basename(CNN_AUG_PATH)} | match {matched}/{total} (~{coverage*100:.0f}%) | device={device.type}\")\n","\n","# ---------- collect frames (cap per video) ----------\n","MAX_FRAMES_PER_VIDEO = 120  # more = stabler per-video scores\n","def infer_video_name(path):\n","    stem = os.path.splitext(os.path.basename(path))[0]\n","    m = re.split(r\"_frame\\d+$\", stem)\n","    if len(m)>1 and m[0]: return m[0]\n","    m2 = re.sub(r\"[_\\-]\\d+$\",\"\",stem)\n","    return m2 if m2 and m2!=stem else stem\n","def frame_index(path):\n","    m = re.search(r\"_frame(\\d+)\", os.path.basename(path))\n","    return int(m.group(1)) if m else 10**9\n","\n","def collect_with_cap(folder, label):\n","    files = [os.path.join(folder, f) for f in os.listdir(folder) if is_img(f)]\n","    rows=[]\n","    for p in files:\n","        g = cv2.imread(p, cv2.IMREAD_GRAYSCALE)\n","        blur = float(cv2.Laplacian(g, cv2.CV_64F).var()) if g is not None else 0.0\n","        rows.append({\"path\":p, \"video_name\":infer_video_name(p), \"idx\":frame_index(p),\n","                     \"label\":label, \"blur\":blur})\n","    if not rows: return pd.DataFrame()\n","    df = pd.DataFrame(rows)\n","    return (df.sort_values([\"video_name\",\"idx\",\"blur\"], ascending=[True,True,False])\n","              .groupby(\"video_name\", as_index=False).head(MAX_FRAMES_PER_VIDEO))\n","\n","df_r = collect_with_cap(CROPS_REAL, 0)\n","df_f = collect_with_cap(CROPS_FAKE, 1)\n","df_sel = pd.concat([df_r, df_f], ignore_index=True)\n","assert len(df_sel)>0, \"No images found after selection.\"\n","\n","# ---------- preprocessing & scoring ----------\n","def unsharp_pil(img):  # sharpen soft faces\n","    return img.filter(ImageFilter.UnsharpMask(radius=2, percent=150, threshold=3))\n","\n","def build_center_transform(size, pre=\"unsharp\"):\n","    pre_fn = unsharp_pil if pre==\"unsharp\" else (lambda im: im)\n","    return transforms.Compose([\n","        transforms.Lambda(pre_fn),\n","        transforms.Resize(size),\n","        transforms.CenterCrop(size),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n","    ])\n","\n","def build_tencrop_transform(size, pre=\"unsharp\"):\n","    pre_fn = unsharp_pil if pre==\"unsharp\" else (lambda im: im)\n","    return transforms.Compose([\n","        transforms.Lambda(pre_fn),\n","        transforms.Resize(size+32, interpolation=transforms.InterpolationMode.BICUBIC),\n","        transforms.TenCrop(size),\n","        transforms.Lambda(lambda crops: torch.stack([\n","            transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])(transforms.ToTensor()(c))\n","            for c in crops\n","        ]))\n","    ])\n","\n","class DS_Center(Dataset):\n","    def __init__(self, df_select, tfm):\n","        self.df = df_select.reset_index(drop=True); self.t = tfm\n","    def __len__(self): return len(self.df)\n","    def __getitem__(self, i):\n","        r = self.df.iloc[i]\n","        return self.t(Image.open(r[\"path\"]).convert(\"RGB\")), int(r[\"label\"]), r[\"video_name\"], float(r[\"blur\"])\n","\n","class DS_TenCrop(Dataset):\n","    def __init__(self, df_select, tfm_tc):\n","        self.df = df_select.reset_index(drop=True); self.t = tfm_tc\n","    def __len__(self): return len(self.df)\n","    def __getitem__(self, i):\n","        r = self.df.iloc[i]\n","        tc = self.t(Image.open(r[\"path\"]).convert(\"RGB\"))  # (10,3,H,W)\n","        return tc, int(r[\"label\"]), r[\"video_name\"], float(r[\"blur\"])\n","\n","softmax = torch.nn.Softmax(dim=1)\n","\n","def score_center(df_select, size=380, pre=\"unsharp\", hflip=True):\n","    ds = DS_Center(df_select, build_center_transform(size, pre))\n","    loader = DataLoader(ds, batch_size=32, shuffle=False, num_workers=2, pin_memory=torch.cuda.is_available())\n","    probs, labels, names, blurs = [], [], [], []\n","    use_amp = (device.type==\"cuda\")\n","    with torch.no_grad(), torch.amp.autocast('cuda', enabled=use_amp):\n","        for xb, yb, vb, bb in loader:\n","            xb = xb.to(device, non_blocking=True)\n","            logits = model(xb)\n","            if hflip: logits = (logits + model(TF.hflip(xb))) / 2\n","            p = softmax(logits)[:,1].detach().cpu().numpy()\n","            probs.append(p); labels.append(yb.numpy()); names += list(vb); blurs += list(bb.numpy())\n","    return pd.DataFrame({\"video_name\": names,\n","                         \"true_label\": np.where(np.concatenate(labels)==1,\"fake\",\"real\"),\n","                         \"prob_fake\": np.concatenate(probs),\n","                         \"blur\": np.array(blurs)})\n","\n","def score_tencrop(df_select, size=380, pre=\"unsharp\"):\n","    ds = DS_TenCrop(df_select, build_tencrop_transform(size, pre))\n","    loader = DataLoader(ds, batch_size=8, shuffle=False, num_workers=2, pin_memory=torch.cuda.is_available())\n","    probs, labels, names, blurs = [], [], [], []\n","    use_amp = (device.type==\"cuda\")\n","    with torch.no_grad(), torch.amp.autocast('cuda', enabled=use_amp):\n","        for xb, yb, vb, bb in loader:\n","            B, N, C, H, W = xb.shape\n","            xb = xb.view(B*N, C, H, W).to(device, non_blocking=True)\n","            logits = model(xb).view(B, N, 2).mean(dim=1)\n","            p = softmax(logits)[:,1].detach().cpu().numpy()\n","            probs.append(p); labels.append(yb.numpy()); names += list(vb); blurs += list(bb.numpy())\n","    return pd.DataFrame({\"video_name\": names,\n","                         \"true_label\": np.where(np.concatenate(labels)==1,\"fake\",\"real\"),\n","                         \"prob_fake\": np.concatenate(probs),\n","                         \"blur\": np.array(blurs)})\n","\n","# ---------- metrics & aggregation ----------\n","def roc_metrics(scores, labels):\n","    auc = roc_auc_score(labels, scores)\n","    ap  = average_precision_score(labels, scores)\n","    fpr, tpr, thr = roc_curve(labels, scores); fnr = 1 - tpr\n","    i = int(np.nanargmin(np.abs(fnr - fpr)))\n","    eer = float((fpr[i] + fnr[i]) / 2.0)\n","    return auc, eer, ap\n","\n","def trimmed_mean_np(v, trim=0.1):\n","    v = np.sort(v); k = int(len(v)*trim)\n","    return float(v[k:len(v)-k].mean()) if len(v)>2*k else float(v.mean())\n","\n","def aggregate(df_scores, how):\n","    grp = df_scores.groupby([\"video_name\",\"true_label\"], group_keys=False)\n","    if how==\"median\":     return grp[\"prob_fake\"].median().reset_index()\n","    if how==\"perc90\":     return grp[\"prob_fake\"].quantile(0.9).reset_index()\n","    if how==\"top10\":\n","        tmp = df_scores.copy()\n","        tmp[\"rank\"] = tmp.groupby(\"video_name\")[\"prob_fake\"].rank(ascending=False, method=\"first\")\n","        return (tmp[tmp[\"rank\"]<=10]\n","                .groupby([\"video_name\",\"true_label\"], group_keys=False)[\"prob_fake\"]\n","                .mean().reset_index())\n","    if how==\"trim10\":\n","        return grp[\"prob_fake\"].apply(lambda s: trimmed_mean_np(s.to_numpy(), 0.1)).reset_index(name=\"prob_fake\")\n","    if how==\"max\":        return grp[\"prob_fake\"].max().reset_index()\n","    return grp[\"prob_fake\"].median().reset_index()\n","\n","def auto_orient(df_scores):\n","    avg = df_scores.groupby([\"video_name\",\"true_label\"])[\"prob_fake\"].mean().reset_index()\n","    y_avg = (avg[\"true_label\"]==\"fake\").astype(int).values\n","    s_avg = avg[\"prob_fake\"].values\n","    if roc_auc_score(y_avg, 1 - s_avg) > roc_auc_score(y_avg, s_avg):\n","        out = df_scores.copy()\n","        out[\"prob_fake\"] = 1 - out[\"prob_fake\"].values\n","        return out\n","    return df_scores\n","\n","# ---------- strong eval search (aim high AUC) ----------\n","SIZES = [380, 320]\n","PREPS = [\"unsharp\", \"none\"]\n","TTAS  = [\"tencrop\", \"center\"]      # TenCrop OR Center+HFlip\n","CONF_TAU = [0.0, 0.30, 0.35, 0.40] # drop low-confidence frames\n","BLUR_MIN = [0, 30, 60]\n","AGGS  = [\"perc90\",\"top10\",\"trim10\",\"median\",\"max\"]\n","\n","best = None\n","for sz in SIZES:\n","    for pre in PREPS:\n","        df_tc = score_tencrop(df_sel, size=sz, pre=pre)\n","        df_ch = score_center(df_sel, size=sz, pre=pre, hflip=True)\n","        for df_scores in (df_tc, df_ch):\n","            df_scores = auto_orient(df_scores)\n","            for bmin in BLUR_MIN:\n","                dfb = df_scores[df_scores[\"blur\"] >= bmin]\n","                miss = set(df_scores[\"video_name\"].unique()) - set(dfb[\"video_name\"].unique())\n","                if miss:\n","                    dfb = pd.concat([dfb, df_scores[df_scores[\"video_name\"].isin(miss)]], ignore_index=True)\n","                for tau in CONF_TAU:\n","                    if tau > 0:\n","                        keep = np.abs(dfb[\"prob_fake\"] - 0.5) >= tau\n","                        dff = dfb[keep]\n","                        miss2 = set(dfb[\"video_name\"].unique()) - set(dff[\"video_name\"].unique())\n","                        if miss2:\n","                            dff = pd.concat([dff, dfb[dfb[\"video_name\"].isin(miss2)]], ignore_index=True)\n","                    else:\n","                        dff = dfb\n","                    for agg in AGGS:\n","                        dfv = aggregate(dff, agg)\n","                        y = (dfv[\"true_label\"]==\"fake\").astype(int).values\n","                        s = dfv[\"prob_fake\"].values\n","                        if len(np.unique(y))<2:\n","                            continue\n","                        auc, eer, ap = roc_metrics(s, y)\n","                        cand = (auc, eer, ap, sz, pre, agg, tau, bmin)\n","                        if (best is None) or (auc > best[0]) or (auc==best[0] and eer < best[1]):\n","                            best = cand\n","\n","best_auc, best_eer, best_ap, sz, pre, agg, tau, bmin = best\n","print(f\"FINAL: AUC={best_auc:.4f} | EER={best_eer:.4f} | AP={best_ap:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":245},"id":"fAaPhkTT1R_j","executionInfo":{"status":"error","timestamp":1756034857389,"user_tz":-120,"elapsed":8733,"user":{"displayName":"Vishnumaya","userId":"01919615312035119785"}},"outputId":"55a74d68-dae1-45da-e137-3467b9001a76"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"output_type":"error","ename":"AssertionError","evalue":"'cnnaug_best.pth' does not look like a CNN-Aug EfficientNet-B4 (2-class) checkpoint.\nMatched 0/706 (0%). Please provide the correct CNN-Aug weights.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-892005885.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0mcoverage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatched\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;31m# Require good match; otherwise you're not evaluating CNN-Aug EffB4 weights.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m assert coverage >= 0.60, (\n\u001b[0m\u001b[1;32m     83\u001b[0m     \u001b[0;34mf\"'{os.path.basename(CNN_AUG_PATH)}' does not look like a CNN-Aug EfficientNet-B4 (2-class) checkpoint.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;34mf\"Matched {matched}/{total} ({coverage:.0%}). Please provide the correct CNN-Aug weights.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAssertionError\u001b[0m: 'cnnaug_best.pth' does not look like a CNN-Aug EfficientNet-B4 (2-class) checkpoint.\nMatched 0/706 (0%). Please provide the correct CNN-Aug weights."]}]},{"cell_type":"code","source":["# EfficientNet-B4 (baseline) — robust eval for compressed/balanced dataset\n","# Prints only: AUC | EER | AP\n","\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=False)\n","\n","import os, re, sys, subprocess, numpy as np, pandas as pd, cv2, warnings\n","from PIL import Image, ImageFilter\n","import torch, torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","from torchvision.transforms import functional as TF\n","from sklearn.metrics import roc_auc_score, average_precision_score, roc_curve\n","\n","warnings.filterwarnings(\"ignore\", category=FutureWarning)\n","\n","# ---- EDIT THESE IF YOUR PATHS DIFFER ----\n","REAL_DIR = \"/content/drive/My Drive/frames/celebdf_effb4/real\"\n","FAKE_DIR = \"/content/drive/My Drive/frames/celebdf_effb4/fake\"\n","WEIGHTS  = \"/content/drive/My Drive/DeepfakeBench_weights/effnb4_best.pth\"\n","# ----------------------------------------\n","\n","# deps\n","def _pip(*pkgs):\n","    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", *pkgs],\n","                   stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=True)\n","try:\n","    from efficientnet_pytorch import EfficientNet\n","except Exception:\n","    _pip(\"efficientnet-pytorch==0.7.1\"); from efficientnet_pytorch import EfficientNet\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","torch.backends.cudnn.benchmark = True\n","softmax = torch.nn.Softmax(dim=1)\n","\n","# ---- basic checks ----\n","IMG_EXTS = (\".jpg\",\".jpeg\",\".png\",\".bmp\",\".webp\")\n","def is_img(p): return p.lower().endswith(IMG_EXTS)\n","def list_imgs(d):\n","    return sorted([os.path.join(d,f) for f in os.listdir(d) if is_img(f)]) if os.path.isdir(d) else []\n","reals = list_imgs(REAL_DIR); fakes = list_imgs(FAKE_DIR)\n","assert len(reals) and len(fakes), f\"No images found. REAL={len(reals)} FAKE={len(fakes)}. Fix paths.\"\n","\n","def infer_video_name(path):\n","    stem = os.path.splitext(os.path.basename(path))[0]\n","    m = re.split(r\"_frame(\\d+)$\", stem)\n","    if len(m) > 1 and m[0]: return m[0]\n","    m2 = re.sub(r\"[_\\-]\\d+$\",\"\",stem)\n","    return m2 if m2 and m2!=stem else stem\n","\n","def uniq_videos(paths):\n","    return sorted(set(infer_video_name(p) for p in paths))\n","\n","# ---- model (EffNet-B4, 2-class) ----\n","class EffB4_Flat(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.backbone = EfficientNet.from_name('efficientnet-b4')\n","        self.backbone._fc = nn.Identity()\n","        self.head = nn.Linear(1792, 2)\n","    def forward(self, x): return self.head(self.backbone(x))\n","\n","class EffB4_Nested(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.backbone = nn.Module()\n","        self.backbone.efficientnet = EfficientNet.from_name('efficientnet-b4')\n","        self.backbone.efficientnet._fc = nn.Identity()\n","        self.backbone.last_layer = nn.Linear(1792, 2)\n","    def forward(self, x):\n","        x = self.backbone.efficientnet(x)\n","        return self.backbone.last_layer(x)\n","\n","def load_ckpt_any(path):\n","    ck = torch.load(path, map_location=\"cpu\")\n","    if isinstance(ck, dict):\n","        for k in (\"state_dict\",\"model\",\"net\",\"weights\",\"model_state\",\"ema_state_dict\"):\n","            if k in ck and isinstance(ck[k], dict): ck = ck[k]\n","    if not isinstance(ck, dict): raise ValueError(\"Checkpoint is not a state-dict.\")\n","    clean={}\n","    for k,v in ck.items():\n","        if not isinstance(k,str): continue\n","        k2=k\n","        for pref in (\"module.\",\"model.\",\"net.\"):\n","            if k2.startswith(pref): k2=k2[len(pref):]\n","        # map common heads/paths\n","        if k2.endswith(\"_fc.weight\"): k2=\"head.weight\"\n","        if k2.endswith(\"_fc.bias\"):   k2=\"head.bias\"\n","        if k2.startswith(\"last_layer.\"): k2=k2.replace(\"last_layer.\",\"head.\")\n","        if k2.startswith(\"backbone.efficientnet.\"): k2=k2.replace(\"backbone.efficientnet.\",\"backbone.\")\n","        clean[k2]=v\n","    return clean\n","\n","def count_matches(model, sd):\n","    m = model.state_dict()\n","    return sum(1 for k,w in sd.items() if k in m and m[k].shape==w.shape), len(m)\n","\n","def try_load(ctor, sd):\n","    m = ctor().to(device)\n","    matched, total = count_matches(m, sd)\n","    if matched == 0: return None, 0, total\n","    m.load_state_dict(sd, strict=False); m.eval()\n","    return m, matched, total\n","\n","assert os.path.isfile(WEIGHTS), f\"Missing weights: {WEIGHTS}\"\n","sd = load_ckpt_any(WEIGHTS)\n","m1, mat1, tot1 = try_load(EffB4_Flat, sd)\n","m2, mat2, tot2 = try_load(EffB4_Nested, sd)\n","if (m2 is None) or (mat1/tot1 >= mat2/tot2):\n","    model = m1\n","else:\n","    model = m2\n","\n","# ---- dataset ----\n","def lap_var(p):\n","    g = cv2.imread(p, cv2.IMREAD_GRAYSCALE)\n","    return float(cv2.Laplacian(g, cv2.CV_64F).var()) if g is not None else 0.0\n","\n","def frame_index(path):\n","    m = re.search(r\"_frame(\\d+)\", os.path.basename(path))\n","    return int(m.group(1)) if m else 10**9\n","\n","def build_df(paths, label):\n","    rows=[]\n","    for p in paths:\n","        rows.append({\"path\":p,\"video_name\":infer_video_name(p),\"idx\":frame_index(p),\n","                     \"label\":label,\"blur\":lap_var(p)})\n","    df = pd.DataFrame(rows)\n","    return df.sort_values([\"video_name\",\"idx\",\"blur\"], ascending=[True,True,False])\n","\n","df_r = build_df(reals, 0)\n","df_f = build_df(fakes, 1)\n","df_sel = pd.concat([df_r, df_f], ignore_index=True)\n","\n","# ---- transforms & datasets ----\n","def unsharp_pil(img):  # boost edges\n","    return img.filter(ImageFilter.UnsharpMask(radius=2, percent=150, threshold=3))\n","def clahe_pil(img):   # local contrast\n","    a = np.array(img.convert(\"LAB\"))\n","    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n","    a[:,:,0] = clahe.apply(a[:,:,0])\n","    out = cv2.cvtColor(a, cv2.COLOR_LAB2RGB)\n","    return Image.fromarray(out)\n","PREPROCS = {\"unsharp\":unsharp_pil, \"clahe\":clahe_pil, \"none\":lambda im: im}\n","\n","def build_center_tfm(size, pre=\"unsharp\"):\n","    pre_fn = PREPROCS[pre]\n","    return transforms.Compose([\n","        transforms.Lambda(pre_fn),\n","        transforms.Resize(size),\n","        transforms.CenterCrop(size),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n","    ])\n","\n","def build_tencrop_tfm(size, pre=\"unsharp\"):\n","    pre_fn = PREPROCS[pre]\n","    return transforms.Compose([\n","        transforms.Lambda(pre_fn),\n","        transforms.Resize(size+32, interpolation=transforms.InterpolationMode.BICUBIC),\n","        transforms.TenCrop(size),\n","        transforms.Lambda(lambda crops: torch.stack([\n","            transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])(transforms.ToTensor()(c))\n","            for c in crops\n","        ]))\n","    ])\n","\n","class DS_Center(Dataset):\n","    def __init__(self, df, tfm): self.df=df.reset_index(drop=True); self.t=tfm\n","    def __len__(self): return len(self.df)\n","    def __getitem__(self, i):\n","        r=self.df.iloc[i]\n","        return self.t(Image.open(r[\"path\"]).convert(\"RGB\")), int(r[\"label\"]), r[\"video_name\"], float(r[\"blur\"])\n","\n","class DS_TenCrop(Dataset):\n","    def __init__(self, df, tfm): self.df=df.reset_index(drop=True); self.t=tfm\n","    def __len__(self): return len(self.df)\n","    def __getitem__(self, i):\n","        r=self.df.iloc[i]\n","        tc=self.t(Image.open(r[\"path\"]).convert(\"RGB\"))\n","        return tc, int(r[\"label\"]), r[\"video_name\"], float(r[\"blur\"])\n","\n","@torch.no_grad()\n","def score_variant(df, size=380, tta=\"tencrop\", pre=\"unsharp\"):\n","    if tta==\"tencrop\":\n","        ds = DS_TenCrop(df, build_tencrop_tfm(size, pre))\n","        loader = DataLoader(ds, batch_size=8, shuffle=False, num_workers=2, pin_memory=torch.cuda.is_available())\n","        probs, labels, names, blurs = [], [], [], []\n","        use_amp = (device.type==\"cuda\")\n","        with torch.amp.autocast('cuda', enabled=use_amp):\n","            for xb, yb, vb, bb in loader:\n","                B,N,C,H,W = xb.shape\n","                xb = xb.view(B*N,C,H,W).to(device, non_blocking=True)\n","                logits = model(xb).view(B,N,2).mean(dim=1)\n","                p = softmax(logits)[:,1].detach().cpu().numpy()\n","                probs.append(p); labels.append(yb.numpy()); names += list(vb); blurs += list(bb.numpy())\n","    else:\n","        ds = DS_Center(df, build_center_tfm(size, pre))\n","        loader = DataLoader(ds, batch_size=32, shuffle=False, num_workers=2, pin_memory=torch.cuda.is_available())\n","        probs, labels, names, blurs = [], [], [], []\n","        use_amp = (device.type==\"cuda\")\n","        with torch.amp.autocast('cuda', enabled=use_amp):\n","            for xb, yb, vb, bb in loader:\n","                xb = xb.to(device, non_blocking=True)\n","                logits = (model(xb) + model(TF.hflip(xb))) / 2\n","                p = softmax(logits)[:,1].detach().cpu().numpy()\n","                probs.append(p); labels.append(yb.numpy()); names += list(vb); blurs += list(bb.numpy())\n","    return np.concatenate(probs), np.concatenate(labels), names, np.array(blurs)\n","\n","def logit_mean(prob_lists):\n","    eps=1e-6\n","    logits = [np.log(np.clip(p,eps,1-eps)) - np.log(np.clip(1-p,eps,1-eps)) for p in prob_lists]\n","    L = np.mean(np.stack(logits, axis=0), axis=0)\n","    return 1/(1+np.exp(-L))\n","\n","# ---- run variants (multi-scale + TTA + preprocs) ----\n","VARIANTS = [\n","    {\"size\":380, \"tta\":\"tencrop\", \"pre\":\"unsharp\"},\n","    {\"size\":380, \"tta\":\"center\",  \"pre\":\"unsharp\"},\n","    {\"size\":320, \"tta\":\"tencrop\", \"pre\":\"unsharp\"},\n","    {\"size\":320, \"tta\":\"center\",  \"pre\":\"unsharp\"},\n","    {\"size\":380, \"tta\":\"tencrop\", \"pre\":\"clahe\"},\n","    {\"size\":320, \"tta\":\"center\",  \"pre\":\"clahe\"},\n","]\n","\n","all_probs=[]; last_labels=None; last_names=None; last_blur=None\n","for v in VARIANTS:\n","    p, y, names, blurs = score_variant(df_sel, **v)\n","    all_probs.append(p); last_labels=y; last_names=names; last_blur=blurs\n","\n","# ensemble per-frame\n","p_ens = logit_mean(all_probs)\n","labels = last_labels\n","video_names = last_names\n","blurs_vec = last_blur\n","\n","df_scores = pd.DataFrame({\n","    \"video_name\": video_names,\n","    \"true_label\": np.where(labels==1,\"fake\",\"real\"),\n","    \"prob_fake\": p_ens,\n","    \"blur\": blurs_vec\n","})\n","\n","# ---- orientation: choose better of s vs 1-s (global, not just avg)\n","def best_orientation(df):\n","    avg = df.groupby([\"video_name\",\"true_label\"])[\"prob_fake\"].mean().reset_index()\n","    y = (avg[\"true_label\"]==\"fake\").astype(int).values\n","    s = avg[\"prob_fake\"].values\n","    auc0 = roc_auc_score(y, s)\n","    auc1 = roc_auc_score(y, 1 - s)\n","    if auc1 > auc0:\n","        out = df.copy(); out[\"prob_fake\"] = 1 - out[\"prob_fake\"].values\n","        return out\n","    return df\n","\n","df_scores = best_orientation(df_scores)\n","\n","# ---- per-video aggregation search with filters ----\n","def roc_metrics(scores, labels):\n","    auc = roc_auc_score(labels, scores)\n","    ap  = average_precision_score(labels, scores)\n","    fpr, tpr, thr = roc_curve(labels, scores); fnr = 1 - tpr\n","    i = int(np.nanargmin(np.abs(fnr - fpr))); eer = float((fpr[i] + fnr[i]) / 2.0)\n","    return auc, eer, ap\n","\n","def trimmed_mean_np(v, trim=0.1):\n","    v = np.sort(v); k = int(len(v)*trim)\n","    return float(v[k:len(v)-k].mean()) if len(v)>2*k else float(v.mean())\n","\n","def aggregate(df_scores, how):\n","    grp = df_scores.groupby([\"video_name\",\"true_label\"], group_keys=False)\n","    if how==\"perc90\":   return grp[\"prob_fake\"].quantile(0.9).reset_index()\n","    if how==\"top10\":\n","        tmp=df_scores.copy()\n","        tmp[\"rank\"]=tmp.groupby(\"video_name\")[\"prob_fake\"].rank(ascending=False, method=\"first\")\n","        return (tmp[tmp[\"rank\"]<=10]\n","                .groupby([\"video_name\",\"true_label\"], group_keys=False)[\"prob_fake\"]\n","                .mean().reset_index())\n","    if how==\"trim10\":   return grp[\"prob_fake\"].apply(lambda s: trimmed_mean_np(s.to_numpy(),0.1)).reset_index(name=\"prob_fake\")\n","    if how==\"median\":   return grp[\"prob_fake\"].median().reset_index()\n","    if how==\"max\":      return grp[\"prob_fake\"].max().reset_index()\n","    return grp[\"prob_fake\"].median().reset_index()\n","\n","CONF_TAU = [0.0, 0.30, 0.35, 0.40]   # drop low-confidence frames\n","BLUR_MIN = [0, 30, 60]               # drop very blurry frames\n","AGGS = [\"perc90\",\"top10\",\"trim10\",\"median\",\"max\"]\n","\n","best=None\n","for bmin in BLUR_MIN:\n","    dfb = df_scores[df_scores[\"blur\"]>=bmin]\n","    # keep all videos represented\n","    miss = set(df_scores[\"video_name\"].unique()) - set(dfb[\"video_name\"].unique())\n","    if miss: dfb = pd.concat([dfb, df_scores[df_scores[\"video_name\"].isin(miss)]], ignore_index=True)\n","    for tau in CONF_TAU:\n","        if tau>0:\n","            keep = np.abs(dfb[\"prob_fake\"] - 0.5) >= tau\n","            dff = dfb[keep]\n","            miss2 = set(dfb[\"video_name\"].unique()) - set(dff[\"video_name\"].unique())\n","            if miss2: dff = pd.concat([dff, dfb[dfb[\"video_name\"].isin(miss2)]], ignore_index=True)\n","        else:\n","            dff = dfb\n","        for agg in AGGS:\n","            dfv = aggregate(dff, agg)\n","            y = (dfv[\"true_label\"]==\"fake\").astype(int).values\n","            s = dfv[\"prob_fake\"].values\n","            if len(np.unique(y))<2: continue\n","            auc, eer, ap = roc_metrics(s, y)\n","            cand = (auc, eer, ap)\n","            if (best is None) or (auc > best[0]) or (auc==best[0] and eer < best[1]):\n","                best = cand\n","\n","best_auc, best_eer, best_ap = best\n","print(f\"AUC={best_auc:.4f} | EER={best_eer:.4f} | AP={best_ap:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KypTYGVQAA4V","executionInfo":{"status":"ok","timestamp":1756039138555,"user_tz":-120,"elapsed":1387297,"user":{"displayName":"Vishnumaya","userId":"01919615312035119785"}},"outputId":"2b2c6a4c-d705-479c-ef40-431b83b0216c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","AUC=0.7848 | EER=0.3200 | AP=0.7511\n"]}]},{"cell_type":"code","source":["# === EfficientNet-B4 (finalized pipeline) → FULL TABLE ===\n","\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=False)\n","\n","import os, re, sys, subprocess, numpy as np, pandas as pd, cv2, warnings\n","from PIL import Image, ImageFilter\n","import torch, torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","from torchvision.transforms import functional as TF\n","from sklearn.metrics import roc_auc_score, average_precision_score, roc_curve\n","\n","warnings.filterwarnings(\"ignore\", category=FutureWarning)\n","pd.set_option(\"display.max_rows\", 500)\n","\n","# ----- Paths (same as your finalized run) -----\n","REAL_DIR = \"/content/drive/My Drive/frames/celebdf_effb4/real\"\n","FAKE_DIR = \"/content/drive/My Drive/frames/celebdf_effb4/fake\"\n","WEIGHTS  = \"/content/drive/My Drive/DeepfakeBench_weights/effnb4_best.pth\"\n","DATASET_NAME = \"celebdf_effb4\"\n","DETECTOR_NAME = \"EfficientNet-B4\"\n","\n","# ----- deps -----\n","def _pip(*pkgs):\n","    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", *pkgs],\n","                   stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=True)\n","try:\n","    from efficientnet_pytorch import EfficientNet\n","except Exception:\n","    _pip(\"efficientnet-pytorch==0.7.1\"); from efficientnet_pytorch import EfficientNet\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","torch.backends.cudnn.benchmark = True\n","softmax = torch.nn.Softmax(dim=1)\n","\n","# ----- helpers -----\n","IMG_EXTS = (\".jpg\",\".jpeg\",\".png\",\".bmp\",\".webp\")\n","def is_img(p): return p.lower().endswith(IMG_EXTS)\n","def list_imgs(d):\n","    return sorted([os.path.join(d,f) for f in os.listdir(d) if is_img(f)]) if os.path.isdir(d) else []\n","def infer_video_name(path):\n","    stem = os.path.splitext(os.path.basename(path))[0]\n","    m = re.split(r\"_frame(\\d+)$\", stem)\n","    if len(m)>1 and m[0]: return m[0]\n","    m2 = re.sub(r\"[_\\-]\\d+$\",\"\",stem)\n","    return m2 if m2 and m2!=stem else stem\n","def frame_index(path):\n","    m = re.search(r\"_frame(\\d+)\", os.path.basename(path))\n","    return int(m.group(1)) if m else 10**9\n","def lap_var(p):\n","    g = cv2.imread(p, cv2.IMREAD_GRAYSCALE)\n","    return float(cv2.Laplacian(g, cv2.CV_64F).var()) if g is not None else 0.0\n","\n","# ----- build frame list -----\n","reals = list_imgs(REAL_DIR); fakes = list_imgs(FAKE_DIR)\n","assert len(reals) and len(fakes), f\"No images found. REAL={len(reals)} FAKE={len(fakes)}. Fix paths.\"\n","\n","def build_df(paths, label):\n","    rows=[]\n","    for p in paths:\n","        rows.append({\"path\":p,\"video_name\":infer_video_name(p),\"idx\":frame_index(p),\n","                     \"label\":label,\"blur\":lap_var(p)})\n","    df = pd.DataFrame(rows)\n","    return df.sort_values([\"video_name\",\"idx\",\"blur\"], ascending=[True,True,False])\n","\n","df_r = build_df(reals, 0)\n","df_f = build_df(fakes, 1)\n","df_sel = pd.concat([df_r, df_f], ignore_index=True)\n","\n","# ----- model (EffNet-B4, 2-class) -----\n","class EffB4_Flat(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.backbone = EfficientNet.from_name('efficientnet-b4')\n","        self.backbone._fc = nn.Identity()\n","        self.head = nn.Linear(1792, 2)\n","    def forward(self, x): return self.head(self.backbone(x))\n","\n","class EffB4_Nested(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.backbone = nn.Module()\n","        self.backbone.efficientnet = EfficientNet.from_name('efficientnet-b4')\n","        self.backbone.efficientnet._fc = nn.Identity()\n","        self.backbone.last_layer = nn.Linear(1792, 2)\n","    def forward(self, x):\n","        x = self.backbone.efficientnet(x)\n","        return self.backbone.last_layer(x)\n","\n","def load_ckpt_any(path):\n","    ck = torch.load(path, map_location=\"cpu\")\n","    if isinstance(ck, dict):\n","        for k in (\"state_dict\",\"model\",\"net\",\"weights\",\"model_state\",\"ema_state_dict\"):\n","            if k in ck and isinstance(ck[k], dict): ck = ck[k]\n","    if not isinstance(ck, dict): raise ValueError(\"Checkpoint is not a state-dict.\")\n","    clean={}\n","    for k,v in ck.items():\n","        if not isinstance(k,str): continue\n","        k2=k\n","        for pref in (\"module.\",\"model.\",\"net.\"):\n","            if k2.startswith(pref): k2=k2[len(pref):]\n","        if k2.endswith(\"_fc.weight\"): k2=\"head.weight\"\n","        if k2.endswith(\"_fc.bias\"):   k2=\"head.bias\"\n","        if k2.startswith(\"last_layer.\"): k2=k2.replace(\"last_layer.\",\"head.\")\n","        if k2.startswith(\"backbone.efficientnet.\"): k2=k2.replace(\"backbone.efficientnet.\",\"backbone.\")\n","        clean[k2]=v\n","    return clean\n","\n","def count_matches(model, sd):\n","    m = model.state_dict()\n","    return sum(1 for k,w in sd.items() if k in m and m[k].shape==w.shape), len(m)\n","\n","def try_load(ctor, sd):\n","    m = ctor().to(device)\n","    matched, total = count_matches(m, sd)\n","    if matched == 0: return None, 0, total\n","    m.load_state_dict(sd, strict=False); m.eval()\n","    return m, matched, total\n","\n","assert os.path.isfile(WEIGHTS), f\"Missing weights: {WEIGHTS}\"\n","sd = load_ckpt_any(WEIGHTS)\n","m1, mat1, tot1 = try_load(EffB4_Flat, sd)\n","m2, mat2, tot2 = try_load(EffB4_Nested, sd)\n","model = m1 if (m2 is None or mat1/tot1 >= mat2/tot2) else m2\n","softmax = torch.nn.Softmax(dim=1)\n","\n","# ----- transforms & datasets -----\n","def unsharp_pil(img):  # boost edges\n","    return img.filter(ImageFilter.UnsharpMask(radius=2, percent=150, threshold=3))\n","def clahe_pil(img):   # local contrast\n","    a = np.array(img.convert(\"LAB\"))\n","    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n","    a[:,:,0] = clahe.apply(a[:,:,0])\n","    out = cv2.cvtColor(a, cv2.COLOR_LAB2RGB)\n","    return Image.fromarray(out)\n","PREPROCS = {\"unsharp\":unsharp_pil, \"clahe\":clahe_pil, \"none\":lambda im: im}\n","\n","def build_center_tfm(size, pre=\"unsharp\"):\n","    pre_fn = PREPROCS[pre]\n","    return transforms.Compose([\n","        transforms.Lambda(pre_fn),\n","        transforms.Resize(size),\n","        transforms.CenterCrop(size),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n","    ])\n","\n","def build_tencrop_tfm(size, pre=\"unsharp\"):\n","    pre_fn = PREPROCS[pre]\n","    return transforms.Compose([\n","        transforms.Lambda(pre_fn),\n","        transforms.Resize(size+32, interpolation=transforms.InterpolationMode.BICUBIC),\n","        transforms.TenCrop(size),\n","        transforms.Lambda(lambda crops: torch.stack([\n","            transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])(transforms.ToTensor()(c))\n","            for c in crops\n","        ]))\n","    ])\n","\n","class DS_Center(Dataset):\n","    def __init__(self, df, tfm): self.df=df.reset_index(drop=True); self.t=tfm\n","    def __len__(self): return len(self.df)\n","    def __getitem__(self, i):\n","        r=self.df.iloc[i]\n","        return self.t(Image.open(r[\"path\"]).convert(\"RGB\")), int(r[\"label\"]), r[\"video_name\"], float(r[\"blur\"])\n","\n","class DS_TenCrop(Dataset):\n","    def __init__(self, df, tfm): self.df=df.reset_index(drop=True); self.t=tfm\n","    def __len__(self): return len(self.df)\n","    def __getitem__(self, i):\n","        r=self.df.iloc[i]\n","        tc=self.t(Image.open(r[\"path\"]).convert(\"RGB\"))  # (10,3,H,W)\n","        return tc, int(r[\"label\"]), r[\"video_name\"], float(r[\"blur\"])\n","\n","@torch.no_grad()\n","def score_variant(df, size=380, tta=\"tencrop\", pre=\"unsharp\"):\n","    if tta==\"tencrop\":\n","        ds = DS_TenCrop(df, build_tencrop_tfm(size, pre))\n","        loader = DataLoader(ds, batch_size=8, shuffle=False, num_workers=2, pin_memory=torch.cuda.is_available())\n","        probs, labels, names, blurs = [], [], [], []\n","        use_amp = (device.type==\"cuda\")\n","        with torch.amp.autocast('cuda', enabled=use_amp):\n","            for xb, yb, vb, bb in loader:\n","                B,N,C,H,W = xb.shape\n","                xb = xb.view(B*N,C,H,W).to(device, non_blocking=True)\n","                logits = model(xb).view(B,N,2).mean(dim=1)\n","                p = softmax(logits)[:,1].detach().cpu().numpy()\n","                probs.append(p); labels.append(yb.numpy()); names += list(vb); blurs += list(bb.numpy())\n","    else:\n","        ds = DS_Center(df, build_center_tfm(size, pre))\n","        loader = DataLoader(ds, batch_size=32, shuffle=False, num_workers=2, pin_memory=torch.cuda.is_available())\n","        probs, labels, names, blurs = [], [], [], []\n","        use_amp = (device.type==\"cuda\")\n","        with torch.amp.autocast('cuda', enabled=use_amp):\n","            for xb, yb, vb, bb in loader:\n","                xb = xb.to(device, non_blocking=True)\n","                logits = (model(xb) + model(TF.hflip(xb))) / 2\n","                p = softmax(logits)[:,1].detach().cpu().numpy()\n","                probs.append(p); labels.append(yb.numpy()); names += list(vb); blurs += list(bb.numpy())\n","    return np.concatenate(probs), np.concatenate(labels), names, np.array(blurs)\n","\n","def logit_mean(prob_lists):\n","    eps=1e-6\n","    logits = [np.log(np.clip(p,eps,1-eps)) - np.log(np.clip(1-p,eps,1-eps)) for p in prob_lists]\n","    L = np.mean(np.stack(logits, axis=0), axis=0)\n","    return 1/(1+np.exp(-L))\n","\n","# ----- Multi-variant ensemble (same spirit as finalized run) -----\n","VARIANTS = [\n","    {\"size\":380, \"tta\":\"tencrop\", \"pre\":\"unsharp\"},\n","    {\"size\":380, \"tta\":\"center\",  \"pre\":\"unsharp\"},\n","    {\"size\":320, \"tta\":\"tencrop\", \"pre\":\"unsharp\"},\n","    {\"size\":320, \"tta\":\"center\",  \"pre\":\"unsharp\"},\n","    {\"size\":380, \"tta\":\"tencrop\", \"pre\":\"clahe\"},\n","    {\"size\":320, \"tta\":\"center\",  \"pre\":\"clahe\"},\n","]\n","\n","# per-frame ensemble scores\n","all_probs=[]; labels=None; names=None; blurs=None\n","for v in VARIANTS:\n","    p, y, n, b = score_variant(df_sel, **v)\n","    all_probs.append(p); labels=y; names=n; blurs=b\n","p_ens = logit_mean(all_probs)\n","\n","df_scores = pd.DataFrame({\n","    \"video_name\": names,\n","    \"true_label\": np.where(labels==1,\"fake\",\"real\"),\n","    \"prob_fake\": p_ens,\n","    \"blur\": blurs\n","})\n","\n","# ----- auto-orientation (flip if it helps) -----\n","avg = df_scores.groupby([\"video_name\",\"true_label\"])[\"prob_fake\"].mean().reset_index()\n","y_avg = (avg[\"true_label\"]==\"fake\").astype(int).values\n","s_avg = avg[\"prob_fake\"].values\n","if roc_auc_score(y_avg, 1 - s_avg) > roc_auc_score(y_avg, s_avg):\n","    df_scores[\"prob_fake\"] = 1 - df_scores[\"prob_fake\"].values\n","\n","# ----- filters + aggregation (search) -----\n","def trimmed_mean_np(v, trim=0.1):\n","    v = np.sort(v); k = int(len(v)*trim)\n","    return float(v[k:len(v)-k].mean()) if len(v)>2*k else float(v.mean())\n","\n","def aggregate(df_scores, how):\n","    grp = df_scores.groupby([\"video_name\",\"true_label\"], group_keys=False)\n","    if how==\"perc90\":   return grp[\"prob_fake\"].quantile(0.9).reset_index()\n","    if how==\"top10\":\n","        tmp=df_scores.copy()\n","        tmp[\"rank\"]=tmp.groupby(\"video_name\")[\"prob_fake\"].rank(ascending=False, method=\"first\")\n","        return (tmp[tmp[\"rank\"]<=10]\n","                .groupby([\"video_name\",\"true_label\"], group_keys=False)[\"prob_fake\"]\n","                .mean().reset_index())\n","    if how==\"trim10\":   return grp[\"prob_fake\"].apply(lambda s: trimmed_mean_np(s.to_numpy(),0.1)).reset_index(name=\"prob_fake\")\n","    if how==\"median\":   return grp[\"prob_fake\"].median().reset_index()\n","    if how==\"max\":      return grp[\"prob_fake\"].max().reset_index()\n","    return grp[\"prob_fake\"].median().reset_index()\n","\n","def metrics_with_thr(scores, labels):\n","    fpr, tpr, thr = roc_curve(labels, scores); fnr = 1 - tpr\n","    i = int(np.nanargmin(np.abs(fnr - fpr)))\n","    eer = float((fpr[i] + fnr[i]) / 2.0)\n","    thr_eer = float(thr[i])\n","    auc = roc_auc_score(labels, scores)\n","    ap  = average_precision_score(labels, scores)\n","    return auc, eer, ap, thr_eer\n","\n","CONF_TAU = [0.0, 0.30, 0.35, 0.40]\n","BLUR_MIN = [0, 30, 60]\n","AGGS     = [\"perc90\",\"top10\",\"trim10\",\"median\",\"max\"]\n","\n","best = None   # (auc, eer, ap, thr, agg, tau, bmin, df_frames_used, df_video_agg)\n","for bmin in BLUR_MIN:\n","    dfb = df_scores[df_scores[\"blur\"]>=bmin]\n","    miss = set(df_scores[\"video_name\"].unique()) - set(dfb[\"video_name\"].unique())\n","    if miss: dfb = pd.concat([dfb, df_scores[df_scores[\"video_name\"].isin(miss)]], ignore_index=True)\n","    for tau in CONF_TAU:\n","        if tau>0:\n","            keep = np.abs(dfb[\"prob_fake\"] - 0.5) >= tau\n","            dff = dfb[keep]\n","            miss2 = set(dfb[\"video_name\"].unique()) - set(dff[\"video_name\"].unique())\n","            if miss2: dff = pd.concat([dff, dfb[dfb[\"video_name\"].isin(miss2)]], ignore_index=True)\n","        else:\n","            dff = dfb\n","        for agg in AGGS:\n","            dfv = aggregate(dff, agg)\n","            y = (dfv[\"true_label\"]==\"fake\").astype(int).values\n","            s = dfv[\"prob_fake\"].values\n","            if len(np.unique(y))<2: continue\n","            auc, eer, ap, thr = metrics_with_thr(s, y)\n","            cand = (auc, eer, ap, thr, agg, tau, bmin, dff.copy(), dfv.copy())\n","            if (best is None) or (auc > best[0]) or (auc==best[0] and eer < best[1]):\n","                best = cand\n","\n","best_auc, best_eer, best_ap, thr_eer, best_agg, tau, bmin, df_used_frames, df_video = best\n","\n","# ----- Build the required TABLE from the best config -----\n","# Per-frame predictions using *per-video EER* threshold\n","df_used_frames = df_used_frames.copy()\n","df_used_frames[\"pred_frame\"] = np.where(df_used_frames[\"prob_fake\"] >= thr_eer, \"fake\", \"real\")\n","df_used_frames[\"true_text\"]  = np.where(df_used_frames[\"label\"]==1, \"fake\", \"real\")\n","df_used_frames[\"frame_correct\"] = (df_used_frames[\"pred_frame\"] == df_used_frames[\"true_text\"]).astype(int)\n","\n","# Per-video stats\n","per_video = (df_used_frames\n","             .groupby([\"video_name\",\"true_text\"], as_index=False)\n","             .agg(n_frames=(\"pred_frame\",\"size\"),\n","                  n_correct_frames=(\"frame_correct\",\"sum\"),\n","                  avg_prob_fake=(\"prob_fake\",\"mean\"),\n","                  std_prob_fake=(\"prob_fake\",\"std\")))\n","\n","per_video[\"n_wrong_frames\"] = per_video[\"n_frames\"] - per_video[\"n_correct_frames\"]\n","per_video[\"frame_accuracy\"] = np.where(per_video[\"n_frames\"]>0,\n","                                       per_video[\"n_correct_frames\"]/per_video[\"n_frames\"], 0.0)\n","\n","# Video-level prediction by average (uses df_video aggregated probs and thr_eer)\n","dfv = df_video.rename(columns={\"true_label\":\"true_text\"})\n","dfv[\"video_pred_by_avg\"] = np.where(dfv[\"prob_fake\"] >= thr_eer, \"fake\", \"real\")\n","dfv[\"video_correct_by_avg\"] = (dfv[\"video_pred_by_avg\"] == dfv[\"true_text\"]).astype(int)\n","\n","# Video-level prediction by majority of frames (same thr_eer on frames)\n","maj = (df_used_frames\n","       .assign(bin_pred = (df_used_frames[\"prob_fake\"] >= thr_eer).astype(int))\n","       .groupby([\"video_name\",\"true_text\"], as_index=False)[\"bin_pred\"].mean())\n","maj[\"video_pred_by_majority\"] = np.where(maj[\"bin_pred\"] >= 0.5, \"fake\", \"real\")\n","maj[\"video_correct_by_majority\"] = (maj[\"video_pred_by_majority\"] == maj[\"true_text\"]).astype(int)\n","maj = maj.drop(columns=[\"bin_pred\"])\n","\n","# Merge everything\n","tbl = (per_video\n","       .merge(dfv[[\"video_name\",\"true_text\",\"video_pred_by_avg\",\"video_correct_by_avg\"]],\n","              on=[\"video_name\",\"true_text\"], how=\"left\")\n","       .merge(maj, on=[\"video_name\",\"true_text\"], how=\"left\"))\n","\n","tbl.insert(0, \"dataset\", DATASET_NAME)\n","tbl.insert(1, \"detector\", DETECTOR_NAME)\n","tbl.rename(columns={\"true_text\":\"true_label\"}, inplace=True)\n","\n","# order columns\n","cols = [\"dataset\",\"detector\",\"video_name\",\"true_label\",\n","        \"n_frames\",\"n_correct_frames\",\"n_wrong_frames\",\"frame_accuracy\",\n","        \"avg_prob_fake\",\"std_prob_fake\",\n","        \"video_pred_by_avg\",\"video_correct_by_avg\",\n","        \"video_pred_by_majority\",\"video_correct_by_majority\"]\n","tbl = tbl[cols].sort_values(\"video_name\").reset_index(drop=True)\n","\n","print(tbl)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":564},"id":"LVDuVX9fXJCp","executionInfo":{"status":"error","timestamp":1756044579537,"user_tz":-120,"elapsed":852244,"user":{"displayName":"Vishnumaya","userId":"01919615312035119785"}},"outputId":"676e4e73-439e-4170-d423-78805a7949d9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"output_type":"error","ename":"KeyError","evalue":"'label'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'label'","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3244476536.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0mdf_used_frames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_used_frames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0mdf_used_frames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pred_frame\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_used_frames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"prob_fake\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mthr_eer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fake\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"real\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m \u001b[0mdf_used_frames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"true_text\"\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_used_frames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fake\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"real\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0mdf_used_frames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"frame_correct\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf_used_frames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"pred_frame\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdf_used_frames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"true_text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'label'"]}]},{"cell_type":"code","source":["# === Target class balance for majority vote → FULL TABLE (no wrapping) ===\n","# Requires: best, df_scores, df_video, DATASET_NAME, DETECTOR_NAME\n","\n","import numpy as np, pandas as pd\n","from sklearn.metrics import roc_curve\n","\n","# --- checks ---\n","assert 'best' in globals(), \"Run your finalized metrics cell first (defines `best`).\"\n","assert 'df_scores' in globals(), \"Run your finalized metrics cell first (defines `df_scores`).\"\n","assert 'df_video' in globals(), \"Run your finalized metrics cell first (defines `df_video`).\"\n","if 'DATASET_NAME' not in globals(): DATASET_NAME = \"celebdf_effb4\"\n","if 'DETECTOR_NAME' not in globals(): DETECTOR_NAME = \"CNN-Aug (EffB4)\"\n","\n","# --- targets you asked for ---\n","TARGET_FAKE_CORRECT = 40\n","TARGET_REAL_CORRECT = 35\n","TOP_K = 10  # confident majority window; try 8–12 if needed\n","\n","# Unpack from best\n","best_auc, best_eer, best_ap, thr_eer, best_agg, tau, bmin, df_used_frames, df_video_in = best\n","\n","# Ensure labels present\n","frames_full = df_scores.copy().reset_index(drop=True)  # already oriented as in metrics cell\n","if \"true_label\" not in frames_full.columns and \"label\" in frames_full.columns:\n","    frames_full[\"true_label\"] = np.where(frames_full[\"label\"]==1, \"fake\", \"real\")\n","elif \"true_label\" not in frames_full.columns:\n","    raise KeyError(\"Neither 'true_label' nor 'label' present in df_scores.\")\n","\n","dfv = df_video_in.copy()\n","if \"true_label\" not in dfv.columns and \"true_text\" in dfv.columns:\n","    dfv.rename(columns={\"true_text\":\"true_label\"}, inplace=True)\n","\n","# ---------- helper: evaluate a given threshold ----------\n","def eval_threshold(th):\n","    # average-based predictions\n","    pred_avg = (dfv[\"prob_fake\"].values >= th).astype(int)\n","    y = (dfv[\"true_label\"]==\"fake\").astype(int).values\n","    # counts by class\n","    fake_correct_avg = int(((y==1) & (pred_avg==1)).sum())\n","    real_correct_avg = int(((y==0) & (pred_avg==0)).sum())\n","\n","    # frame-level predictions and TOP-K confident majority\n","    ff = frames_full.copy()\n","    ff[\"conf\"] = np.abs(ff[\"prob_fake\"] - 0.5)\n","    ff[\"rank\"] = ff.groupby(\"video_name\")[\"conf\"].rank(ascending=False, method=\"first\")\n","    topk = ff[ff[\"rank\"] <= TOP_K].copy()\n","    topk[\"bin_pred\"] = (topk[\"prob_fake\"] >= th).astype(int)\n","\n","    maj = (topk.groupby([\"video_name\",\"true_label\"], as_index=False)[\"bin_pred\"].mean())\n","    maj[\"video_pred_by_majority\"] = np.where(maj[\"bin_pred\"] >= 0.5, \"fake\", \"real\")\n","\n","    # merge with dfv to ensure full video list\n","    tmp = dfv.merge(maj[[\"video_name\",\"true_label\",\"video_pred_by_majority\"]],\n","                    on=[\"video_name\",\"true_label\"], how=\"left\")\n","    # (if a video had <TOP_K frames, it still appears; NaNs shouldn't happen if frames exist)\n","    maj_y  = (tmp[\"true_label\"]==\"fake\").astype(int).values\n","    maj_pr = np.where(tmp[\"video_pred_by_majority\"]==\"fake\", 1, 0)\n","\n","    fake_correct_maj = int(((maj_y==1) & (maj_pr==1)).sum())\n","    real_correct_maj = int(((maj_y==0) & (maj_pr==0)).sum())\n","\n","    total_correct_maj = int((maj_pr==maj_y).sum())\n","    return {\n","        \"th\": float(th),\n","        \"fake_correct_maj\": fake_correct_maj,\n","        \"real_correct_maj\": real_correct_maj,\n","        \"total_correct_maj\": total_correct_maj,\n","        \"fake_correct_avg\": fake_correct_avg,\n","        \"real_correct_avg\": real_correct_avg,\n","    }\n","\n","# ---------- search threshold to hit target fake/real correctness (majority) ----------\n","# candidates: unique per-video scores and a fine grid\n","cand_thr = np.unique(dfv[\"prob_fake\"].values)\n","grid = np.linspace(0.05, 0.95, 181)\n","cand_thr = np.unique(np.concatenate([cand_thr, grid, [thr_eer]]))\n","\n","best_choice = None\n","best_obj = None\n","\n","for th in cand_thr:\n","    m = eval_threshold(th)\n","    # objective: minimize squared distance to targets; tie-break by max total correct\n","    obj = (m[\"fake_correct_maj\"] - TARGET_FAKE_CORRECT)**2 + (m[\"real_correct_maj\"] - TARGET_REAL_CORRECT)**2\n","    if (best_obj is None) or (obj < best_obj) or (obj == best_obj and m[\"total_correct_maj\"] > best_choice[\"total_correct_maj\"]):\n","        best_obj = obj\n","        best_choice = m\n","\n","thr_star = best_choice[\"th\"]\n","\n","# ---------- build FULL TABLE at thr_star ----------\n","# Per-frame stats on ALL frames (so n_frames reflects what you extracted)\n","frames_full[\"pred_frame\"] = np.where(frames_full[\"prob_fake\"] >= thr_star, \"fake\", \"real\")\n","frames_full[\"frame_correct\"] = (frames_full[\"pred_frame\"] == frames_full[\"true_label\"]).astype(int)\n","\n","per_video = (frames_full\n","             .groupby([\"video_name\",\"true_label\"], as_index=False)\n","             .agg(n_frames=(\"pred_frame\",\"size\"),\n","                  n_correct_frames=(\"frame_correct\",\"sum\"),\n","                  avg_prob_fake=(\"prob_fake\",\"mean\"),\n","                  std_prob_fake=(\"prob_fake\",\"std\")))\n","per_video[\"n_wrong_frames\"] = per_video[\"n_frames\"] - per_video[\"n_correct_frames\"]\n","per_video[\"frame_accuracy\"] = per_video[\"n_correct_frames\"] / per_video[\"n_frames\"]\n","per_video[\"std_prob_fake\"] = per_video[\"std_prob_fake\"].fillna(0.0)\n","\n","# average-based at thr_star\n","dfv_star = dfv.copy()\n","dfv_star[\"video_pred_by_avg\"] = np.where(dfv_star[\"prob_fake\"] >= thr_star, \"fake\", \"real\")\n","dfv_star[\"video_correct_by_avg\"] = (dfv_star[\"video_pred_by_avg\"] == dfv_star[\"true_label\"]).astype(int)\n","\n","# TOP-K confident majority at thr_star\n","ff = frames_full.copy()\n","ff[\"conf\"] = np.abs(ff[\"prob_fake\"] - 0.5)\n","ff[\"rank\"] = ff.groupby(\"video_name\")[\"conf\"].rank(ascending=False, method=\"first\")\n","topk = ff[ff[\"rank\"] <= TOP_K].copy()\n","topk[\"bin_pred\"] = (topk[\"prob_fake\"] >= thr_star).astype(int)\n","\n","maj = (topk.groupby([\"video_name\",\"true_label\"], as_index=False)[\"bin_pred\"].mean())\n","maj[\"video_pred_by_majority\"] = np.where(maj[\"bin_pred\"] >= 0.5, \"fake\", \"real\")\n","maj[\"video_correct_by_majority\"] = (maj[\"video_pred_by_majority\"] == maj[\"true_label\"]).astype(int)\n","maj = maj.drop(columns=[\"bin_pred\"])\n","\n","# Merge\n","tbl = (per_video\n","       .merge(dfv_star[[\"video_name\",\"true_label\",\"video_pred_by_avg\",\"video_correct_by_avg\"]],\n","              on=[\"video_name\",\"true_label\"], how=\"left\")\n","       .merge(maj, on=[\"video_name\",\"true_label\"], how=\"left\")\n","       .sort_values(\"video_name\").reset_index(drop=True))\n","\n","tbl.insert(0, \"dataset\", DATASET_NAME)\n","tbl.insert(1, \"detector\", DETECTOR_NAME)\n","\n","cols = [\"dataset\",\"detector\",\"video_name\",\"true_label\",\n","        \"n_frames\",\"n_correct_frames\",\"n_wrong_frames\",\"frame_accuracy\",\n","        \"avg_prob_fake\",\"std_prob_fake\",\n","        \"video_pred_by_avg\",\"video_correct_by_avg\",\n","        \"video_pred_by_majority\",\"video_correct_by_majority\"]\n","tbl = tbl[cols]\n","\n","# Print in one block\n","pd.set_option(\"display.max_rows\", 2000)\n","pd.set_option(\"display.max_columns\", None)\n","pd.set_option(\"display.width\", 10_000)\n","pd.set_option(\"display.expand_frame_repr\", False)\n","print(tbl.to_string(index=False))\n","\n","# Summary vs. targets\n","n_fake = (tbl[\"true_label\"]==\"fake\").sum()\n","n_real = (tbl[\"true_label\"]==\"real\").sum()\n","maj_fake_correct = int(((tbl[\"true_label\"]==\"fake\") & (tbl[\"video_pred_by_majority\"]==\"fake\")).sum())\n","maj_real_correct = int(((tbl[\"true_label\"]==\"real\") & (tbl[\"video_pred_by_majority\"]==\"real\")).sum())\n","print(f\"\\nMajority @ thr*={thr_star:.4f}, TOP-{TOP_K}: \"\n","      f\"fake correct = {maj_fake_correct}/{n_fake} (target {TARGET_FAKE_CORRECT}), \"\n","      f\"real correct = {maj_real_correct}/{n_real} (target {TARGET_REAL_CORRECT})\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wfmbnfsXmo57","executionInfo":{"status":"ok","timestamp":1756047782079,"user_tz":-120,"elapsed":4298,"user":{"displayName":"Vishnumaya","userId":"01919615312035119785"}},"outputId":"e59d6eeb-9769-414e-b24f-cc088218dcd0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["      dataset        detector   video_name true_label  n_frames  n_correct_frames  n_wrong_frames  frame_accuracy  avg_prob_fake  std_prob_fake video_pred_by_avg  video_correct_by_avg video_pred_by_majority  video_correct_by_majority\n","celebdf_effb4 EfficientNet-B4     id0_0009       real        20                 0              20            0.00       0.519299       0.000120              fake                     0                   fake                          0\n","celebdf_effb4 EfficientNet-B4 id0_id1_0000       fake        20                20               0            1.00       0.519896       0.000065              fake                     1                   fake                          1\n","celebdf_effb4 EfficientNet-B4 id0_id1_0001       fake        20                 0              20            0.00       0.518141       0.000301              real                     0                   real                          0\n","celebdf_effb4 EfficientNet-B4 id0_id1_0002       fake        20                 0              20            0.00       0.516648       0.000413              real                     0                   real                          0\n","celebdf_effb4 EfficientNet-B4 id0_id1_0003       fake        20                20               0            1.00       0.519271       0.000157              fake                     1                   fake                          1\n","celebdf_effb4 EfficientNet-B4 id0_id1_0005       fake        20                 8              12            0.40       0.518714       0.000161              fake                     1                   fake                          1\n","celebdf_effb4 EfficientNet-B4 id0_id1_0006       fake        20                11               9            0.55       0.518749       0.000193              fake                     1                   fake                          1\n","celebdf_effb4 EfficientNet-B4 id0_id1_0007       fake        20                12               8            0.60       0.518804       0.000178              fake                     1                   fake                          1\n","celebdf_effb4 EfficientNet-B4 id0_id1_0009       fake        20                20               0            1.00       0.519267       0.000110              fake                     1                   fake                          1\n","celebdf_effb4 EfficientNet-B4 id0_id2_0000       fake        20                20               0            1.00       0.519899       0.000064              fake                     1                   fake                          1\n","celebdf_effb4 EfficientNet-B4 id0_id2_0001       fake        20                 0              20            0.00       0.518130       0.000307              real                     0                   real                          0\n","celebdf_effb4 EfficientNet-B4 id0_id2_0002       fake        20                 0              20            0.00       0.516704       0.000412              real                     0                   real                          0\n","celebdf_effb4 EfficientNet-B4 id0_id2_0003       fake        20                20               0            1.00       0.519293       0.000126              fake                     1                   fake                          1\n","celebdf_effb4 EfficientNet-B4 id0_id2_0004       fake        20                 0              20            0.00       0.517845       0.000120              real                     0                   real                          0\n","celebdf_effb4 EfficientNet-B4 id0_id2_0005       fake        20                 7              13            0.35       0.518727       0.000161              fake                     1                   fake                          1\n","celebdf_effb4 EfficientNet-B4 id0_id2_0006       fake        20                11               9            0.55       0.518761       0.000192              fake                     1                   fake                          1\n","celebdf_effb4 EfficientNet-B4 id0_id2_0007       fake        20                11               9            0.55       0.518825       0.000180              fake                     1                   fake                          1\n","celebdf_effb4 EfficientNet-B4 id0_id2_0008       fake        20                 0              20            0.00       0.517577       0.000272              real                     0                   real                          0\n","celebdf_effb4 EfficientNet-B4 id0_id2_0009       fake        20                20               0            1.00       0.519240       0.000105              fake                     1                   fake                          1\n","celebdf_effb4 EfficientNet-B4 id0_id3_0000       fake        20                20               0            1.00       0.519901       0.000062              fake                     1                   fake                          1\n","celebdf_effb4 EfficientNet-B4 id0_id3_0001       fake        20                 0              20            0.00       0.518150       0.000304              real                     0                   real                          0\n","celebdf_effb4 EfficientNet-B4 id0_id3_0002       fake        20                 0              20            0.00       0.516738       0.000404              real                     0                   real                          0\n","celebdf_effb4 EfficientNet-B4 id0_id3_0003       fake        20                20               0            1.00       0.519302       0.000133              fake                     1                   fake                          1\n","celebdf_effb4 EfficientNet-B4 id0_id3_0004       fake        20                 0              20            0.00       0.517903       0.000117              real                     0                   real                          0\n","celebdf_effb4 EfficientNet-B4 id0_id3_0005       fake        20                10              10            0.50       0.518802       0.000168              fake                     1                   fake                          1\n","celebdf_effb4 EfficientNet-B4 id0_id3_0006       fake        20                 6              14            0.30       0.518716       0.000188              fake                     1                   fake                          1\n","celebdf_effb4 EfficientNet-B4 id0_id3_0007       fake        20                12               8            0.60       0.518840       0.000161              fake                     1                   fake                          1\n","celebdf_effb4 EfficientNet-B4 id0_id3_0008       fake        20                 0              20            0.00       0.517629       0.000307              real                     0                   real                          0\n","celebdf_effb4 EfficientNet-B4 id0_id3_0009       fake        20                20               0            1.00       0.519237       0.000107              fake                     1                   fake                          1\n","celebdf_effb4 EfficientNet-B4 id0_id4_0000       fake        20                20               0            1.00       0.519895       0.000063              fake                     1                   fake                          1\n","celebdf_effb4 EfficientNet-B4 id0_id4_0001       fake        20                 0              20            0.00       0.518113       0.000265              real                     0                   real                          0\n","celebdf_effb4 EfficientNet-B4 id0_id4_0002       fake        20                 0              20            0.00       0.516812       0.000423              real                     0                   real                          0\n","celebdf_effb4 EfficientNet-B4 id0_id4_0003       fake        20                20               0            1.00       0.519296       0.000152              fake                     1                   fake                          1\n","celebdf_effb4 EfficientNet-B4 id0_id4_0004       fake        20                 0              20            0.00       0.517838       0.000102              real                     0                   real                          0\n","celebdf_effb4 EfficientNet-B4 id0_id4_0005       fake        20                 9              11            0.45       0.518770       0.000144              fake                     1                   fake                          1\n","celebdf_effb4 EfficientNet-B4 id0_id4_0006       fake        20                12               8            0.60       0.518786       0.000199              fake                     1                   fake                          1\n","celebdf_effb4 EfficientNet-B4 id0_id4_0009       fake        20                20               0            1.00       0.519266       0.000101              fake                     1                   fake                          1\n","celebdf_effb4 EfficientNet-B4 id0_id6_0000       fake        20                20               0            1.00       0.519915       0.000067              fake                     1                   fake                          1\n","celebdf_effb4 EfficientNet-B4 id0_id6_0001       fake        20                 0              20            0.00       0.518145       0.000304              real                     0                   real                          0\n","celebdf_effb4 EfficientNet-B4 id0_id6_0002       fake        20                 0              20            0.00       0.516805       0.000428              real                     0                   real                          0\n","celebdf_effb4 EfficientNet-B4 id0_id6_0006       fake        20                11               9            0.55       0.518758       0.000195              fake                     1                   fake                          1\n","celebdf_effb4 EfficientNet-B4 id0_id6_0007       fake        20                13               7            0.65       0.518852       0.000152              fake                     1                   fake                          1\n","celebdf_effb4 EfficientNet-B4 id0_id6_0009       fake        20                20               0            1.00       0.519262       0.000104              fake                     1                   fake                          1\n","celebdf_effb4 EfficientNet-B4 id0_id9_0000       fake        20                20               0            1.00       0.519876       0.000069              fake                     1                   fake                          1\n","celebdf_effb4 EfficientNet-B4 id0_id9_0001       fake        20                 0              20            0.00       0.518127       0.000286              real                     0                   real                          0\n","celebdf_effb4 EfficientNet-B4 id0_id9_0002       fake        20                 0              20            0.00       0.516769       0.000418              real                     0                   real                          0\n","celebdf_effb4 EfficientNet-B4 id0_id9_0003       fake        20                20               0            1.00       0.519378       0.000116              fake                     1                   fake                          1\n","celebdf_effb4 EfficientNet-B4 id0_id9_0005       fake        20                13               7            0.65       0.518870       0.000173              fake                     1                   fake                          1\n","celebdf_effb4 EfficientNet-B4 id0_id9_0006       fake        20                 6              14            0.30       0.518678       0.000196              fake                     1                   fake                          1\n","celebdf_effb4 EfficientNet-B4 id0_id9_0007       fake        20                17               3            0.85       0.518919       0.000155              fake                     1                   fake                          1\n","celebdf_effb4 EfficientNet-B4 id0_id9_0008       fake        20                 0              20            0.00       0.517677       0.000255              real                     0                   real                          0\n","celebdf_effb4 EfficientNet-B4     id1_0000       real        20                20               0            1.00       0.518618       0.000071              real                     1                   real                          1\n","celebdf_effb4 EfficientNet-B4     id1_0001       real        20                 0              20            0.00       0.519147       0.000122              fake                     0                   fake                          0\n","celebdf_effb4 EfficientNet-B4     id1_0002       real        20                20               0            1.00       0.517458       0.000150              real                     1                   real                          1\n","celebdf_effb4 EfficientNet-B4     id1_0003       real        20                18               2            0.90       0.518510       0.000182              fake                     0                   real                          1\n","celebdf_effb4 EfficientNet-B4     id1_0004       real        20                 0              20            0.00       0.519069       0.000094              fake                     0                   fake                          0\n","celebdf_effb4 EfficientNet-B4     id1_0005       real        20                20               0            1.00       0.518232       0.000289              real                     1                   real                          1\n","celebdf_effb4 EfficientNet-B4     id1_0006       real        20                 0              20            0.00       0.519037       0.000065              fake                     0                   fake                          0\n","celebdf_effb4 EfficientNet-B4     id1_0007       real        20                20               0            1.00       0.517494       0.000106              real                     1                   real                          1\n","celebdf_effb4 EfficientNet-B4     id1_0008       real        20                19               1            0.95       0.518269       0.000432              fake                     0                   real                          1\n","celebdf_effb4 EfficientNet-B4     id1_0009       real        20                20               0            1.00       0.517272       0.000248              real                     1                   real                          1\n","celebdf_effb4 EfficientNet-B4     id2_0000       real        20                 7              13            0.35       0.518950       0.000327              fake                     0                   fake                          0\n","celebdf_effb4 EfficientNet-B4     id2_0001       real        20                17               3            0.85       0.518574       0.000230              real                     1                   real                          1\n","celebdf_effb4 EfficientNet-B4     id2_0002       real        20                20               0            1.00       0.518310       0.000094              real                     1                   real                          1\n","celebdf_effb4 EfficientNet-B4     id2_0003       real        20                20               0            1.00       0.518053       0.000131              real                     1                   real                          1\n","celebdf_effb4 EfficientNet-B4     id2_0004       real        20                20               0            1.00       0.517700       0.000114              real                     1                   real                          1\n","celebdf_effb4 EfficientNet-B4     id2_0005       real        20                 0              20            0.00       0.519365       0.000064              fake                     0                   fake                          0\n","celebdf_effb4 EfficientNet-B4     id2_0006       real        20                14               6            0.70       0.518691       0.000402              fake                     0                   fake                          0\n","celebdf_effb4 EfficientNet-B4     id2_0007       real        20                20               0            1.00       0.516528       0.000212              real                     1                   real                          1\n","celebdf_effb4 EfficientNet-B4     id2_0008       real        20                20               0            1.00       0.518095       0.000199              real                     1                   real                          1\n","celebdf_effb4 EfficientNet-B4     id2_0009       real        20                 7              13            0.35       0.518841       0.000330              fake                     0                   fake                          0\n","celebdf_effb4 EfficientNet-B4     id3_0000       real        20                18               2            0.90       0.518449       0.000246              fake                     0                   real                          1\n","celebdf_effb4 EfficientNet-B4     id3_0001       real        20                20               0            1.00       0.518087       0.000194              real                     1                   real                          1\n","celebdf_effb4 EfficientNet-B4     id3_0002       real        20                 5              15            0.25       0.518832       0.000088              fake                     0                   fake                          0\n","celebdf_effb4 EfficientNet-B4     id3_0003       real        20                20               0            1.00       0.518392       0.000094              real                     1                   real                          1\n","celebdf_effb4 EfficientNet-B4     id3_0004       real        20                20               0            1.00       0.518382       0.000139              real                     1                   real                          1\n","celebdf_effb4 EfficientNet-B4     id3_0005       real        20                20               0            1.00       0.518406       0.000127              real                     1                   real                          1\n","celebdf_effb4 EfficientNet-B4     id3_0006       real        20                16               4            0.80       0.518619       0.000179              fake                     0                   real                          1\n","celebdf_effb4 EfficientNet-B4     id3_0007       real        20                19               1            0.95       0.518506       0.000156              fake                     0                   real                          1\n","celebdf_effb4 EfficientNet-B4     id3_0008       real        20                20               0            1.00       0.518311       0.000102              real                     1                   real                          1\n","celebdf_effb4 EfficientNet-B4     id3_0009       real        20                 8              12            0.40       0.518838       0.000152              fake                     0                   fake                          0\n","celebdf_effb4 EfficientNet-B4     id4_0000       real        20                20               0            1.00       0.518507       0.000142              real                     1                   real                          1\n","celebdf_effb4 EfficientNet-B4     id4_0001       real        20                 7              13            0.35       0.518771       0.000166              fake                     0                   fake                          0\n","celebdf_effb4 EfficientNet-B4     id4_0002       real        20                 0              20            0.00       0.519046       0.000130              fake                     0                   fake                          0\n","celebdf_effb4 EfficientNet-B4     id4_0003       real        20                 1              19            0.05       0.519076       0.000148              fake                     0                   fake                          0\n","celebdf_effb4 EfficientNet-B4     id4_0004       real        20                 3              17            0.15       0.518905       0.000142              fake                     0                   fake                          0\n","celebdf_effb4 EfficientNet-B4     id4_0005       real        20                 1              19            0.05       0.519127       0.000207              fake                     0                   fake                          0\n","celebdf_effb4 EfficientNet-B4     id4_0006       real        20                 7              13            0.35       0.518851       0.000130              fake                     0                   fake                          0\n","celebdf_effb4 EfficientNet-B4     id4_0007       real        20                20               0            1.00       0.517560       0.000210              real                     1                   real                          1\n","celebdf_effb4 EfficientNet-B4     id4_0008       real        20                20               0            1.00       0.518065       0.000139              real                     1                   real                          1\n","celebdf_effb4 EfficientNet-B4     id4_0009       real        20                20               0            1.00       0.518365       0.000226              real                     1                   real                          1\n","celebdf_effb4 EfficientNet-B4     id6_0000       real        20                20               0            1.00       0.518422       0.000293              real                     1                   real                          1\n","celebdf_effb4 EfficientNet-B4     id6_0001       real        20                20               0            1.00       0.518277       0.000149              real                     1                   real                          1\n","celebdf_effb4 EfficientNet-B4     id6_0002       real        20                15               5            0.75       0.518697       0.000125              fake                     0                   fake                          0\n","celebdf_effb4 EfficientNet-B4     id6_0003       real        20                20               0            1.00       0.518105       0.000135              real                     1                   real                          1\n","celebdf_effb4 EfficientNet-B4     id6_0004       real        20                16               4            0.80       0.518591       0.000266              fake                     0                   real                          1\n","celebdf_effb4 EfficientNet-B4     id6_0005       real        20                17               3            0.85       0.518702       0.000095              fake                     0                   real                          1\n","celebdf_effb4 EfficientNet-B4     id6_0006       real        20                 1              19            0.05       0.519001       0.000156              fake                     0                   fake                          0\n","celebdf_effb4 EfficientNet-B4     id6_0007       real        20                 0              20            0.00       0.518999       0.000094              fake                     0                   fake                          0\n","celebdf_effb4 EfficientNet-B4     id6_0008       real        20                 0              20            0.00       0.518945       0.000056              fake                     0                   fake                          0\n","\n","Majority @ thr*=0.5188, TOP-10: fake correct = 32/50 (target 40), real correct = 30/50 (target 35)\n"]}]},{"cell_type":"code","source":["# Save the current results table `tbl` to Drive\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=False)\n","\n","import os, time\n","save_dir = \"/content/drive/My Drive/deepfake_results\"\n","os.makedirs(save_dir, exist_ok=True)\n","\n","fname = f\"cnnaug_celebdf_results_{time.strftime('%Y%m%d_%H%M%S')}.csv\"\n","out_path = os.path.join(save_dir, fname)\n","\n","tbl.to_csv(out_path, index=False)\n","print(f\"✅ Saved: {out_path}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DbcX3swknQjP","executionInfo":{"status":"ok","timestamp":1756047942916,"user_tz":-120,"elapsed":2320,"user":{"displayName":"Vishnumaya","userId":"01919615312035119785"}},"outputId":"b5725ce9-cc50-4af7-87fe-7778910e04b8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","✅ Saved: /content/drive/My Drive/deepfake_results/cnnaug_celebdf_results_20250824_150542.csv\n"]}]},{"cell_type":"code","source":["# === Compact table: dataset, detector, video_name, true_label, correctly_predicted (Yes/No) ===\n","# Run AFTER your finalized metrics cell so best/df_scores/df_video exist.\n","\n","import numpy as np, pandas as pd\n","from sklearn.metrics import roc_curve\n","\n","# Defaults if not already set\n","if 'DATASET_NAME' not in globals(): DATASET_NAME = \"celebdf_effb4\"\n","if 'DETECTOR_NAME' not in globals(): DETECTOR_NAME = \"CNN-Aug (EffB4)\"\n","TOP_K = 10  # confident majority\n","\n","# Unpack from best\n","best_auc, best_eer, best_ap, thr_eer, best_agg, tau, bmin, df_used_frames, df_video_in = best\n","\n","# Base per-video scores/labels\n","dfv = df_video_in.copy()\n","if \"true_label\" not in dfv.columns and \"true_text\" in dfv.columns:\n","    dfv.rename(columns={\"true_text\":\"true_label\"}, inplace=True)\n","\n","# Choose threshold: prefer your tuned thr_star; else balanced-accuracy fallback\n","if 'thr_star' in globals():\n","    thr = float(thr_star)\n","else:\n","    y = (dfv[\"true_label\"]==\"fake\").astype(int).values\n","    s = dfv[\"prob_fake\"].values\n","    fpr, tpr, thr_list = roc_curve(y, s)\n","    thr = float(thr_list[np.nanargmax(tpr - fpr)])  # Youden's J (balanced)\n","\n","# Use ALL frames for majority vote\n","frames_full = df_scores.copy().reset_index(drop=True)\n","if \"true_label\" not in frames_full.columns and \"label\" in frames_full.columns:\n","    frames_full[\"true_label\"] = np.where(frames_full[\"label\"]==1, \"fake\", \"real\")\n","elif \"true_label\" not in frames_full.columns:\n","    raise KeyError(\"Neither 'true_label' nor 'label' present in df_scores.\")\n","\n","# Top-K confident majority at chosen threshold\n","ff = frames_full.copy()\n","ff[\"conf\"] = np.abs(ff[\"prob_fake\"] - 0.5)\n","ff[\"rank\"] = ff.groupby(\"video_name\")[\"conf\"].rank(ascending=False, method=\"first\")\n","topk = ff[ff[\"rank\"] <= TOP_K].copy()\n","topk[\"bin_pred\"] = (topk[\"prob_fake\"] >= thr).astype(int)\n","\n","maj = (topk.groupby([\"video_name\",\"true_label\"], as_index=False)[\"bin_pred\"].mean())\n","maj[\"video_pred_by_majority\"] = np.where(maj[\"bin_pred\"] >= 0.5, \"fake\", \"real\")\n","maj = maj.drop(columns=[\"bin_pred\"])\n","\n","# Merge with dfv to ensure all videos appear\n","merged = (dfv[[\"video_name\",\"true_label\"]]\n","          .merge(maj, on=[\"video_name\",\"true_label\"], how=\"left\"))\n","\n","# Correctness → Yes/No\n","merged[\"correctly_predicted\"] = np.where(\n","    merged[\"video_pred_by_majority\"] == merged[\"true_label\"], \"yes\", \"no\"\n",")\n","\n","# Final compact table\n","out = merged[[\"video_name\",\"true_label\",\"correctly_predicted\"]].copy()\n","out.insert(0, \"detector\", DETECTOR_NAME)\n","out.insert(0, \"dataset\", DATASET_NAME)\n","\n","# Print only the table (no wrapping)\n","pd.set_option(\"display.max_rows\", 2000)\n","pd.set_option(\"display.max_columns\", None)\n","pd.set_option(\"display.width\", 10_000)\n","pd.set_option(\"display.expand_frame_repr\", False)\n","print(out.to_string(index=False))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ea2sp_h3oPlI","executionInfo":{"status":"ok","timestamp":1756048197363,"user_tz":-120,"elapsed":91,"user":{"displayName":"Vishnumaya","userId":"01919615312035119785"}},"outputId":"2bee5ad7-c1b6-44de-9d5e-d40f6475b031"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["      dataset        detector   video_name true_label correctly_predicted\n","celebdf_effb4 EfficientNet-B4     id0_0009       real                  no\n","celebdf_effb4 EfficientNet-B4 id0_id1_0000       fake                 yes\n","celebdf_effb4 EfficientNet-B4 id0_id1_0001       fake                  no\n","celebdf_effb4 EfficientNet-B4 id0_id1_0002       fake                  no\n","celebdf_effb4 EfficientNet-B4 id0_id1_0003       fake                 yes\n","celebdf_effb4 EfficientNet-B4 id0_id1_0005       fake                 yes\n","celebdf_effb4 EfficientNet-B4 id0_id1_0006       fake                 yes\n","celebdf_effb4 EfficientNet-B4 id0_id1_0007       fake                 yes\n","celebdf_effb4 EfficientNet-B4 id0_id1_0009       fake                 yes\n","celebdf_effb4 EfficientNet-B4 id0_id2_0000       fake                 yes\n","celebdf_effb4 EfficientNet-B4 id0_id2_0001       fake                  no\n","celebdf_effb4 EfficientNet-B4 id0_id2_0002       fake                  no\n","celebdf_effb4 EfficientNet-B4 id0_id2_0003       fake                 yes\n","celebdf_effb4 EfficientNet-B4 id0_id2_0004       fake                  no\n","celebdf_effb4 EfficientNet-B4 id0_id2_0005       fake                 yes\n","celebdf_effb4 EfficientNet-B4 id0_id2_0006       fake                 yes\n","celebdf_effb4 EfficientNet-B4 id0_id2_0007       fake                 yes\n","celebdf_effb4 EfficientNet-B4 id0_id2_0008       fake                  no\n","celebdf_effb4 EfficientNet-B4 id0_id2_0009       fake                 yes\n","celebdf_effb4 EfficientNet-B4 id0_id3_0000       fake                 yes\n","celebdf_effb4 EfficientNet-B4 id0_id3_0001       fake                  no\n","celebdf_effb4 EfficientNet-B4 id0_id3_0002       fake                  no\n","celebdf_effb4 EfficientNet-B4 id0_id3_0003       fake                 yes\n","celebdf_effb4 EfficientNet-B4 id0_id3_0004       fake                  no\n","celebdf_effb4 EfficientNet-B4 id0_id3_0005       fake                 yes\n","celebdf_effb4 EfficientNet-B4 id0_id3_0006       fake                 yes\n","celebdf_effb4 EfficientNet-B4 id0_id3_0007       fake                 yes\n","celebdf_effb4 EfficientNet-B4 id0_id3_0008       fake                  no\n","celebdf_effb4 EfficientNet-B4 id0_id3_0009       fake                 yes\n","celebdf_effb4 EfficientNet-B4 id0_id4_0000       fake                 yes\n","celebdf_effb4 EfficientNet-B4 id0_id4_0001       fake                  no\n","celebdf_effb4 EfficientNet-B4 id0_id4_0002       fake                  no\n","celebdf_effb4 EfficientNet-B4 id0_id4_0003       fake                 yes\n","celebdf_effb4 EfficientNet-B4 id0_id4_0004       fake                  no\n","celebdf_effb4 EfficientNet-B4 id0_id4_0005       fake                 yes\n","celebdf_effb4 EfficientNet-B4 id0_id4_0006       fake                 yes\n","celebdf_effb4 EfficientNet-B4 id0_id4_0009       fake                 yes\n","celebdf_effb4 EfficientNet-B4 id0_id6_0000       fake                 yes\n","celebdf_effb4 EfficientNet-B4 id0_id6_0001       fake                  no\n","celebdf_effb4 EfficientNet-B4 id0_id6_0002       fake                  no\n","celebdf_effb4 EfficientNet-B4 id0_id6_0006       fake                 yes\n","celebdf_effb4 EfficientNet-B4 id0_id6_0007       fake                 yes\n","celebdf_effb4 EfficientNet-B4 id0_id6_0009       fake                 yes\n","celebdf_effb4 EfficientNet-B4 id0_id9_0000       fake                 yes\n","celebdf_effb4 EfficientNet-B4 id0_id9_0001       fake                  no\n","celebdf_effb4 EfficientNet-B4 id0_id9_0002       fake                  no\n","celebdf_effb4 EfficientNet-B4 id0_id9_0003       fake                 yes\n","celebdf_effb4 EfficientNet-B4 id0_id9_0005       fake                 yes\n","celebdf_effb4 EfficientNet-B4 id0_id9_0006       fake                 yes\n","celebdf_effb4 EfficientNet-B4 id0_id9_0007       fake                 yes\n","celebdf_effb4 EfficientNet-B4 id0_id9_0008       fake                  no\n","celebdf_effb4 EfficientNet-B4     id1_0000       real                 yes\n","celebdf_effb4 EfficientNet-B4     id1_0001       real                  no\n","celebdf_effb4 EfficientNet-B4     id1_0002       real                 yes\n","celebdf_effb4 EfficientNet-B4     id1_0003       real                 yes\n","celebdf_effb4 EfficientNet-B4     id1_0004       real                  no\n","celebdf_effb4 EfficientNet-B4     id1_0005       real                 yes\n","celebdf_effb4 EfficientNet-B4     id1_0006       real                  no\n","celebdf_effb4 EfficientNet-B4     id1_0007       real                 yes\n","celebdf_effb4 EfficientNet-B4     id1_0008       real                 yes\n","celebdf_effb4 EfficientNet-B4     id1_0009       real                 yes\n","celebdf_effb4 EfficientNet-B4     id2_0000       real                  no\n","celebdf_effb4 EfficientNet-B4     id2_0001       real                 yes\n","celebdf_effb4 EfficientNet-B4     id2_0002       real                 yes\n","celebdf_effb4 EfficientNet-B4     id2_0003       real                 yes\n","celebdf_effb4 EfficientNet-B4     id2_0004       real                 yes\n","celebdf_effb4 EfficientNet-B4     id2_0005       real                  no\n","celebdf_effb4 EfficientNet-B4     id2_0006       real                  no\n","celebdf_effb4 EfficientNet-B4     id2_0007       real                 yes\n","celebdf_effb4 EfficientNet-B4     id2_0008       real                 yes\n","celebdf_effb4 EfficientNet-B4     id2_0009       real                  no\n","celebdf_effb4 EfficientNet-B4     id3_0000       real                 yes\n","celebdf_effb4 EfficientNet-B4     id3_0001       real                 yes\n","celebdf_effb4 EfficientNet-B4     id3_0002       real                  no\n","celebdf_effb4 EfficientNet-B4     id3_0003       real                 yes\n","celebdf_effb4 EfficientNet-B4     id3_0004       real                 yes\n","celebdf_effb4 EfficientNet-B4     id3_0005       real                 yes\n","celebdf_effb4 EfficientNet-B4     id3_0006       real                 yes\n","celebdf_effb4 EfficientNet-B4     id3_0007       real                 yes\n","celebdf_effb4 EfficientNet-B4     id3_0008       real                 yes\n","celebdf_effb4 EfficientNet-B4     id3_0009       real                  no\n","celebdf_effb4 EfficientNet-B4     id4_0000       real                 yes\n","celebdf_effb4 EfficientNet-B4     id4_0001       real                  no\n","celebdf_effb4 EfficientNet-B4     id4_0002       real                  no\n","celebdf_effb4 EfficientNet-B4     id4_0003       real                  no\n","celebdf_effb4 EfficientNet-B4     id4_0004       real                  no\n","celebdf_effb4 EfficientNet-B4     id4_0005       real                  no\n","celebdf_effb4 EfficientNet-B4     id4_0006       real                  no\n","celebdf_effb4 EfficientNet-B4     id4_0007       real                 yes\n","celebdf_effb4 EfficientNet-B4     id4_0008       real                 yes\n","celebdf_effb4 EfficientNet-B4     id4_0009       real                 yes\n","celebdf_effb4 EfficientNet-B4     id6_0000       real                 yes\n","celebdf_effb4 EfficientNet-B4     id6_0001       real                 yes\n","celebdf_effb4 EfficientNet-B4     id6_0002       real                  no\n","celebdf_effb4 EfficientNet-B4     id6_0003       real                 yes\n","celebdf_effb4 EfficientNet-B4     id6_0004       real                 yes\n","celebdf_effb4 EfficientNet-B4     id6_0005       real                 yes\n","celebdf_effb4 EfficientNet-B4     id6_0006       real                  no\n","celebdf_effb4 EfficientNet-B4     id6_0007       real                  no\n","celebdf_effb4 EfficientNet-B4     id6_0008       real                  no\n"]}]},{"cell_type":"code","source":["# Save the compact table `out` to Drive at: My Drive/CNN Aug results celeb df\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=False)\n","\n","import os, time\n","assert 'out' in globals(), \"Please run the previous cell that creates the DataFrame `out` first.\"\n","\n","save_dir = \"/content/drive/My Drive/CNN Aug results celeb df\"\n","os.makedirs(save_dir, exist_ok=True)\n","\n","fname = f\"cnnaug_celebdf_pred_table_{time.strftime('%Y%m%d_%H%M%S')}.csv\"\n","out_path = os.path.join(save_dir, fname)\n","\n","out.to_csv(out_path, index=False)\n","print(f\"✅ Saved: {out_path}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J1GmIe5LolRh","executionInfo":{"status":"ok","timestamp":1756048288556,"user_tz":-120,"elapsed":2203,"user":{"displayName":"Vishnumaya","userId":"01919615312035119785"}},"outputId":"dd3a76ed-2823-4e01-9136-775c3ad76fdb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","✅ Saved: /content/drive/My Drive/CNN Aug results celeb df/cnnaug_celebdf_pred_table_20250824_151128.csv\n"]}]}]}