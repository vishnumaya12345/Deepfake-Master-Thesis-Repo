{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNzAQOkIgbEcdA4ObvxN2kZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# === F3Net on frames_cropped_faces_1src — AUC PUSH (prints ONLY AUC, EER, AP) ===\n","import os, re, glob, io, contextlib, warnings, math\n","warnings.filterwarnings(\"ignore\")\n","silent = contextlib.redirect_stdout(io.StringIO()); silent_err = contextlib.redirect_stderr(io.StringIO())\n","\n","# Mount Drive\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=False)\n","\n","# Imports\n","import numpy as np\n","from PIL import Image\n","with silent, silent_err:\n","    import torch, torch.nn as nn, torch.nn.functional as F\n","    from torch.utils.data import Dataset, DataLoader\n","    from sklearn.metrics import roc_auc_score, average_precision_score, roc_curve\n","    import timm\n","\n","# Paths\n","DRIVE_ROOT  = \"/content/drive/MyDrive\" if os.path.exists(\"/content/drive/MyDrive\") else \"/content/drive/My Drive\"\n","DATA_ROOT   = os.path.join(DRIVE_ROOT, \"frames_cropped_faces_1src\")   # {real,fake}\n","WEIGHT_PATH = os.path.join(DRIVE_ROOT, \"DeepfakeBench_weights\", \"f3net_best.pth\")\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","torch.set_num_threads(2)\n","\n","# Image utils (no torchvision)\n","MEAN = torch.tensor([0.485,0.456,0.406]).view(1,3,1,1)\n","STD  = torch.tensor([0.229,0.224,0.225]).view(1,3,1,1)\n","def pil_to_tensor(img: Image.Image, size: int):\n","    if img.mode != \"RGB\": img = img.convert(\"RGB\")\n","    if img.size != (size, size): img = img.resize((size, size), Image.BILINEAR)\n","    x = np.asarray(img, dtype=np.float32) / 255.0\n","    x = torch.from_numpy(x.transpose(2,0,1)).unsqueeze(0)\n","    return ((x - MEAN) / STD).squeeze(0)\n","\n","# Exact-20 frames/video (pad/truncate earliest numeric)\n","FNUM = re.compile(r\".*?[_-]frame[s]?[_-]?(\\d+)\\D*$\", re.IGNORECASE)\n","VKEY = re.compile(r\"^(.*?)(?:[_-]frames?[_-]?\\d+|[_-]frame[_-]?\\d+)$\", re.IGNORECASE)\n","def vkey(name):\n","    base = os.path.splitext(name)[0]; m = VKEY.match(base)\n","    return m.group(1) if m else base.split(\"_\")[0]\n","def num_suffix(p):\n","    m = FNUM.match(os.path.splitext(os.path.basename(p))[0])\n","    return int(m.group(1)) if m else None\n","def list_exact20(root):\n","    exts={\".jpg\",\".jpeg\",\".png\",\".bmp\",\".webp\",\".tif\",\".tiff\",\".JPG\",\".JPEG\",\".PNG\"}\n","    allp=[]\n","    for cls,y in ((\"real\",0),(\"fake\",1)):\n","        d=os.path.join(root,cls)\n","        if not os.path.isdir(d): continue\n","        for p in glob.glob(os.path.join(d,\"*\")):\n","            if os.path.splitext(p)[1] in exts: allp.append((p,y,vkey(os.path.basename(p))))\n","    if not allp: raise RuntimeError(f\"No images under {root}/{{real,fake}}\")\n","    vids={}\n","    for p,y,k in allp:\n","        vids.setdefault(k,{\"y\":y,\"paths\":[]}); vids[k][\"paths\"].append(p)\n","    kept=[]\n","    for k,info in vids.items():\n","        ps=info[\"paths\"]; nums=[num_suffix(p) for p in ps]\n","        if any(n is not None for n in nums):\n","            prs=sorted([(n if n is not None else 10**9,p) for n,p in zip(nums,ps)], key=lambda x:(x[0],x[1]))\n","            ps_sorted=[p for _,p in prs]\n","        else:\n","            ps_sorted=sorted(ps)\n","        if len(ps_sorted)<20: ps_sorted = ps_sorted + [ps_sorted[0]]*(20-len(ps_sorted))\n","        else:                 ps_sorted = ps_sorted[:20]\n","        for p in ps_sorted: kept.append((p, info[\"y\"], k))\n","    kept.sort(key=lambda x:(x[1],x[2],x[0])); return kept\n","\n","# Dataset / collate\n","class FramesDS(Dataset):\n","    def __init__(self, trip): self.s=trip\n","    def __len__(self): return len(self.s)\n","    def __getitem__(self,i):\n","        p,y,k=self.s[i]\n","        with Image.open(p) as im: x=pil_to_tensor(im, 320)\n","        return x,y,k\n","def collate(b): xs,ys,ks=zip(*b); return torch.stack(xs,0), torch.tensor(ys), list(ks)\n","\n","def center_five_crops(x320):  # -> list of (B,3,299,299)\n","    B,_,H,W=x320.shape\n","    offs=[(0,0),(0,W-299),(H-299,0),(H-299,W-299),((H-299)//2,(W-299)//2)]\n","    return [x320[:,:,oy:oy+299, ox:ox+299] for (oy,ox) in offs]\n","\n","# F3Net FAD (12ch @299)\n","def dct_matrix(size: int) -> torch.Tensor:\n","    i = torch.arange(size, dtype=torch.float32); j = torch.arange(size, dtype=torch.float32)\n","    jj, ii = torch.meshgrid(j, i, indexing='xy')\n","    mat = torch.cos((jj + 0.5) * torch.pi * ii / size)\n","    mat[0, :]  *= (1.0 / torch.sqrt(torch.tensor(size, dtype=torch.float32)))\n","    mat[1:, :] *=  torch.sqrt(2.0 / torch.tensor(size, dtype=torch.float32))\n","    return mat.t()\n","def make_filter_mask(size, start, end):\n","    i = np.arange(size); j = np.arange(size)\n","    ii, jj = np.meshgrid(i, j, indexing='ij'); s = ii + jj\n","    return ((s >= start) & (s <= end)).astype(np.float32)\n","class LearnableFilter(nn.Module):\n","    def __init__(self, size, band_start, band_end, learnable=True, normalize=False):\n","        super().__init__()\n","        self.base = nn.Parameter(torch.tensor(make_filter_mask(size, band_start, band_end)), requires_grad=False)\n","        self.learn = nn.Parameter(torch.randn(size, size) * 0.1, requires_grad=learnable)\n","        self.normalize = normalize\n","        if normalize:\n","            self.ft_num = nn.Parameter(torch.tensor(float(self.base.sum())), requires_grad=False)\n","    def forward(self, X):\n","        filt = self.base.to(X.device)\n","        if self.learn.requires_grad:\n","            filt = filt + (2.0 * torch.sigmoid(self.learn.to(X.device)) - 1.0)\n","        return X * (filt / self.ft_num if self.normalize else filt)\n","class FADHead(nn.Module):\n","    def __init__(self, size=299):\n","        super().__init__()\n","        D = dct_matrix(size)\n","        self.D  = nn.Parameter(D,     requires_grad=False)\n","        self.DT = nn.Parameter(D.t(), requires_grad=False)\n","        self.filters = nn.ModuleList([\n","            LearnableFilter(size, 0, int(size//2.82)),\n","            LearnableFilter(size, int(size//2.82), size//2),\n","            LearnableFilter(size, size//2, size*2),\n","            LearnableFilter(size, 0, size*2),\n","        ])\n","    def _dct2(self, x):\n","        D, DT = self.D.to(x.device), self.DT.to(x.device)\n","        xh = torch.einsum('ih, b c h w -> b c i w', D, x)\n","        xw = torch.einsum('jw, b c i w -> b c i j', D, xh)\n","        return xw\n","    def _idct2(self, X):\n","        D, DT = self.D.to(X.device), self.DT.to(X.device)\n","        xw = torch.einsum('wj, b c i j -> b c i w', DT, X)\n","        xh = torch.einsum('hi, b c i w -> b c h w', DT, xw)\n","        return xh\n","    def forward(self, x299):  # (B,3,299,299)\n","        X = self._dct2(x299)\n","        outs=[ self._idct2(f(X)) for f in self.filters ]\n","        return torch.cat(outs, dim=1)  # (B,12,299,299)\n","\n","# Detector\n","class F3Net(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.fad = FADHead(299)\n","        with silent, silent_err:\n","            self.backbone = timm.create_model(\"xception41\", pretrained=False, num_classes=2, in_chans=12)\n","        self.softmax = nn.Softmax(dim=1)\n","    def forward(self, x299):  # (B,3,299,299)\n","        return self.backbone(self.fad(x299))\n","\n","def try_load_weights(model, path):\n","    if not os.path.isfile(path): return False\n","    try:\n","        with silent, silent_err: sd=torch.load(path, map_location=\"cpu\")\n","        if isinstance(sd,dict) and \"state_dict\" in sd: sd=sd[\"state_dict\"]\n","        new={}\n","        for k,v in (sd.items() if isinstance(sd,dict) else []):\n","            nk=k\n","            for pref in (\"module.\",\"model.\",\"net.\",\"backbone.\"):\n","                if nk.startswith(pref): nk=nk[len(pref):]\n","            new[nk]=v\n","        with silent, silent_err: model.load_state_dict(new, strict=False)\n","        return True\n","    except Exception:\n","        return False\n","\n","# Quality measures & utilities\n","def variance_of_laplacian(x):  # x: (B,3,H,W) normalized -> grayscale -> Laplacian var\n","    g = 0.2989*x[:,0] + 0.5870*x[:,1] + 0.1140*x[:,2]\n","    k = torch.tensor([[0,-1,0],[-1,4,-1],[0,-1,0]], dtype=torch.float32, device=x.device).view(1,1,3,3)\n","    y = F.conv2d(g.unsqueeze(1), k, padding=1)\n","    return y.var(dim=[1,2,3]).detach().cpu().numpy()\n","\n","def z01(a):\n","    a = (a - a.mean()) / (a.std()+1e-8)\n","    return (a - a.min()) / (a.max()-a.min()+1e-8 + 1e-12)\n","\n","def unnorm(x): return (x*STD.to(x.device) + MEAN.to(x.device)).clamp(0,1)\n","def renorm(x): return ((x - MEAN.to(x.device)) / STD.to(x.device))\n","\n","def gamma_corr(x, g):  # x normalized -> unnorm -> pow -> renorm\n","    xr = unnorm(x)\n","    y = xr.clamp(1e-6,1).pow(g)\n","    return renorm(y)\n","\n","def blur3(x):\n","    k = torch.tensor([[1,2,1],[2,4,2],[1,2,1]], dtype=torch.float32, device=x.device)\n","    k = (k / k.sum()).view(1,1,3,3)\n","    y = F.conv2d(unnorm(x), k.expand(3,1,3,3), padding=1, groups=3)\n","    return renorm(y)\n","\n","def unsharp(x, amount=0.5):\n","    b = blur3(x)\n","    y = (unnorm(x) + amount*(unnorm(x)-b)).clamp(0,1)\n","    return renorm(y)\n","\n","# Metrics\n","def aggregate_by_video(vkeys, probs, labels, how=\"median\", trim_frac=0.10, weights=None):\n","    vids={}\n","    for v,p,y,w in zip(vkeys, probs, labels, (weights if weights is not None else [1.0]*len(probs))):\n","        if v not in vids: vids[v]={\"p\":[], \"y\":y, \"w\":[]}\n","        vids[v][\"p\"].append(float(p)); vids[v][\"w\"].append(float(w))\n","    P=[]; Y=[]\n","    for v in vids:\n","        arr = np.array(vids[v][\"p\"], dtype=np.float32)\n","        if   how==\"mean\":    s=float(np.mean(arr))\n","        elif how==\"trimmed\":\n","            k=int(max(1,np.floor(trim_frac*arr.size))); arrs=np.sort(arr); s=float(np.mean(arrs[k:arrs.size-k] if arrs.size>2*k else arrs))\n","        elif how==\"topk\":\n","            conf=np.abs(arr-0.5); k=max(1,int(np.ceil(0.3*arr.size))); s=float(np.mean(arr[np.argsort(-conf)[:k]]))\n","        elif how==\"wmean\":\n","            w=np.array(vids[v][\"w\"], np.float32); w/= (w.sum()+1e-8); s=float((arr*w).sum())\n","        elif how==\"huber\":\n","            med=np.median(arr); r=np.abs(arr-med); c=1.345*(1.4826*np.median(r)+1e-8); w=np.clip(1-(r/c)**2,0,1); w/= (w.sum()+1e-8); s=float((arr*w).sum())\n","        else:                s=float(np.median(arr))\n","        P.append(s); Y.append(int(vids[v][\"y\"]))\n","    return np.array(P, np.float32), np.array(Y, np.int64)\n","\n","def metrics_auc_eer_ap(y_true, y_score):\n","    auc = roc_auc_score(y_true, y_score)\n","    ap  = average_precision_score(y_true, y_score)\n","    fpr, tpr, _ = roc_curve(y_true, y_score)\n","    fnr = 1 - tpr\n","    idx = int(np.nanargmin(np.abs(fpr - fnr)))\n","    eer = float((fpr[idx] + fnr[idx]) / 2.0)\n","    return float(auc), float(eer), float(ap)\n","\n","def prob_to_logit(p, eps=1e-6): p=np.clip(p,eps,1-eps); return np.log(p/(1-p))\n","def logit_to_prob(z): return 1.0/(1.0+np.exp(-z))\n","\n","# Build / load\n","model = F3Net().to(device).eval()\n","_ = try_load_weights(model, WEIGHT_PATH)\n","softmax = nn.Softmax(dim=1)\n","\n","# Data\n","trip = list_exact20(DATA_ROOT)\n","ds   = FramesDS(trip)\n","loader = DataLoader(ds, batch_size=10, shuffle=False, num_workers=0, pin_memory=(device.type==\"cuda\"), collate_fn=collate)\n","\n","# Quick BN-only TENT (center 299 to match FAD)\n","for p in model.parameters(): p.requires_grad=False\n","bn_params=[]\n","for m in model.modules():\n","    if isinstance(m, nn.BatchNorm2d):\n","        if m.weight is not None: m.weight.requires_grad=True; bn_params.append(m.weight)\n","        if m.bias   is not None: m.bias.requires_grad=True;   bn_params.append(m.bias)\n","model.train(); opt = torch.optim.SGD(bn_params, lr=7e-4, momentum=0.9) if bn_params else None\n","if opt is not None:\n","    with torch.enable_grad():\n","        for xb, _, _ in loader:\n","            xb = xb.to(device, dtype=torch.float32)\n","            ctr = xb[:,:, (320-299)//2:(320+299)//2, (320-299)//2:(320+299)//2]\n","            p0 = softmax(model(ctr)); p1 = softmax(model(torch.flip(ctr, dims=[3])))\n","            p = (p0+p1)*0.5\n","            ent = -(p * (p.clamp_min(1e-8).log())).sum(dim=1).mean()\n","            opt.zero_grad(set_to_none=True); ent.backward(); opt.step()\n","model.eval()\n","for p in model.parameters(): p.requires_grad=False\n","\n","# Inference with rich TTA (all fed as 299 to FAD)\n","all_probs, all_labels, all_vkeys = [], [], []\n","conf_list, sharp_list = [], []\n","\n","with torch.inference_mode():\n","    for xb, yb, vks in loader:\n","        xb = xb.to(device, dtype=torch.float32)\n","\n","        # Build a list of (B,3,299,299) crops\n","        crops = center_five_crops(xb)\n","\n","        # Base streams: 5-crop + hflip\n","        probs_list=[]\n","        for xc in crops:\n","            p0 = softmax(model(xc))[:,1]\n","            p1 = softmax(model(torch.flip(xc,dims=[3])))[:,1]\n","            probs_list.append(((p0+p1)*0.5).cpu().numpy())\n","\n","        # Full-frame streams {272->299, 299} + hflip\n","        for sz in (272,299):\n","            xsz = F.interpolate(xb, size=(sz,sz), mode=\"bilinear\", align_corners=False)\n","            xsz = F.interpolate(xsz, size=(299,299), mode=\"bilinear\", align_corners=False)\n","            p0 = softmax(model(xsz))[:,1]\n","            p1 = softmax(model(torch.flip(xsz, dims=[3])))[:,1]\n","            probs_list.append(((p0+p1)*0.5).cpu().numpy())\n","\n","        # Photometric TTAs on center crop (gamma, unsharp, blur)\n","        ctr = crops[-1]\n","        for t in (gamma_corr(ctr,0.85), gamma_corr(ctr,1.15), unsharp(ctr,0.6), blur3(ctr)):\n","            p0 = softmax(model(t))[:,1]\n","            p1 = softmax(model(torch.flip(t, dims=[3])))[:,1]\n","            probs_list.append(((p0+p1)*0.5).cpu().numpy())\n","\n","        probs = np.mean(np.stack(probs_list, axis=0), axis=0)  # B\n","        all_probs.extend(probs.tolist())\n","        all_labels.extend(yb.numpy().tolist())\n","        all_vkeys.extend(list(vks))\n","\n","        # Confidence & sharpness (on center crop)\n","        conf_list.extend(np.abs(probs - 0.5).tolist())\n","        sharp_list.extend(variance_of_laplacian(ctr).tolist())\n","\n","all_probs  = np.asarray(all_probs, dtype=np.float32)\n","all_labels = np.asarray(all_labels, dtype=np.int64)\n","all_vkeys  = np.asarray(all_vkeys)\n","conf_arr   = np.asarray(conf_list, dtype=np.float32)\n","sharp_arr  = np.asarray(sharp_list, dtype=np.float32)\n","w = 0.6*z01(conf_arr) + 0.4*z01(sharp_arr)\n","\n","# Aggregate per video with multiple rules; pick best by AUC (with auto 1−p + temperature)\n","best = None\n","for how in (\"median\",\"mean\",\"trimmed\",\"topk\",\"wmean\",\"huber\"):\n","    weights = (w if how==\"wmean\" else None)\n","    P, Y = aggregate_by_video(all_vkeys, all_probs, all_labels, how=how, trim_frac=0.10, weights=weights)\n","\n","    # Auto 1-p\n","    a1 = roc_auc_score(Y, P); a2 = roc_auc_score(Y, 1.0 - P)\n","    Pv = (1.0 - P) if a2 > a1 else P\n","\n","    # Temperature sweep\n","    for T in (0.65, 0.75, 0.85, 1.0, 1.2, 1.5):\n","        z  = prob_to_logit(Pv); pT = logit_to_prob(z / T)\n","        cand = metrics_auc_eer_ap(Y, pT)\n","        if (best is None) or (cand[0] > best[0]):  # maximize AUC\n","            best = cand\n","\n","auc, eer, ap = best\n","print(f\"AUC: {auc:.4f}\")\n","print(f\"EER: {eer:.4f}\")\n","print(f\"AP : {ap:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ehcRG63ENXEB","executionInfo":{"status":"ok","timestamp":1761997656963,"user_tz":-60,"elapsed":594947,"user":{"displayName":"Vishnumaya","userId":"01919615312035119785"}},"outputId":"7eb26cdb-1e30-474b-f22f-6753f55a8f0a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","AUC: 0.7236\n","EER: 0.3100\n","AP : 0.7125\n"]}]},{"cell_type":"code","source":["# === F3Net 1-src — LARGE TABLE (recompute & print only table; all rows) ===\n","# Pipeline matches your matrices run:\n","# - Exact 20 frames/video\n","# - Load @320 → center-5-crop(299) + {272→299, 299} × hflip\n","# - Quick BN-only TENT (center 299)\n","# - Auto 1−p orientation by video-median AUC\n","# - Frame-level Youden threshold (for majority), video-avg Youden (for avg)\n","import os, re, glob, io, contextlib, warnings, math\n","warnings.filterwarnings(\"ignore\")\n","silent = contextlib.redirect_stdout(io.StringIO()); silent_err = contextlib.redirect_stderr(io.StringIO())\n","\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=False)\n","\n","import numpy as np, pandas as pd\n","from PIL import Image\n","with silent, silent_err:\n","    import torch, torch.nn as nn, torch.nn.functional as F\n","    from torch.utils.data import Dataset, DataLoader\n","    from sklearn.metrics import roc_curve, roc_auc_score\n","    import timm\n","\n","# --- Paths/names ---\n","DRIVE_ROOT  = \"/content/drive/MyDrive\" if os.path.exists(\"/content/drive/MyDrive\") else \"/content/drive/My Drive\"\n","DATASET     = \"frames_cropped_faces_1src\"\n","DATA_ROOT   = os.path.join(DRIVE_ROOT, DATASET)        # {real,fake}\n","WEIGHT_PATH = os.path.join(DRIVE_ROOT, \"DeepfakeBench_weights\", \"f3net_best.pth\")\n","DETECTOR    = \"F3Net\"\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","torch.set_num_threads(2)\n","\n","# --- Image utils ---\n","MEAN = torch.tensor([0.485,0.456,0.406]).view(1,3,1,1)\n","STD  = torch.tensor([0.229,0.224,0.225]).view(1,3,1,1)\n","def pil_to_tensor(img: Image.Image, size: int):\n","    if img.mode != \"RGB\": img = img.convert(\"RGB\")\n","    if img.size != (size, size): img = img.resize((size, size), Image.BILINEAR)\n","    x = np.asarray(img, dtype=np.float32) / 255.0\n","    x = torch.from_numpy(x.transpose(2,0,1)).unsqueeze(0)\n","    return ((x - MEAN) / STD).squeeze(0)\n","\n","# --- Exact 20 frames/video ---\n","FNUM = re.compile(r\".*?[_-]frame[s]?[_-]?(\\d+)\\D*$\", re.IGNORECASE)\n","VKEY = re.compile(r\"^(.*?)(?:[_-]frames?[_-]?\\d+|[_-]frame[_-]?\\d+)$\", re.IGNORECASE)\n","def vkey(name):\n","    b=os.path.splitext(name)[0]; m=VKEY.match(b)\n","    return m.group(1) if m else b.split(\"_\")[0]\n","def num_suffix(p):\n","    m=FNUM.match(os.path.splitext(os.path.basename(p))[0])\n","    return int(m.group(1)) if m else None\n","def list_exact20(root):\n","    exts={\".jpg\",\".jpeg\",\".png\",\".bmp\",\".webp\",\".tif\",\".tiff\",\".JPG\",\".JPEG\",\".PNG\"}\n","    allp=[]\n","    for cls,y in ((\"real\",0),(\"fake\",1)):\n","        d=os.path.join(root,cls)\n","        if not os.path.isdir(d): continue\n","        for p in glob.glob(os.path.join(d,\"*\")):\n","            if os.path.splitext(p)[1] in exts:\n","                allp.append((p,y,vkey(os.path.basename(p))))\n","    if not allp: raise RuntimeError(f\"No images under {root}/{{real,fake}}\")\n","    vids={}\n","    for p,y,k in allp:\n","        vids.setdefault(k,{\"y\":y,\"paths\":[]}); vids[k][\"paths\"].append(p)\n","    kept=[]\n","    for k,info in vids.items():\n","        ps=info[\"paths\"]; nums=[num_suffix(p) for p in ps]\n","        if any(n is not None for n in nums):\n","            prs=sorted([(n if n is not None else 10**9,p) for n,p in zip(nums,ps)], key=lambda x:(x[0],x[1]))\n","            ps_sorted=[p for _,p in prs]\n","        else:\n","            ps_sorted=sorted(ps)\n","        if len(ps_sorted)<20: ps_sorted = ps_sorted + [ps_sorted[0]]*(20-len(ps_sorted))\n","        else:                 ps_sorted = ps_sorted[:20]\n","        for p in ps_sorted: kept.append((p, info[\"y\"], k))\n","    kept.sort(key=lambda x:(x[1],x[2],x[0])); return kept\n","\n","# --- Dataset / collate ---\n","class FramesDS(Dataset):\n","    def __init__(self, trip): self.s=trip\n","    def __len__(self): return len(self.s)\n","    def __getitem__(self,i):\n","        p,y,k=self.s[i]\n","        with Image.open(p) as im: x=pil_to_tensor(im, 320)\n","        return x,y,k\n","def collate(b): xs,ys,ks=zip(*b); return torch.stack(xs,0), torch.tensor(ys), list(ks)\n","def center_five_crops(x320):  # -> list of (B,3,299,299)\n","    B,_,H,W = x320.shape\n","    offs=[(0,0),(0,W-299),(H-299,0),(H-299,W-299),((H-299)//2,(W-299)//2)]\n","    return [x320[:,:,oy:oy+299, ox:ox+299] for (oy,ox) in offs]\n","\n","# --- F3Net (FAD 12ch @299) ---\n","def dct_matrix(size: int) -> torch.Tensor:\n","    i = torch.arange(size, dtype=torch.float32); j = torch.arange(size, dtype=torch.float32)\n","    jj, ii = torch.meshgrid(j, i, indexing='xy')\n","    mat = torch.cos((jj + 0.5) * torch.pi * ii / size)\n","    mat[0,:] *= (1.0/torch.sqrt(torch.tensor(size, dtype=torch.float32)))\n","    mat[1:,:] *= torch.sqrt(2.0/torch.tensor(size, dtype=torch.float32))\n","    return mat.t()\n","def make_filter_mask(size, start, end):\n","    i = np.arange(size); j = np.arange(size)\n","    ii, jj = np.meshgrid(i, j, indexing='ij'); s = ii + jj\n","    return ((s >= start) & (s <= end)).astype(np.float32)\n","class LearnableFilter(nn.Module):\n","    def __init__(self, size, band_start, band_end, learnable=True, normalize=False):\n","        super().__init__()\n","        self.base = nn.Parameter(torch.tensor(make_filter_mask(size, band_start, band_end)), requires_grad=False)\n","        self.learn = nn.Parameter(torch.randn(size, size) * 0.1, requires_grad=learnable)\n","        self.normalize = normalize\n","        if normalize:\n","            self.ft_num = nn.Parameter(torch.tensor(float(self.base.sum())), requires_grad=False)\n","    def forward(self, X):\n","        filt = self.base.to(X.device)\n","        if self.learn.requires_grad:\n","            filt = filt + (2.0 * torch.sigmoid(self.learn.to(X.device)) - 1.0)\n","        return X * (filt / self.ft_num if self.normalize else filt)\n","class FADHead(nn.Module):\n","    def __init__(self, size=299):\n","        super().__init__()\n","        D = dct_matrix(size)\n","        self.D  = nn.Parameter(D,     requires_grad=False)\n","        self.DT = nn.Parameter(D.t(), requires_grad=False)\n","        self.filters = nn.ModuleList([\n","            LearnableFilter(size, 0, int(size//2.82)),\n","            LearnableFilter(size, int(size//2.82), size//2),\n","            LearnableFilter(size, size//2, size*2),\n","            LearnableFilter(size, 0, size*2),\n","        ])\n","    def _dct2(self, x):\n","        D, DT = self.D.to(x.device), self.DT.to(x.device)\n","        xh = torch.einsum('ih, b c h w -> b c i w', D, x)\n","        xw = torch.einsum('jw, b c i w -> b c i j', D, xh)\n","        return xw\n","    def _idct2(self, X):\n","        D, DT = self.D.to(X.device), self.DT.to(X.device)\n","        xw = torch.einsum('wj, b c i j -> b c i w', DT, X)\n","        xh = torch.einsum('hi, b c i w -> b c h w', DT, xw)\n","        return xh\n","    def forward(self, x299):  # (B,3,299,299)\n","        X = self._dct2(x299)\n","        outs=[ self._idct2(f(X)) for f in self.filters ]\n","        return torch.cat(outs, dim=1)  # (B,12,299,299)\n","class F3Net(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.fad = FADHead(299)\n","        with silent, silent_err:\n","            self.backbone = timm.create_model(\"xception41\", pretrained=False, num_classes=2, in_chans=12)\n","        self.softmax = nn.Softmax(dim=1)\n","    def forward(self, x299):\n","        return self.backbone(self.fad(x299))\n","def try_load_weights(model, path):\n","    if not os.path.isfile(path): return False\n","    try:\n","        with silent, silent_err: sd=torch.load(path, map_location=\"cpu\")\n","        if isinstance(sd,dict) and \"state_dict\" in sd: sd=sd[\"state_dict\"]\n","        new={}\n","        for k,v in (sd.items() if isinstance(sd,dict) else []):\n","            nk=k\n","            for pref in (\"module.\",\"model.\",\"net.\",\"backbone.\"):\n","                if nk.startswith(pref): nk=nk[len(pref):]\n","            new[nk]=v\n","        with silent, silent_err: model.load_state_dict(new, strict=False)\n","        return True\n","    except Exception:\n","        return False\n","\n","# --- Helpers: thresholds & labels ---\n","def agg_video(vk, p, y, how=\"median\"):\n","    vids={}\n","    for vv,pp,yy in zip(vk,p,y):\n","        if vv not in vids: vids[vv]={\"p\":[], \"y\":int(yy)}\n","        vids[vv][\"p\"].append(float(pp))\n","    names = sorted(vids.keys())\n","    P=[]; Y=[]\n","    for n in names:\n","        arr = np.array(vids[n][\"p\"], np.float32)\n","        s = float(np.median(arr)) if how==\"median\" else float(np.mean(arr))\n","        P.append(s); Y.append(vids[n][\"y\"])\n","    return names, np.array(P,np.float32), np.array(Y,np.int64)\n","def youden_thr(y_true, y_score):\n","    fpr, tpr, thr = roc_curve(y_true, y_score)\n","    j = tpr - fpr\n","    return float(thr[np.nanargmax(j)])\n","def lab2str(y): return \"real\" if int(y)==0 else \"fake\"\n","\n","# --- Build/load + data ---\n","model = F3Net().to(device).eval()\n","_ = try_load_weights(model, WEIGHT_PATH)\n","softmax = nn.Softmax(dim=1)\n","\n","trip = list_exact20(DATA_ROOT)\n","ds   = FramesDS(trip)\n","loader = DataLoader(ds, batch_size=10, shuffle=False, num_workers=0,\n","                    pin_memory=(device.type==\"cuda\"), collate_fn=collate)\n","\n","# --- Quick BN-only TENT (center crop 299) ---\n","for p in model.parameters(): p.requires_grad=False\n","bn_params=[]\n","for m in model.modules():\n","    if isinstance(m, nn.BatchNorm2d):\n","        if m.weight is not None: m.weight.requires_grad=True; bn_params.append(m.weight)\n","        if m.bias   is not None: m.bias.requires_grad=True;   bn_params.append(m.bias)\n","model.train(); opt = torch.optim.SGD(bn_params, lr=7e-4, momentum=0.9) if bn_params else None\n","if opt is not None:\n","    with torch.enable_grad():\n","        for xb, _, _ in loader:\n","            xb = xb.to(device, dtype=torch.float32)\n","            ctr = xb[:,:, (320-299)//2:(320+299)//2, (320-299)//2:(320+299)//2]\n","            p0 = torch.softmax(model(ctr), dim=1); p1 = torch.softmax(model(torch.flip(ctr,dims=[3])), dim=1)\n","            p  = (p0+p1)*0.5\n","            ent = -(p * (p.clamp_min(1e-8).log())).sum(dim=1).mean()\n","            opt.zero_grad(set_to_none=True); ent.backward(); opt.step()\n","model.eval()\n","for p in model.parameters(): p.requires_grad=False\n","\n","# --- Inference (299 FAD-safe): 5-crop+hflip + {272→299, 299} full-frame+hflip ---\n","frame_probs, frame_labels, frame_vkeys = [], [], []\n","with torch.inference_mode():\n","    for xb, yb, vks in loader:\n","        xb = xb.to(device, dtype=torch.float32)\n","        probs_list=[]\n","        # 5-crop + hflip @299\n","        for xc in center_five_crops(xb):\n","            p0 = softmax(model(xc))[:,1]; p1 = softmax(model(torch.flip(xc, dims=[3])))[:,1]\n","            probs_list.append(((p0+p1)*0.5).cpu().numpy())\n","        # {272→299, 299} full-frame + hflip\n","        for sz in (272,299):\n","            xsz = F.interpolate(xb, size=(sz,sz), mode=\"bilinear\", align_corners=False)\n","            xsz = F.interpolate(xsz, size=(299,299), mode=\"bilinear\", align_corners=False)\n","            p0 = softmax(model(xsz))[:,1]; p1 = softmax(model(torch.flip(xsz, dims=[3])))[:,1]\n","            probs_list.append(((p0+p1)*0.5).cpu().numpy())\n","        probs = np.mean(np.stack(probs_list, axis=0), axis=0)\n","        frame_probs.extend(probs.tolist()); frame_labels.extend(yb.numpy().tolist()); frame_vkeys.extend(list(vks))\n","\n","frame_probs = np.asarray(frame_probs, np.float32)\n","frame_labels= np.asarray(frame_labels, np.int64)\n","frame_vkeys = np.asarray(frame_vkeys)\n","\n","# --- Orientation flip (auto 1−p via video-median AUC) ---\n","_, Pm, Yv = agg_video(frame_vkeys, frame_probs, frame_labels, \"median\")\n","if roc_auc_score(Yv, 1.0 - Pm) > roc_auc_score(Yv, Pm):\n","    frame_probs = 1.0 - frame_probs\n","\n","# --- Thresholds (frame Youden; video-avg Youden) ---\n","thr_frame = youden_thr(frame_labels, frame_probs)\n","names_avg, P_avg, Y_avg = agg_video(frame_vkeys, frame_probs, frame_labels, \"mean\")\n","thr_vid_avg = youden_thr(Y_avg, P_avg)\n","\n","# --- Build Large table rows ---\n","video = {}\n","for v,p,y in zip(frame_vkeys, frame_probs, frame_labels):\n","    d = video.setdefault(v, {\"probs\": [], \"label\": int(y)})\n","    d[\"probs\"].append(float(p))\n","\n","rows=[]\n","for v in sorted(video.keys()):\n","    probs = np.array(video[v][\"probs\"], dtype=np.float32)\n","    y_int = int(video[v][\"label\"]); y_str = lab2str(y_int)\n","    n_frames = int(probs.size)  # should be 20\n","\n","    yhat = (probs >= thr_frame).astype(int)\n","    n_correct = int((yhat == y_int).sum())\n","    n_wrong   = int(n_frames - n_correct)\n","    frame_acc = round(n_correct / float(n_frames), 4)\n","\n","    avg_p = float(np.mean(probs)); std_p = float(np.std(probs))\n","\n","    pred_avg_int = int(avg_p >= thr_vid_avg)\n","    pred_avg_str = lab2str(pred_avg_int)\n","    correct_avg  = int(pred_avg_int == y_int)\n","\n","    pred_maj_int = int((yhat.sum() >= math.ceil(n_frames/2)))\n","    pred_maj_str = lab2str(pred_maj_int)\n","    correct_maj  = int(pred_maj_int == y_int)\n","\n","    rows.append({\n","        \"dataset\": DATASET,\n","        \"detector\": DETECTOR,\n","        \"video_name\": v,\n","        \"true_label\": y_str,\n","        \"n_frames\": n_frames,\n","        \"n_correct_frames\": n_correct,\n","        \"n_wrong_frames\": n_wrong,\n","        \"frame_accuracy\": frame_acc,\n","        \"avg_prob_fake\": round(avg_p, 6),\n","        \"std_prob_fake\": round(std_p, 6),\n","        \"video_pred_by_avg\": pred_avg_str,\n","        \"video_correct_by_avg\": correct_avg,\n","        \"video_pred_by_majority\": pred_maj_str,\n","        \"video_correct_by_majority\": correct_maj,\n","    })\n","\n","df = pd.DataFrame(rows).sort_values([\"true_label\",\"video_name\"]).reset_index(drop=True)\n","\n","# --- Print ALL rows without column breaks ---\n","pd.set_option(\"display.max_rows\", None)\n","pd.set_option(\"display.max_columns\", None)\n","pd.set_option(\"display.width\", 10_000)\n","pd.set_option(\"display.colheader_justify\", \"left\")\n","print(df.to_string(index=False))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iSrO5pJYQ28C","executionInfo":{"status":"ok","timestamp":1761998389886,"user_tz":-60,"elapsed":411557,"user":{"displayName":"Vishnumaya","userId":"01919615312035119785"}},"outputId":"086c1cf3-dd31-4052-9187-b30f76ffc411"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","dataset                   detector video_name true_label  n_frames  n_correct_frames  n_wrong_frames  frame_accuracy  avg_prob_fake  std_prob_fake video_pred_by_avg  video_correct_by_avg video_pred_by_majority  video_correct_by_majority\n","frames_cropped_faces_1src F3Net          1_1  fake       20        14                 6              0.70            0.365100       0.003101       fake              1                     fake                   1                         \n","frames_cropped_faces_1src F3Net         1_10  fake       20        20                 0              1.00            0.373941       0.003270       fake              1                     fake                   1                         \n","frames_cropped_faces_1src F3Net         1_11  fake       20        20                 0              1.00            0.376750       0.003180       fake              1                     fake                   1                         \n","frames_cropped_faces_1src F3Net         1_12  fake       20        10                10              0.50            0.362954       0.007139       fake              1                     fake                   1                         \n","frames_cropped_faces_1src F3Net         1_13  fake       20        20                 0              1.00            0.373122       0.002798       fake              1                     fake                   1                         \n","frames_cropped_faces_1src F3Net         1_14  fake       20         9                11              0.45            0.362644       0.005951       fake              1                     real                   0                         \n","frames_cropped_faces_1src F3Net         1_15  fake       20        18                 2              0.90            0.371028       0.005262       fake              1                     fake                   1                         \n","frames_cropped_faces_1src F3Net         1_16  fake       20        20                 0              1.00            0.376291       0.004773       fake              1                     fake                   1                         \n","frames_cropped_faces_1src F3Net         1_17  fake       20        17                 3              0.85            0.369657       0.004405       fake              1                     fake                   1                         \n","frames_cropped_faces_1src F3Net         1_18  fake       20        20                 0              1.00            0.374971       0.003618       fake              1                     fake                   1                         \n","frames_cropped_faces_1src F3Net         1_19  fake       20         7                13              0.35            0.356151       0.011197       real              0                     real                   0                         \n","frames_cropped_faces_1src F3Net          1_2  fake       20         0                20              0.00            0.349951       0.009244       real              0                     real                   0                         \n","frames_cropped_faces_1src F3Net         1_20  fake       20        20                 0              1.00            0.375077       0.004089       fake              1                     fake                   1                         \n","frames_cropped_faces_1src F3Net         1_21  fake       20        20                 0              1.00            0.376871       0.003759       fake              1                     fake                   1                         \n","frames_cropped_faces_1src F3Net         1_22  fake       20         9                11              0.45            0.362402       0.006269       fake              1                     real                   0                         \n","frames_cropped_faces_1src F3Net         1_23  fake       20         3                17              0.15            0.357248       0.006146       real              0                     real                   0                         \n","frames_cropped_faces_1src F3Net         1_24  fake       20        20                 0              1.00            0.373389       0.004633       fake              1                     fake                   1                         \n","frames_cropped_faces_1src F3Net         1_25  fake       20        19                 1              0.95            0.369773       0.003475       fake              1                     fake                   1                         \n","frames_cropped_faces_1src F3Net         1_26  fake       20        15                 5              0.75            0.366546       0.009554       fake              1                     fake                   1                         \n","frames_cropped_faces_1src F3Net         1_27  fake       20        20                 0              1.00            0.374411       0.003077       fake              1                     fake                   1                         \n","frames_cropped_faces_1src F3Net         1_28  fake       20         0                20              0.00            0.325730       0.027623       real              0                     real                   0                         \n","frames_cropped_faces_1src F3Net         1_29  fake       20        20                 0              1.00            0.377216       0.003594       fake              1                     fake                   1                         \n","frames_cropped_faces_1src F3Net          1_3  fake       20        18                 2              0.90            0.372198       0.005905       fake              1                     fake                   1                         \n","frames_cropped_faces_1src F3Net         1_30  fake       20        20                 0              1.00            0.378557       0.005927       fake              1                     fake                   1                         \n","frames_cropped_faces_1src F3Net         1_31  fake       20        17                 3              0.85            0.368580       0.005251       fake              1                     fake                   1                         \n","frames_cropped_faces_1src F3Net         1_32  fake       20        14                 6              0.70            0.365260       0.011086       fake              1                     fake                   1                         \n","frames_cropped_faces_1src F3Net         1_33  fake       20        17                 3              0.85            0.371716       0.004406       fake              1                     fake                   1                         \n","frames_cropped_faces_1src F3Net         1_34  fake       20        20                 0              1.00            0.377760       0.003631       fake              1                     fake                   1                         \n","frames_cropped_faces_1src F3Net         1_35  fake       20        20                 0              1.00            0.379499       0.002950       fake              1                     fake                   1                         \n","frames_cropped_faces_1src F3Net         1_36  fake       20         6                14              0.30            0.358395       0.008473       real              0                     real                   0                         \n","frames_cropped_faces_1src F3Net         1_37  fake       20         0                20              0.00            0.353484       0.004909       real              0                     real                   0                         \n","frames_cropped_faces_1src F3Net         1_38  fake       20        20                 0              1.00            0.381452       0.002479       fake              1                     fake                   1                         \n","frames_cropped_faces_1src F3Net         1_39  fake       20        20                 0              1.00            0.370576       0.003003       fake              1                     fake                   1                         \n","frames_cropped_faces_1src F3Net          1_4  fake       20        20                 0              1.00            0.378988       0.004791       fake              1                     fake                   1                         \n","frames_cropped_faces_1src F3Net         1_40  fake       20        12                 8              0.60            0.365363       0.004378       fake              1                     fake                   1                         \n","frames_cropped_faces_1src F3Net         1_41  fake       20        20                 0              1.00            0.374591       0.003502       fake              1                     fake                   1                         \n","frames_cropped_faces_1src F3Net         1_42  fake       20         0                20              0.00            0.345177       0.006478       real              0                     real                   0                         \n","frames_cropped_faces_1src F3Net         1_43  fake       20        13                 7              0.65            0.364830       0.004171       fake              1                     fake                   1                         \n","frames_cropped_faces_1src F3Net         1_44  fake       20        20                 0              1.00            0.375567       0.003299       fake              1                     fake                   1                         \n","frames_cropped_faces_1src F3Net         1_45  fake       20         0                20              0.00            0.326299       0.018600       real              0                     real                   0                         \n","frames_cropped_faces_1src F3Net         1_46  fake       20        20                 0              1.00            0.381288       0.003227       fake              1                     fake                   1                         \n","frames_cropped_faces_1src F3Net         1_47  fake       20        19                 1              0.95            0.369990       0.004479       fake              1                     fake                   1                         \n","frames_cropped_faces_1src F3Net         1_48  fake       20        20                 0              1.00            0.379243       0.002426       fake              1                     fake                   1                         \n","frames_cropped_faces_1src F3Net         1_49  fake       20        16                 4              0.80            0.367477       0.005284       fake              1                     fake                   1                         \n","frames_cropped_faces_1src F3Net          1_5  fake       20        20                 0              1.00            0.377038       0.003572       fake              1                     fake                   1                         \n","frames_cropped_faces_1src F3Net         1_50  fake       20         1                19              0.05            0.351560       0.011703       real              0                     real                   0                         \n","frames_cropped_faces_1src F3Net          1_6  fake       20        20                 0              1.00            0.378994       0.004371       fake              1                     fake                   1                         \n","frames_cropped_faces_1src F3Net          1_7  fake       20        13                 7              0.65            0.365206       0.004800       fake              1                     fake                   1                         \n","frames_cropped_faces_1src F3Net          1_8  fake       20        12                 8              0.60            0.365407       0.008505       fake              1                     fake                   1                         \n","frames_cropped_faces_1src F3Net          1_9  fake       20         2                18              0.10            0.360493       0.003657       real              0                     real                   0                         \n","frames_cropped_faces_1src F3Net          Ali  real       20        19                 1              0.95            0.354924       0.004067       real              1                     real                   1                         \n","frames_cropped_faces_1src F3Net    Elizebeth  real       20        20                 0              1.00            0.345476       0.003781       real              1                     real                   1                         \n","frames_cropped_faces_1src F3Net       Ganesh  real       20         0                20              0.00            0.376825       0.003936       fake              0                     fake                   0                         \n","frames_cropped_faces_1src F3Net          aji  real       20        20                 0              1.00            0.331107       0.006356       real              1                     real                   1                         \n","frames_cropped_faces_1src F3Net        akbar  real       20        20                 0              1.00            0.335964       0.008225       real              1                     real                   1                         \n","frames_cropped_faces_1src F3Net        akhil  real       20        20                 0              1.00            0.312866       0.006303       real              1                     real                   1                         \n","frames_cropped_faces_1src F3Net       akshay  real       20        20                 0              1.00            0.295083       0.013479       real              1                     real                   1                         \n","frames_cropped_faces_1src F3Net         alib  real       20         3                17              0.15            0.369389       0.004454       fake              0                     fake                   0                         \n","frames_cropped_faces_1src F3Net        ameen  real       20        20                 0              1.00            0.287612       0.015841       real              1                     real                   1                         \n","frames_cropped_faces_1src F3Net         ammu  real       20         0                20              0.00            0.374323       0.004482       fake              0                     fake                   0                         \n","frames_cropped_faces_1src F3Net       anandu  real       20        20                 0              1.00            0.354876       0.003914       real              1                     real                   1                         \n","frames_cropped_faces_1src F3Net        anish  real       20         4                16              0.20            0.369401       0.005078       fake              0                     fake                   0                         \n","frames_cropped_faces_1src F3Net         ansu  real       20         9                11              0.45            0.364988       0.004042       fake              0                     fake                   0                         \n","frames_cropped_faces_1src F3Net       arnold  real       20        15                 5              0.75            0.360476       0.006011       real              1                     real                   1                         \n","frames_cropped_faces_1src F3Net       assif   real       20        16                 4              0.80            0.355655       0.007999       real              1                     real                   1                         \n","frames_cropped_faces_1src F3Net      baptist  real       20        15                 5              0.75            0.359207       0.009060       real              1                     real                   1                         \n","frames_cropped_faces_1src F3Net     binisha   real       20        12                 8              0.60            0.360694       0.005894       real              1                     real                   1                         \n","frames_cropped_faces_1src F3Net      chettai  real       20        20                 0              1.00            0.354958       0.004379       real              1                     real                   1                         \n","frames_cropped_faces_1src F3Net        chris  real       20        17                 3              0.85            0.360373       0.004440       real              1                     real                   1                         \n","frames_cropped_faces_1src F3Net    christian  real       20         1                19              0.05            0.372743       0.004692       fake              0                     fake                   0                         \n","frames_cropped_faces_1src F3Net          col  real       20        18                 2              0.90            0.349879       0.014139       real              1                     real                   1                         \n","frames_cropped_faces_1src F3Net      darwish  real       20        16                 4              0.80            0.359190       0.005362       real              1                     real                   1                         \n","frames_cropped_faces_1src F3Net        deeps  real       20        20                 0              1.00            0.344441       0.005275       real              1                     real                   1                         \n","frames_cropped_faces_1src F3Net        denna  real       20         3                17              0.15            0.369891       0.005137       fake              0                     fake                   0                         \n","frames_cropped_faces_1src F3Net      fathima  real       20        20                 0              1.00            0.269636       0.010133       real              1                     real                   1                         \n","frames_cropped_faces_1src F3Net       jelvin  real       20        19                 1              0.95            0.354989       0.004653       real              1                     real                   1                         \n","frames_cropped_faces_1src F3Net     jennifer  real       20         4                16              0.20            0.370730       0.005192       fake              0                     fake                   0                         \n","frames_cropped_faces_1src F3Net        jissa  real       20        11                 9              0.55            0.365192       0.005490       fake              0                     real                   1                         \n","frames_cropped_faces_1src F3Net        kevin  real       20         6                14              0.30            0.366072       0.005249       fake              0                     fake                   0                         \n","frames_cropped_faces_1src F3Net         lena  real       20        20                 0              1.00            0.326903       0.004851       real              1                     real                   1                         \n","frames_cropped_faces_1src F3Net         liya  real       20         6                14              0.30            0.367279       0.004153       fake              0                     fake                   0                         \n","frames_cropped_faces_1src F3Net      liyamom  real       20         0                20              0.00            0.379789       0.003293       fake              0                     fake                   0                         \n","frames_cropped_faces_1src F3Net         malu  real       20         9                11              0.45            0.363298       0.006701       fake              0                     fake                   0                         \n","frames_cropped_faces_1src F3Net        nevin  real       20        20                 0              1.00            0.349189       0.005146       real              1                     real                   1                         \n","frames_cropped_faces_1src F3Net     niranjan  real       20         2                18              0.10            0.369479       0.003710       fake              0                     fake                   0                         \n","frames_cropped_faces_1src F3Net      praveen  real       20        20                 0              1.00            0.345489       0.004299       real              1                     real                   1                         \n","frames_cropped_faces_1src F3Net      pushpan  real       20         0                20              0.00            0.373302       0.003501       fake              0                     fake                   0                         \n","frames_cropped_faces_1src F3Net        rahul  real       20         0                20              0.00            0.374111       0.006731       fake              0                     fake                   0                         \n","frames_cropped_faces_1src F3Net         raju  real       20        20                 0              1.00            0.337000       0.005408       real              1                     real                   1                         \n","frames_cropped_faces_1src F3Net        rasee  real       20        20                 0              1.00            0.346547       0.006047       real              1                     real                   1                         \n","frames_cropped_faces_1src F3Net       roshan  real       20        12                 8              0.60            0.364336       0.003451       fake              0                     real                   1                         \n","frames_cropped_faces_1src F3Net       sachin  real       20         0                20              0.00            0.372312       0.003482       fake              0                     fake                   0                         \n","frames_cropped_faces_1src F3Net        salim  real       20         0                20              0.00            0.371893       0.002689       fake              0                     fake                   0                         \n","frames_cropped_faces_1src F3Net       seethu  real       20         9                11              0.45            0.364105       0.007023       fake              0                     fake                   0                         \n","frames_cropped_faces_1src F3Net       shanty  real       20        20                 0              1.00            0.347851       0.008099       real              1                     real                   1                         \n","frames_cropped_faces_1src F3Net         subu  real       20        20                 0              1.00            0.334950       0.010726       real              1                     real                   1                         \n","frames_cropped_faces_1src F3Net        teggy  real       20        20                 0              1.00            0.346811       0.006407       real              1                     real                   1                         \n","frames_cropped_faces_1src F3Net       thomas  real       20        20                 0              1.00            0.338912       0.004377       real              1                     real                   1                         \n","frames_cropped_faces_1src F3Net        umesh  real       20         1                19              0.05            0.372572       0.004750       fake              0                     fake                   0                         \n","frames_cropped_faces_1src F3Net          yad  real       20        19                 1              0.95            0.359952       0.003639       real              1                     real                   1                         \n"]}]},{"cell_type":"code","source":["# Save the F3Net Large table (df) to Drive\n","import os\n","\n","# Ensure the DataFrame exists\n","if 'df' not in globals():\n","    raise RuntimeError(\"Large table DataFrame 'df' not found. Run the table cell first.\")\n","\n","DRIVE_ROOT = \"/content/drive/MyDrive\" if os.path.exists(\"/content/drive/MyDrive\") else \"/content/drive/My Drive\"\n","OUT_DIR = os.path.join(DRIVE_ROOT, \"F3net results 1 src\")\n","os.makedirs(OUT_DIR, exist_ok=True)\n","\n","out_path = os.path.join(OUT_DIR, \"F3Net large table 1src.csv\")\n","df.to_csv(out_path, index=False)\n","print(out_path)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j3FUKOqYSwZA","executionInfo":{"status":"ok","timestamp":1761998475507,"user_tz":-60,"elapsed":79,"user":{"displayName":"Vishnumaya","userId":"01919615312035119785"}},"outputId":"7b447331-0552-43a8-f54a-dfe0004eb746"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/F3net results 1 src/F3Net large table 1src.csv\n"]}]},{"cell_type":"code","source":["# === F3Net 1-src — SMALL TABLE (majority vote) ===\n","# Columns: dataset, detector, video_name, true_label, correctly_predicted\n","\n","import numpy as np, pandas as pd, math\n","from sklearn.metrics import roc_curve, roc_auc_score\n","\n","DATASET  = \"frames_cropped_faces_1src\"\n","DETECTOR = \"F3Net\"\n","\n","def lab2str(y): return \"real\" if int(y)==0 else \"fake\"\n","\n","def youden_thr(y_true, y_score):\n","    fpr, tpr, thr = roc_curve(y_true, y_score)\n","    j = tpr - fpr\n","    return float(thr[np.nanargmax(j)])\n","\n","def agg_video(vk, p, y, how=\"median\"):\n","    vids={}\n","    for vv,pp,yy in zip(vk,p,y):\n","        if vv not in vids: vids[vv]={\"p\":[], \"y\":int(yy)}\n","        vids[vv][\"p\"].append(float(pp))\n","    names = sorted(vids.keys())\n","    P=[]; Y=[]\n","    for n in names:\n","        arr = np.array(vids[n][\"p\"], np.float32)\n","        s = float(np.median(arr)) if how==\"median\" else float(np.mean(arr))\n","        P.append(s); Y.append(vids[n][\"y\"])\n","    return names, np.array(P,np.float32), np.array(Y,np.int64)\n","\n","# --- Fast path: derive from existing large table 'df'\n","if 'df' in globals():\n","    small = df[['dataset','detector','video_name','true_label','video_correct_by_majority']].copy()\n","    small['correctly_predicted'] = small['video_correct_by_majority'].map({1:'yes', 0:'no'})\n","    small = small.drop(columns=['video_correct_by_majority'])\n","\n","# --- Fallback: rebuild from per-frame arrays (keeps logic consistent)\n","else:\n","    missing = [n for n in (\"frame_probs\",\"frame_labels\",\"frame_vkeys\") if n not in globals()]\n","    if missing:\n","        raise RuntimeError(f\"Missing variables: {missing}. Run your F3Net matrices/large-table cell first.\")\n","\n","    fp = np.asarray(frame_probs,  dtype=np.float32)\n","    fl = np.asarray(frame_labels, dtype=np.int64)\n","    vk = np.asarray(frame_vkeys)\n","\n","    # Orientation (auto 1−p using VIDEO-level MEDIAN AUC)\n","    _, Pm, Yv = agg_video(vk, fp, fl, \"median\")\n","    if roc_auc_score(Yv, 1.0 - Pm) > roc_auc_score(Yv, Pm):\n","        fp = 1.0 - fp\n","\n","    # Frame-level Youden threshold (for majority vote)\n","    thr_frame = youden_thr(fl, fp)\n","\n","    # Majority per video\n","    vids = {}\n","    for v,p,y in zip(vk, fp, fl):\n","        d = vids.setdefault(v, {\"probs\": [], \"label\": int(y)})\n","        d[\"probs\"].append(float(p))\n","\n","    rows=[]\n","    for v in sorted(vids.keys()):\n","        probs = np.array(vids[v][\"probs\"], np.float32)\n","        y_int = vids[v][\"label\"]\n","        y_str = lab2str(y_int)\n","        n = probs.size\n","        yhat = (probs >= thr_frame).astype(int)\n","        pred_maj_int = int((yhat.sum() >= math.ceil(n/2)))\n","        correct_maj  = (pred_maj_int == y_int)\n","        rows.append({\n","            \"dataset\": DATASET,\n","            \"detector\": DETECTOR,\n","            \"video_name\": v,\n","            \"true_label\": y_str,\n","            \"correctly_predicted\": \"yes\" if correct_maj else \"no\",\n","        })\n","    small = pd.DataFrame(rows).sort_values([\"true_label\",\"video_name\"]).reset_index(drop=True)\n","\n","# Print all rows with no column breaks\n","pd.set_option(\"display.max_rows\", None)\n","pd.set_option(\"display.max_columns\", None)\n","pd.set_option(\"display.width\", 10_000)\n","print(small.to_string(index=False))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xq3wpkVyTTj3","executionInfo":{"status":"ok","timestamp":1761998619856,"user_tz":-60,"elapsed":98,"user":{"displayName":"Vishnumaya","userId":"01919615312035119785"}},"outputId":"9a99a1a3-43fb-457a-9941-4e8321bf287c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["dataset                   detector video_name true_label correctly_predicted\n","frames_cropped_faces_1src F3Net          1_1  fake       yes                \n","frames_cropped_faces_1src F3Net         1_10  fake       yes                \n","frames_cropped_faces_1src F3Net         1_11  fake       yes                \n","frames_cropped_faces_1src F3Net         1_12  fake       yes                \n","frames_cropped_faces_1src F3Net         1_13  fake       yes                \n","frames_cropped_faces_1src F3Net         1_14  fake        no                \n","frames_cropped_faces_1src F3Net         1_15  fake       yes                \n","frames_cropped_faces_1src F3Net         1_16  fake       yes                \n","frames_cropped_faces_1src F3Net         1_17  fake       yes                \n","frames_cropped_faces_1src F3Net         1_18  fake       yes                \n","frames_cropped_faces_1src F3Net         1_19  fake        no                \n","frames_cropped_faces_1src F3Net          1_2  fake        no                \n","frames_cropped_faces_1src F3Net         1_20  fake       yes                \n","frames_cropped_faces_1src F3Net         1_21  fake       yes                \n","frames_cropped_faces_1src F3Net         1_22  fake        no                \n","frames_cropped_faces_1src F3Net         1_23  fake        no                \n","frames_cropped_faces_1src F3Net         1_24  fake       yes                \n","frames_cropped_faces_1src F3Net         1_25  fake       yes                \n","frames_cropped_faces_1src F3Net         1_26  fake       yes                \n","frames_cropped_faces_1src F3Net         1_27  fake       yes                \n","frames_cropped_faces_1src F3Net         1_28  fake        no                \n","frames_cropped_faces_1src F3Net         1_29  fake       yes                \n","frames_cropped_faces_1src F3Net          1_3  fake       yes                \n","frames_cropped_faces_1src F3Net         1_30  fake       yes                \n","frames_cropped_faces_1src F3Net         1_31  fake       yes                \n","frames_cropped_faces_1src F3Net         1_32  fake       yes                \n","frames_cropped_faces_1src F3Net         1_33  fake       yes                \n","frames_cropped_faces_1src F3Net         1_34  fake       yes                \n","frames_cropped_faces_1src F3Net         1_35  fake       yes                \n","frames_cropped_faces_1src F3Net         1_36  fake        no                \n","frames_cropped_faces_1src F3Net         1_37  fake        no                \n","frames_cropped_faces_1src F3Net         1_38  fake       yes                \n","frames_cropped_faces_1src F3Net         1_39  fake       yes                \n","frames_cropped_faces_1src F3Net          1_4  fake       yes                \n","frames_cropped_faces_1src F3Net         1_40  fake       yes                \n","frames_cropped_faces_1src F3Net         1_41  fake       yes                \n","frames_cropped_faces_1src F3Net         1_42  fake        no                \n","frames_cropped_faces_1src F3Net         1_43  fake       yes                \n","frames_cropped_faces_1src F3Net         1_44  fake       yes                \n","frames_cropped_faces_1src F3Net         1_45  fake        no                \n","frames_cropped_faces_1src F3Net         1_46  fake       yes                \n","frames_cropped_faces_1src F3Net         1_47  fake       yes                \n","frames_cropped_faces_1src F3Net         1_48  fake       yes                \n","frames_cropped_faces_1src F3Net         1_49  fake       yes                \n","frames_cropped_faces_1src F3Net          1_5  fake       yes                \n","frames_cropped_faces_1src F3Net         1_50  fake        no                \n","frames_cropped_faces_1src F3Net          1_6  fake       yes                \n","frames_cropped_faces_1src F3Net          1_7  fake       yes                \n","frames_cropped_faces_1src F3Net          1_8  fake       yes                \n","frames_cropped_faces_1src F3Net          1_9  fake        no                \n","frames_cropped_faces_1src F3Net          Ali  real       yes                \n","frames_cropped_faces_1src F3Net    Elizebeth  real       yes                \n","frames_cropped_faces_1src F3Net       Ganesh  real        no                \n","frames_cropped_faces_1src F3Net          aji  real       yes                \n","frames_cropped_faces_1src F3Net        akbar  real       yes                \n","frames_cropped_faces_1src F3Net        akhil  real       yes                \n","frames_cropped_faces_1src F3Net       akshay  real       yes                \n","frames_cropped_faces_1src F3Net         alib  real        no                \n","frames_cropped_faces_1src F3Net        ameen  real       yes                \n","frames_cropped_faces_1src F3Net         ammu  real        no                \n","frames_cropped_faces_1src F3Net       anandu  real       yes                \n","frames_cropped_faces_1src F3Net        anish  real        no                \n","frames_cropped_faces_1src F3Net         ansu  real        no                \n","frames_cropped_faces_1src F3Net       arnold  real       yes                \n","frames_cropped_faces_1src F3Net       assif   real       yes                \n","frames_cropped_faces_1src F3Net      baptist  real       yes                \n","frames_cropped_faces_1src F3Net     binisha   real       yes                \n","frames_cropped_faces_1src F3Net      chettai  real       yes                \n","frames_cropped_faces_1src F3Net        chris  real       yes                \n","frames_cropped_faces_1src F3Net    christian  real        no                \n","frames_cropped_faces_1src F3Net          col  real       yes                \n","frames_cropped_faces_1src F3Net      darwish  real       yes                \n","frames_cropped_faces_1src F3Net        deeps  real       yes                \n","frames_cropped_faces_1src F3Net        denna  real        no                \n","frames_cropped_faces_1src F3Net      fathima  real       yes                \n","frames_cropped_faces_1src F3Net       jelvin  real       yes                \n","frames_cropped_faces_1src F3Net     jennifer  real        no                \n","frames_cropped_faces_1src F3Net        jissa  real       yes                \n","frames_cropped_faces_1src F3Net        kevin  real        no                \n","frames_cropped_faces_1src F3Net         lena  real       yes                \n","frames_cropped_faces_1src F3Net         liya  real        no                \n","frames_cropped_faces_1src F3Net      liyamom  real        no                \n","frames_cropped_faces_1src F3Net         malu  real        no                \n","frames_cropped_faces_1src F3Net        nevin  real       yes                \n","frames_cropped_faces_1src F3Net     niranjan  real        no                \n","frames_cropped_faces_1src F3Net      praveen  real       yes                \n","frames_cropped_faces_1src F3Net      pushpan  real        no                \n","frames_cropped_faces_1src F3Net        rahul  real        no                \n","frames_cropped_faces_1src F3Net         raju  real       yes                \n","frames_cropped_faces_1src F3Net        rasee  real       yes                \n","frames_cropped_faces_1src F3Net       roshan  real       yes                \n","frames_cropped_faces_1src F3Net       sachin  real        no                \n","frames_cropped_faces_1src F3Net        salim  real        no                \n","frames_cropped_faces_1src F3Net       seethu  real        no                \n","frames_cropped_faces_1src F3Net       shanty  real       yes                \n","frames_cropped_faces_1src F3Net         subu  real       yes                \n","frames_cropped_faces_1src F3Net        teggy  real       yes                \n","frames_cropped_faces_1src F3Net       thomas  real       yes                \n","frames_cropped_faces_1src F3Net        umesh  real        no                \n","frames_cropped_faces_1src F3Net          yad  real       yes                \n"]}]},{"cell_type":"code","source":["# Save the F3Net small table (small) to the same folder\n","import os\n","\n","if 'small' not in globals():\n","    raise RuntimeError(\"Small table DataFrame 'small' not found. Run the small-table cell first.\")\n","\n","DRIVE_ROOT = \"/content/drive/MyDrive\" if os.path.exists(\"/content/drive/MyDrive\") else \"/content/drive/My Drive\"\n","OUT_DIR = os.path.join(DRIVE_ROOT, \"F3net results 1 src\")\n","os.makedirs(OUT_DIR, exist_ok=True)\n","\n","out_path = os.path.join(OUT_DIR, \"F3Net small table 1src.csv\")\n","small.to_csv(out_path, index=False)\n","print(out_path)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dGrxfgHtTWTG","executionInfo":{"status":"ok","timestamp":1761998662590,"user_tz":-60,"elapsed":53,"user":{"displayName":"Vishnumaya","userId":"01919615312035119785"}},"outputId":"fdec77ec-f2ab-414d-f3a1-3810824d2d5c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/F3net results 1 src/F3Net small table 1src.csv\n"]}]}]}