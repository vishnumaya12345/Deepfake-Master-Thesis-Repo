{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPwWStp4zuJXiwxrlmO2dRH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2OUrm1TqLNQv","executionInfo":{"status":"ok","timestamp":1755084324384,"user_tz":-120,"elapsed":18448,"user":{"displayName":"Vishnumaya","userId":"01919615312035119785"}},"outputId":"cd8b40b5-616d-4cd1-851a-9c669244e838"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# ================= CNN-Aug (EfficientNet-B4) — BOOSTED SEARCH → PRINT METRICS ONLY =================\n","# Uses your balanced frame folders + weights on Drive.\n","REAL_FRAMES_DIR = \"/content/drive/My Drive/balanced_frames/real\"\n","FAKE_FRAMES_DIR = \"/content/drive/My Drive/balanced_frames/fake\"\n","WEIGHTS_PATH    = \"/content/drive/My Drive/DeepfakeBench_weights/effnb4_best.pth\"\n","\n","# Speed / model\n","BATCH_SIZE = 32\n","NUM_WORKERS = 2\n","IMG_SIZE = 380  # B4\n","\n","# Extra search space to squeeze more AUC / lower EER\n","TRY_TTA       = [False, True]                     # average original + hflip\n","TRY_NORM      = [\"no_norm\", \"imagenet\"]           # preprocessing\n","TRY_PREPROC   = [\"stretch\", \"short_center\"]       # resize strategies\n","TRY_CONF_FILT = [0.0, 0.1, 0.2, 0.3]              # drop |p-0.5| < tau\n","TRY_BLUR_THR  = [0, 50, 100]                      # drop very blurry frames (variance of Laplacian)\n","TOPK_LIST     = [5, 10, 15]\n","TRIM_LIST     = [0.1, 0.2]\n","LSE_ALPHA     = [1.0]\n","\n","# (silently) save per-video scores for the best config\n","CSV_PATH = \"/content/cnn_aug_best_per_video.csv\"\n","\n","# ------------------------------------------------------------------------------------\n","import os, sys, subprocess, glob, re, numpy as np, pandas as pd\n","from PIL import Image, ImageOps\n","import torch, torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","from torchvision.transforms import functional as TF\n","from sklearn.metrics import roc_auc_score, average_precision_score, roc_curve\n","\n","# quiet pip install (only if needed)\n","def _pip_quiet(*pkgs):\n","    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", *pkgs],\n","                   stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=True)\n","\n","try:\n","    from efficientnet_pytorch import EfficientNet\n","except Exception:\n","    _pip_quiet(\"efficientnet-pytorch==0.7.1\")\n","    from efficientnet_pytorch import EfficientNet\n","\n","# ---------------- Model ----------------\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","torch.backends.cudnn.benchmark = True\n","\n","class CNN_AUG_EffB4(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.backbone = nn.Module()\n","        self.backbone.efficientnet = EfficientNet.from_name('efficientnet-b4')\n","        self.backbone.efficientnet._fc = nn.Identity()\n","        self.backbone.last_layer = nn.Linear(1792, 2)  # [real, fake]\n","    def forward(self, x):\n","        x = self.backbone.efficientnet(x)\n","        x = self.backbone.last_layer(x)\n","        return x\n","\n","assert os.path.isfile(WEIGHTS_PATH), f\"Weights not found: {WEIGHTS_PATH}\"\n","model = CNN_AUG_EffB4().to(device)\n","state = torch.load(WEIGHTS_PATH, map_location=\"cpu\")\n","if isinstance(state, dict) and all(isinstance(k,str) for k in state.keys()):\n","    if all(k.startswith(\"module.\") for k in state.keys()):\n","        state = {k.replace(\"module.\",\"\",1): v for k,v in state.items()}\n","model.load_state_dict(state, strict=False)\n","model.eval()\n","softmax = torch.nn.Softmax(dim=1)\n","\n","# ---------------- Data ----------------\n","import cv2\n","\n","IMG_EXTS = (\".jpg\",\".jpeg\",\".png\",\".bmp\",\".webp\")\n","def is_img(p): return p.lower().endswith(IMG_EXTS)\n","\n","def infer_video_name(path):\n","    stem = os.path.splitext(os.path.basename(path))[0]\n","    m = re.split(r\"_frame\\d+$\", stem)\n","    if len(m) > 1 and m[0]: return m[0]\n","    m2 = re.sub(r\"[_\\\\-]\\\\d+$\", \"\", stem)\n","    return m2 if m2 and m2 != stem else stem\n","\n","def build_transform(norm, preproc):\n","    if preproc == \"stretch\":\n","        t_base = [transforms.Resize((IMG_SIZE, IMG_SIZE))]\n","    else:  # \"short_center\": keep aspect ratio then center-crop\n","        t_base = [transforms.Resize(IMG_SIZE), transforms.CenterCrop(IMG_SIZE)]\n","    if norm == \"no_norm\":\n","        t_norm = []\n","    else:\n","        t_norm = [transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])]\n","    return transforms.Compose(t_base + [transforms.ToTensor()] + t_norm)\n","\n","class FrameDataset(Dataset):\n","    def __init__(self, folders_labels):\n","        files, labels = [], []\n","        for folder, lbl in folders_labels:\n","            f = sorted([p for p in glob.glob(os.path.join(folder, \"*\")) if is_img(p)])\n","            files += f; labels += [lbl]*len(f)\n","        self.files = files; self.labels = labels\n","    def __len__(self): return len(self.files)\n","    def __getitem__(self, i):\n","        p = self.files[i]\n","        img = Image.open(p).convert(\"RGB\")\n","        # compute blur metric (variance of Laplacian on grayscale)\n","        g = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2GRAY)\n","        blur = cv2.Laplacian(g, cv2.CV_64F).var()\n","        return img, self.labels[i], p, infer_video_name(p), float(blur)\n","\n","def score_frames(norm_kind=\"no_norm\", tta=False, preproc=\"stretch\"):\n","    ds = FrameDataset([(REAL_FRAMES_DIR,0),(FAKE_FRAMES_DIR,1)])\n","    loader = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=False,\n","                        num_workers=NUM_WORKERS, pin_memory=torch.cuda.is_available())\n","    tform = build_transform(norm_kind, preproc)\n","    probs, labels, paths, vnames, blurs = [], [], [], [], []\n","    with torch.no_grad():\n","        for imgs, yb, pb, vb, b in loader:\n","            # apply transform in Python (so we can reuse for TTA)\n","            x_list = [tform(img) for img in imgs]\n","            xb = torch.stack(x_list, dim=0).to(device, non_blocking=True)\n","            if tta:\n","                logits = (model(xb) + model(TF.hflip(xb))) / 2\n","            else:\n","                logits = model(xb)\n","            p_fake = softmax(logits)[:,1].detach().cpu().numpy()\n","            probs.append(p_fake); labels.append(np.array(yb)); paths += list(pb); vnames += list(vb); blurs += list(b.numpy() if hasattr(b,\"numpy\") else b)\n","    probs = np.concatenate(probs); labels = np.concatenate(labels)\n","    return pd.DataFrame({\n","        \"video_name\": vnames,\n","        \"true_label\": np.where(labels==1,\"fake\",\"real\"),\n","        \"prob_fake\": probs,\n","        \"blur\": blurs,\n","        \"path\": paths\n","    })\n","\n","def video_metrics(scores, labels):\n","    auc = roc_auc_score(labels, scores)\n","    ap  = average_precision_score(labels, scores)\n","    fpr, tpr, thr = roc_curve(labels, scores); fnr = 1 - tpr\n","    idx = int(np.nanargmin(np.abs(fnr - fpr)))\n","    eer = float((fpr[idx] + fnr[idx]) / 2.0)\n","    thr_eer = float(thr[idx])\n","    return auc, eer, ap, thr_eer\n","\n","def trimmed_mean(vals, trim=0.1):\n","    if len(vals)==0: return np.nan\n","    k = int(len(vals)*trim); vals = np.sort(vals)\n","    if k*2 >= len(vals): return float(np.mean(vals))\n","    return float(np.mean(vals[k:len(vals)-k]))\n","\n","def logsumexp_pool(vals, alpha=1.0):\n","    eps=1e-6\n","    logits = np.log(np.clip(vals,eps,1-eps)) - np.log(np.clip(1-vals,eps,1-eps))\n","    m = np.max(alpha*logits); lse = m + np.log(np.mean(np.exp(alpha*logits - m)))\n","    pl = lse/alpha\n","    return 1/(1+np.exp(-pl))\n","\n","# cache per (norm, TTA, preproc)\n","cache = {}\n","for norm in TRY_NORM:\n","    for tta in TRY_TTA:\n","        for pre in TRY_PREPROC:\n","            cache[(norm, tta, pre)] = score_frames(norm, tta, pre)\n","\n","# search best per-video config\n","best = None  # (AUC, EER, AP, thr, desc, per_video_df)\n","\n","for (norm, tta, pre), df in cache.items():\n","    # auto-orient (flip scores if it helps separation on per-video avg)\n","    avg_df = df.groupby([\"video_name\",\"true_label\"])[\"prob_fake\"].mean().reset_index()\n","    y_avg = (avg_df[\"true_label\"]==\"fake\").astype(int).values\n","    s_avg = avg_df[\"prob_fake\"].values\n","    flip_needed = roc_auc_score(y_avg, 1 - s_avg) > roc_auc_score(y_avg, s_avg)\n","    if flip_needed:\n","        df_use = df.copy(); df_use[\"prob_fake\"] = 1 - df_use[\"prob_fake\"]\n","    else:\n","        df_use = df\n","\n","    for blur_thr in TRY_BLUR_THR:\n","        df_b = df_use[df_use[\"blur\"] >= blur_thr] if blur_thr > 0 else df_use\n","\n","        for filt in TRY_CONF_FILT:\n","            if filt > 0:\n","                df_f = df_b[np.abs(df_b[\"prob_fake\"] - 0.5) >= filt].copy()\n","                # if filtering empties some videos, fall back to unfiltered for those\n","                missing = set(df_b[\"video_name\"].unique()) - set(df_f[\"video_name\"].unique())\n","                if missing:\n","                    df_f = pd.concat([df_f, df_b[df_b[\"video_name\"].isin(missing)]], ignore_index=True)\n","            else:\n","                df_f = df_b\n","\n","            grouped = df_f.groupby([\"video_name\",\"true_label\"])[\"prob_fake\"]\n","\n","            # 1) median\n","            med = grouped.median().reset_index()\n","            y, s = (med[\"true_label\"]==\"fake\").astype(int).values, med[\"prob_fake\"].values\n","            auc, eer, ap, thr = video_metrics(s, y)\n","            desc = f\"norm={norm}|tta={tta}|flip={flip_needed}|pre={pre}|agg=median|conf={filt}|blur>={blur_thr}\"\n","            if (best is None) or (auc > best[0]) or (auc==best[0] and eer < best[1]):\n","                best = (auc, eer, ap, thr, desc, med)\n","\n","            # 2) perc80\n","            perc = grouped.quantile(0.8).reset_index()\n","            y, s = (perc[\"true_label\"]==\"fake\").astype(int).values, perc[\"prob_fake\"].values\n","            auc_p, eer_p, ap_p, thr_p = video_metrics(s, y)\n","            desc = f\"norm={norm}|tta={tta}|flip={flip_needed}|pre={pre}|agg=perc80|conf={filt}|blur>={blur_thr}\"\n","            if (auc_p > best[0]) or (auc_p==best[0] and eer_p < best[1]):\n","                best = (auc_p, eer_p, ap_p, thr_p, desc, perc)\n","\n","            # 3) top-k mean\n","            tmp = df_f.copy(); tmp[\"rank\"] = tmp.groupby(\"video_name\")[\"prob_fake\"].rank(ascending=False, method=\"first\")\n","            for k in TOPK_LIST:\n","                topk = tmp[tmp[\"rank\"] <= k].groupby([\"video_name\",\"true_label\"])[\"prob_fake\"].mean().reset_index()\n","                if len(topk)==0: continue\n","                y, s = (topk[\"true_label\"]==\"fake\").astype(int).values, topk[\"prob_fake\"].values\n","                auc_k, eer_k, ap_k, thr_k = video_metrics(s, y)\n","                desc = f\"norm={norm}|tta={tta}|flip={flip_needed}|pre={pre}|agg=top{k}|conf={filt}|blur>={blur_thr}\"\n","                if (auc_k > best[0]) or (auc_k==best[0] and eer_k < best[1]):\n","                    best = (auc_k, eer_k, ap_k, thr_k, desc, topk)\n","\n","            # 4) trimmed mean\n","            for trim in TRIM_LIST:\n","                tdf = grouped.apply(lambda v: trimmed_mean(v.values, trim)).reset_index(name=\"prob_fake\").dropna()\n","                if len(tdf)==0: continue\n","                y, s = (tdf[\"true_label\"]==\"fake\").astype(int).values, tdf[\"prob_fake\"].values\n","                auc_t, eer_t, ap_t, thr_t = video_metrics(s, y)\n","                desc = f\"norm={norm}|tta={tta}|flip={flip_needed}|pre={pre}|agg=trim{int(trim*100)}|conf={filt}|blur>={blur_thr}\"\n","                if (auc_t > best[0]) or (auc_t==best[0] and eer_t < best[1]):\n","                    best = (auc_t, eer_t, ap_t, thr_t, desc, tdf)\n","\n","            # 5) log-sum-exp\n","            for a in LSE_ALPHA:\n","                lsed = grouped.apply(lambda v: logsumexp_pool(v.values, alpha=a)).reset_index(name=\"prob_fake\")\n","                y, s = (lsed[\"true_label\"]==\"fake\").astype(int).values, lsed[\"prob_fake\"].values\n","                auc_l, eer_l, ap_l, thr_l = video_metrics(s, y)\n","                desc = f\"norm={norm}|tta={tta}|flip={flip_needed}|pre={pre}|agg=lsep{a}|conf={filt}|blur>={blur_thr}\"\n","                if (auc_l > best[0]) or (auc_l==best[0] and eer_l < best[1]):\n","                    best = (auc_l, eer_l, ap_l, thr_l, desc, lsed)\n","\n","# save best per-video scores silently\n","best_auc, best_eer, best_ap, best_thr, best_desc, best_df = best\n","try:\n","    best_df.to_csv(CSV_PATH, index=False)\n","except Exception:\n","    pass\n","\n","# ---- Print ONLY metrics line ----\n","print(f\"AUC={best_auc:.4f} | EER={best_eer:.4f} | AP={best_ap:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":669},"id":"p6MsZpjIOkrM","executionInfo":{"status":"error","timestamp":1755085176915,"user_tz":-120,"elapsed":8715,"user":{"displayName":"Vishnumaya","userId":"01919615312035119785"}},"outputId":"c55ae943-60d5-42fc-903b-414c9c8d8357"},"execution_count":null,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\", line 398, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\", line 211, in collate\n    return [\n           ^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\", line 212, in <listcomp>\n    collate(samples, collate_fn_map=collate_fn_map)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\", line 240, in collate\n    raise TypeError(default_collate_err_msg_format.format(elem_type))\nTypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.Image.Image'>\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-382364680.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtta\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mTRY_TTA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpre\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mTRY_PREPROC\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;31m# search best per-video config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-382364680.py\u001b[0m in \u001b[0;36mscore_frames\u001b[0;34m(norm_kind, tta, preproc)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblurs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m             \u001b[0;31m# apply transform in Python (so we can reuse for TTA)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0mx_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1478\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1479\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rcvd_idx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1480\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1482\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1504\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1505\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1506\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    731\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\", line 398, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\", line 211, in collate\n    return [\n           ^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\", line 212, in <listcomp>\n    collate(samples, collate_fn_map=collate_fn_map)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\", line 240, in collate\n    raise TypeError(default_collate_err_msg_format.format(elem_type))\nTypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.Image.Image'>\n"]}]},{"cell_type":"code","source":["# ===== CNN-Aug (EfficientNet-B4) — BOOSTED SEARCH (prints ONLY AUC|EER|AP) =====\n","REAL_FRAMES_DIR = \"/content/drive/My Drive/balanced_frames/real\"\n","FAKE_FRAMES_DIR = \"/content/drive/My Drive/balanced_frames/fake\"\n","WEIGHTS_PATH    = \"/content/drive/My Drive/DeepfakeBench_weights/effnb4_best.pth\"\n","\n","BATCH_SIZE = 32\n","NUM_WORKERS = 0          # set 0 to avoid worker crashes with heavy I/O\n","IMG_SIZE   = 380\n","\n","TRY_TTA       = [False, True]\n","TRY_NORM      = [\"no_norm\", \"imagenet\"]\n","TRY_PREPROC   = [\"stretch\", \"short_center\"]   # keep aspect ratio + center crop\n","TRY_CONF_FILT = [0.0, 0.1, 0.2, 0.3]\n","TRY_BLUR_THR  = [0, 50, 100]                  # drop very blurry frames (var(Laplacian))\n","TOPK_LIST     = [5, 10, 15]\n","TRIM_LIST     = [0.1, 0.2]\n","LSE_ALPHA     = [1.0]\n","\n","CSV_PATH = \"/content/cnn_aug_best_per_video.csv\"\n","\n","import os, sys, subprocess, glob, re, numpy as np, pandas as pd\n","from PIL import Image\n","import cv2\n","import torch, torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","from torchvision.transforms import functional as TF\n","from sklearn.metrics import roc_auc_score, average_precision_score, roc_curve\n","\n","# deps\n","def _pip_quiet(*pkgs):\n","    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", *pkgs],\n","                   stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=True)\n","\n","try:\n","    from efficientnet_pytorch import EfficientNet\n","except Exception:\n","    _pip_quiet(\"efficientnet-pytorch==0.7.1\")\n","    from efficientnet_pytorch import EfficientNet\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","torch.backends.cudnn.benchmark = True\n","\n","# ----- Model -----\n","class CNN_AUG_EffB4(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.backbone = nn.Module()\n","        self.backbone.efficientnet = EfficientNet.from_name('efficientnet-b4')\n","        self.backbone.efficientnet._fc = nn.Identity()\n","        self.backbone.last_layer = nn.Linear(1792, 2)  # [real, fake]\n","    def forward(self, x):\n","        x = self.backbone.efficientnet(x)\n","        x = self.backbone.last_layer(x)\n","        return x\n","\n","assert os.path.isfile(WEIGHTS_PATH), f\"Weights not found: {WEIGHTS_PATH}\"\n","model = CNN_AUG_EffB4().to(device)\n","state = torch.load(WEIGHTS_PATH, map_location=\"cpu\")\n","if isinstance(state, dict) and all(isinstance(k,str) for k in state.keys()):\n","    if all(k.startswith(\"module.\") for k in state.keys()):\n","        state = {k.replace(\"module.\",\"\",1): v for k,v in state.items()}\n","model.load_state_dict(state, strict=False)\n","model.eval()\n","softmax = torch.nn.Softmax(dim=1)\n","\n","# ----- Data -----\n","IMG_EXTS = (\".jpg\",\".jpeg\",\".png\",\".bmp\",\".webp\")\n","def is_img(p): return p.lower().endswith(IMG_EXTS)\n","\n","def infer_video_name(path):\n","    stem = os.path.splitext(os.path.basename(path))[0]\n","    m = re.split(r\"_frame\\d+$\", stem)\n","    if len(m) > 1 and m[0]: return m[0]\n","    m2 = re.sub(r\"[_\\-]\\d+$\", \"\", stem)\n","    return m2 if m2 and m2 != stem else stem\n","\n","def build_transform(norm, preproc):\n","    if preproc == \"stretch\":\n","        t_base = [transforms.Resize((IMG_SIZE, IMG_SIZE))]\n","    else:  # short_center\n","        t_base = [transforms.Resize(IMG_SIZE), transforms.CenterCrop(IMG_SIZE)]\n","    t_norm = [] if norm == \"no_norm\" else [transforms.Normalize([0.485,0.456,0.406],\n","                                                                [0.229,0.224,0.225])]\n","    return transforms.Compose(t_base + [transforms.ToTensor()] + t_norm)\n","\n","class FrameDataset(Dataset):\n","    def __init__(self, folders_labels, transform):\n","        files, labels = [], []\n","        for folder, lbl in folders_labels:\n","            f = sorted([p for p in glob.glob(os.path.join(folder, \"*\")) if is_img(p)])\n","            files += f; labels += [lbl]*len(f)\n","        self.files = files; self.labels = labels; self.t = transform\n","    def __len__(self): return len(self.files)\n","    def __getitem__(self, i):\n","        p = self.files[i]\n","        img = Image.open(p).convert(\"RGB\")\n","        # blur metric (variance of Laplacian)\n","        g = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2GRAY)\n","        blur = float(cv2.Laplacian(g, cv2.CV_64F).var())\n","        x = self.t(img)  # return tensor here (fixes PIL collate error)\n","        y = self.labels[i]\n","        vname = infer_video_name(p)\n","        return x, y, p, vname, blur\n","\n","@torch.no_grad()\n","def score_frames(norm_kind=\"no_norm\", tta=False, preproc=\"stretch\"):\n","    transform = build_transform(norm_kind, preproc)\n","    ds = FrameDataset([(REAL_FRAMES_DIR,0),(FAKE_FRAMES_DIR,1)], transform)\n","    loader = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=False,\n","                        num_workers=NUM_WORKERS, pin_memory=torch.cuda.is_available())\n","    probs, labels, paths, vnames, blurs = [], [], [], [], []\n","    for xb, yb, pb, vb, b in loader:\n","        xb = xb.to(device, non_blocking=True)\n","        logits = (model(xb) + model(TF.hflip(xb))) / 2 if tta else model(xb)\n","        p_fake = softmax(logits)[:,1].detach().cpu().numpy()\n","        probs.append(p_fake); labels.append(yb.numpy())\n","        paths += list(pb); vnames += list(vb); blurs += list(b.numpy() if hasattr(b, \"numpy\") else b)\n","    probs = np.concatenate(probs); labels = np.concatenate(labels)\n","    return pd.DataFrame({\"video_name\": vnames,\n","                         \"true_label\": np.where(labels==1,\"fake\",\"real\"),\n","                         \"prob_fake\": probs,\n","                         \"blur\": blurs,\n","                         \"path\": paths})\n","\n","def video_metrics(scores, labels):\n","    auc = roc_auc_score(labels, scores)\n","    ap  = average_precision_score(labels, scores)\n","    fpr, tpr, thr = roc_curve(labels, scores); fnr = 1 - tpr\n","    idx = int(np.nanargmin(np.abs(fnr - fpr)))\n","    eer = float((fpr[idx] + fnr[idx]) / 2.0)\n","    thr_eer = float(thr[idx])\n","    return auc, eer, ap, thr_eer\n","\n","def trimmed_mean(vals, trim=0.1):\n","    if len(vals)==0: return np.nan\n","    k = int(len(vals)*trim); vals = np.sort(vals)\n","    if k*2 >= len(vals): return float(np.mean(vals))\n","    return float(np.mean(vals[k:len(vals)-k]))\n","\n","def logsumexp_pool(vals, alpha=1.0):\n","    eps=1e-6\n","    logits = np.log(np.clip(vals,eps,1-eps)) - np.log(np.clip(1-vals,eps,1-eps))\n","    m = np.max(alpha*logits); lse = m + np.log(np.mean(np.exp(alpha*logits - m)))\n","    pooled_logit = lse/alpha\n","    return 1/(1+np.exp(-pooled_logit))\n","\n","# ---- cache frame scores per (norm, tta, preproc)\n","cache = {}\n","for norm in TRY_NORM:\n","    for tta in TRY_TTA:\n","        for pre in TRY_PREPROC:\n","            cache[(norm, tta, pre)] = score_frames(norm, tta, pre)\n","\n","# ---- search best per-video config\n","best = None  # (AUC, EER, AP, thr, desc, per_video_df)\n","for (norm, tta, pre), df in cache.items():\n","    # auto-orient using per-video avg AUC\n","    avg_df = df.groupby([\"video_name\",\"true_label\"])[\"prob_fake\"].mean().reset_index()\n","    y_avg = (avg_df[\"true_label\"]==\"fake\").astype(int).values\n","    s_avg = avg_df[\"prob_fake\"].values\n","    flip_needed = roc_auc_score(y_avg, 1 - s_avg) > roc_auc_score(y_avg, s_avg)\n","    if flip_needed:\n","        df_use = df.copy(); df_use[\"prob_fake\"] = 1 - df_use[\"prob_fake\"]\n","    else:\n","        df_use = df\n","\n","    for blur_thr in TRY_BLUR_THR:\n","        df_b = df_use[df_use[\"blur\"] >= blur_thr] if blur_thr > 0 else df_use\n","\n","        for filt in TRY_CONF_FILT:\n","            if filt > 0:\n","                df_f = df_b[np.abs(df_b[\"prob_fake\"] - 0.5) >= filt].copy()\n","                missing = set(df_b[\"video_name\"].unique()) - set(df_f[\"video_name\"].unique())\n","                if missing:\n","                    df_f = pd.concat([df_f, df_b[df_b[\"video_name\"].isin(missing)]], ignore_index=True)\n","            else:\n","                df_f = df_b\n","\n","            grouped = df_f.groupby([\"video_name\",\"true_label\"])[\"prob_fake\"]\n","\n","            # median\n","            med = grouped.median().reset_index()\n","            y, s = (med[\"true_label\"]==\"fake\").astype(int).values, med[\"prob_fake\"].values\n","            auc, eer, ap, thr = video_metrics(s, y)\n","            desc = f\"norm={norm}|tta={tta}|flip={flip_needed}|pre={pre}|agg=median|conf={filt}|blur>={blur_thr}\"\n","            if (best is None) or (auc > best[0]) or (auc==best[0] and eer < best[1]):\n","                best = (auc, eer, ap, thr, desc, med)\n","\n","            # perc80\n","            perc = grouped.quantile(0.8).reset_index()\n","            y, s = (perc[\"true_label\"]==\"fake\").astype(int).values, perc[\"prob_fake\"].values\n","            auc_p, eer_p, ap_p, thr_p = video_metrics(s, y)\n","            if (auc_p > best[0]) or (auc_p==best[0] and eer_p < best[1]):\n","                best = (auc_p, eer_p, ap_p, thr_p, f\"...perc80...\", perc)\n","\n","            # top-k means\n","            tmp = df_f.copy(); tmp[\"rank\"] = tmp.groupby(\"video_name\")[\"prob_fake\"].rank(ascending=False, method=\"first\")\n","            for k in TOPK_LIST:\n","                topk = tmp[tmp[\"rank\"] <= k].groupby([\"video_name\",\"true_label\"])[\"prob_fake\"].mean().reset_index()\n","                if len(topk)==0: continue\n","                y, s = (topk[\"true_label\"]==\"fake\").astype(int).values, topk[\"prob_fake\"].values\n","                auc_k, eer_k, ap_k, thr_k = video_metrics(s, y)\n","                if (auc_k > best[0]) or (auc_k==best[0] and eer_k < best[1]):\n","                    best = (auc_k, eer_k, ap_k, thr_k, f\"...top{k}...\", topk)\n","\n","            # trimmed mean\n","            for trim in TRIM_LIST:\n","                tdf = grouped.apply(lambda v: trimmed_mean(v.values, trim)).reset_index(name=\"prob_fake\").dropna()\n","                if len(tdf)==0: continue\n","                y, s = (tdf[\"true_label\"]==\"fake\").astype(int).values, tdf[\"prob_fake\"].values\n","                auc_t, eer_t, ap_t, thr_t = video_metrics(s, y)\n","                if (auc_t > best[0]) or (auc_t==best[0] and eer_t < best[1]):\n","                    best = (auc_t, eer_t, ap_t, thr_t, f\"...trim{int(trim*100)}...\", tdf)\n","\n","            # log-sum-exp\n","            for a in LSE_ALPHA:\n","                lsed = grouped.apply(lambda v: logsumexp_pool(v.values, alpha=a)).reset_index(name=\"prob_fake\")\n","                y, s = (lsed[\"true_label\"]==\"fake\").astype(int).values, lsed[\"prob_fake\"].values\n","                auc_l, eer_l, ap_l, thr_l = video_metrics(s, y)\n","                if (auc_l > best[0]) or (auc_l==best[0] and eer_l < best[1]):\n","                    best = (auc_l, eer_l, ap_l, thr_l, f\"...lsep{a}...\", lsed)\n","\n","# save silently\n","best_auc, best_eer, best_ap, best_thr, best_desc, best_df = best\n","try:\n","    best_df.to_csv(CSV_PATH, index=False)\n","except Exception:\n","    pass\n","\n","print(f\"AUC={best_auc:.4f} | EER={best_eer:.4f} | AP={best_ap:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"ZlWeqWZRPTct","executionInfo":{"status":"error","timestamp":1755086277803,"user_tz":-120,"elapsed":906806,"user":{"displayName":"Vishnumaya","userId":"01919615312035119785"}},"outputId":"3f769cbb-d351-494b-ebd4-aa86541a2daa"},"execution_count":null,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2538757563.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtta\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mTRY_TTA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpre\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mTRY_PREPROC\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;31m# ---- search best per-video config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-2538757563.py\u001b[0m in \u001b[0;36mscore_frames\u001b[0;34m(norm_kind, tta, preproc)\u001b[0m\n\u001b[1;32m    111\u001b[0m                         num_workers=NUM_WORKERS, pin_memory=torch.cuda.is_available())\n\u001b[1;32m    112\u001b[0m     \u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblurs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0mxb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtta\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["# ===== CNN-Aug (EfficientNet-B4) — FAST SEARCH → PRINT METRICS ONLY =====\n","# Uses frames & weights already in Drive (no mount here).\n","REAL_FRAMES_DIR = \"/content/drive/My Drive/balanced_frames/real\"\n","FAKE_FRAMES_DIR = \"/content/drive/My Drive/balanced_frames/fake\"\n","WEIGHTS_PATH    = \"/content/drive/My Drive/DeepfakeBench_weights/effnb4_best.pth\"\n","\n","BATCH_SIZE = 32\n","NUM_WORKERS = 0       # 0 is more stable in Colab\n","IMG_SIZE   = 380\n","\n","# Small, effective search space\n","TRY_TTA       = [False, True]            # 2\n","TRY_NORM      = [\"no_norm\", \"imagenet\"]  # 2\n","TRY_PREPROC   = [\"short_center\"]         # 1 (better than stretch on faces)\n","TRY_CONF_FILT = [0.2, 0.3]               # 2\n","# Aggregations: 4 → total combos = 2*2*1*2*4 = 16\n","AGGS = (\"median\", \"perc80\", \"top10\", \"trim10\")\n","\n","import os, glob, re, sys, subprocess, numpy as np, pandas as pd\n","from PIL import Image\n","import torch, torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","from torchvision.transforms import functional as TF\n","from sklearn.metrics import roc_auc_score, average_precision_score, roc_curve\n","\n","# deps\n","def _pip_quiet(*pkgs):\n","    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", *pkgs],\n","                   stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=True)\n","\n","try:\n","    from efficientnet_pytorch import EfficientNet\n","except Exception:\n","    _pip_quiet(\"efficientnet-pytorch==0.7.1\")\n","    from efficientnet_pytorch import EfficientNet\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","torch.backends.cudnn.benchmark = True\n","\n","# ----- Model -----\n","class CNN_AUG_EffB4(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.backbone = nn.Module()\n","        self.backbone.efficientnet = EfficientNet.from_name('efficientnet-b4')\n","        self.backbone.efficientnet._fc = nn.Identity()\n","        self.backbone.last_layer = nn.Linear(1792, 2)  # [real, fake]\n","    def forward(self, x):\n","        x = self.backbone.efficientnet(x)\n","        x = self.backbone.last_layer(x)\n","        return x\n","\n","assert os.path.isfile(WEIGHTS_PATH), f\"Weights not found: {WEIGHTS_PATH}\"\n","model = CNN_AUG_EffB4().to(device)\n","state = torch.load(WEIGHTS_PATH, map_location=\"cpu\")\n","if isinstance(state, dict) and all(isinstance(k,str) for k in state.keys()):\n","    if all(k.startswith(\"module.\") for k in state.keys()):\n","        state = {k.replace(\"module.\",\"\",1): v for k,v in state.items()}\n","model.load_state_dict(state, strict=False)\n","model.eval()\n","softmax = torch.nn.Softmax(dim=1)\n","\n","# ----- Data -----\n","IMG_EXTS = (\".jpg\",\".jpeg\",\".png\",\".bmp\",\".webp\")\n","def is_img(p): return p.lower().endswith(IMG_EXTS)\n","\n","def infer_video_name(path):\n","    stem = os.path.splitext(os.path.basename(path))[0]\n","    m = re.split(r\"_frame\\d+$\", stem)\n","    if len(m) > 1 and m[0]: return m[0]\n","    m2 = re.sub(r\"[_\\-]\\d+$\", \"\", stem)\n","    return m2 if m2 and m2 != stem else stem\n","\n","def build_transform(norm, preproc):\n","    if preproc == \"short_center\":\n","        t_base = [transforms.Resize(IMG_SIZE), transforms.CenterCrop(IMG_SIZE)]\n","    else:  # fallback\n","        t_base = [transforms.Resize((IMG_SIZE, IMG_SIZE))]\n","    t_norm = [] if norm == \"no_norm\" else [transforms.Normalize([0.485,0.456,0.406],\n","                                                                [0.229,0.224,0.225])]\n","    return transforms.Compose(t_base + [transforms.ToTensor()] + t_norm)\n","\n","class FrameDataset(Dataset):\n","    def __init__(self, folders_labels, transform):\n","        files, labels = [], []\n","        for folder, lbl in folders_labels:\n","            f = sorted([p for p in glob.glob(os.path.join(folder, \"*\")) if is_img(p)])\n","            files += f; labels += [lbl]*len(f)\n","        self.files = files; self.labels = labels; self.t = transform\n","    def __len__(self): return len(self.files)\n","    def __getitem__(self, i):\n","        p = self.files[i]\n","        img = Image.open(p).convert(\"RGB\")\n","        return self.t(img), self.labels[i], p, infer_video_name(p)\n","\n","@torch.no_grad()\n","def score_frames(norm_kind=\"no_norm\", tta=False, preproc=\"short_center\"):\n","    t = build_transform(norm_kind, preproc)\n","    ds = FrameDataset([(REAL_FRAMES_DIR,0),(FAKE_FRAMES_DIR,1)], t)\n","    loader = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=False,\n","                        num_workers=NUM_WORKERS, pin_memory=torch.cuda.is_available())\n","    probs, labels, paths, vnames = [], [], [], []\n","    for xb, yb, pb, vb in loader:\n","        xb = xb.to(device, non_blocking=True)\n","        logits = (model(xb) + model(TF.hflip(xb))) / 2 if tta else model(xb)\n","        p_fake = softmax(logits)[:,1].detach().cpu().numpy()\n","        probs.append(p_fake); labels.append(yb.numpy()); paths += list(pb); vnames += list(vb)\n","    probs = np.concatenate(probs); labels = np.concatenate(labels)\n","    return pd.DataFrame({\"video_name\": vnames,\n","                         \"true_label\": np.where(labels==1,\"fake\",\"real\"),\n","                         \"prob_fake\": probs,\n","                         \"path\": paths})\n","\n","def video_metrics(scores, labels):\n","    auc = roc_auc_score(labels, scores)\n","    ap  = average_precision_score(labels, scores)\n","    fpr, tpr, thr = roc_curve(labels, scores); fnr = 1 - tpr\n","    idx = int(np.nanargmin(np.abs(fnr - fpr)))\n","    eer = float((fpr[idx] + fnr[idx]) / 2.0)\n","    thr_eer = float(thr[idx])\n","    return auc, eer, ap, thr_eer\n","\n","# cache per (norm, tta, preproc) — only 4 combos here\n","cache = {}\n","for norm in TRY_NORM:\n","    for tta in TRY_TTA:\n","        for pre in TRY_PREPROC:\n","            cache[(norm, tta, pre)] = score_frames(norm, tta, pre)\n","\n","# small, strong search over filters + aggs\n","best = None  # (AUC, EER, AP, thr, desc, per_video_df)\n","for (norm, tta, pre), df in cache.items():\n","    # auto-orient: flip scores if it improves per-video average AUC\n","    avg_df = df.groupby([\"video_name\",\"true_label\"])[\"prob_fake\"].mean().reset_index()\n","    y_avg = (avg_df[\"true_label\"]==\"fake\").astype(int).values\n","    s_avg = avg_df[\"prob_fake\"].values\n","    flip_needed = roc_auc_score(y_avg, 1 - s_avg) > roc_auc_score(y_avg, s_avg)\n","    if flip_needed:\n","        df_use = df.copy(); df_use[\"prob_fake\"] = 1 - df_use[\"prob_fake\"]\n","    else:\n","        df_use = df\n","\n","    for filt in TRY_CONF_FILT:\n","        if filt > 0:\n","            df_f = df_use[np.abs(df_use[\"prob_fake\"] - 0.5) >= filt].copy()\n","            missing = set(df_use[\"video_name\"].unique()) - set(df_f[\"video_name\"].unique())\n","            if missing:\n","                df_f = pd.concat([df_f, df_use[df_use[\"video_name\"].isin(missing)]], ignore_index=True)\n","        else:\n","            df_f = df_use\n","\n","        grouped = df_f.groupby([\"video_name\",\"true_label\"])[\"prob_fake\"]\n","\n","        # median\n","        if \"median\" in AGGS:\n","            med = grouped.median().reset_index()\n","            y, s = (med[\"true_label\"]==\"fake\").astype(int).values, med[\"prob_fake\"].values\n","            auc, eer, ap, thr = video_metrics(s, y)\n","            cand = (auc, eer, ap, thr, f\"{norm}|{tta}|flip={flip_needed}|median|f={filt}\", med)\n","            best = cand if (best is None or auc > best[0] or (auc==best[0] and eer < best[1])) else best\n","\n","        # perc80\n","        if \"perc80\" in AGGS:\n","            perc = grouped.quantile(0.8).reset_index()\n","            y, s = (perc[\"true_label\"]==\"fake\").astype(int).values, perc[\"prob_fake\"].values\n","            auc_p, eer_p, ap_p, thr_p = video_metrics(s, y)\n","            cand = (auc_p, eer_p, ap_p, thr_p, f\"{norm}|{tta}|flip={flip_needed}|perc80|f={filt}\", perc)\n","            best = cand if (auc_p > best[0] or (auc_p==best[0] and eer_p < best[1])) else best\n","\n","        # top10 mean\n","        if \"top10\" in AGGS:\n","            tmp = df_f.copy(); tmp[\"rank\"] = tmp.groupby(\"video_name\")[\"prob_fake\"].rank(ascending=False, method=\"first\")\n","            topk = tmp[tmp[\"rank\"] <= 10].groupby([\"video_name\",\"true_label\"])[\"prob_fake\"].mean().reset_index()\n","            if len(topk):\n","                y, s = (topk[\"true_label\"]==\"fake\").astype(int).values, topk[\"prob_fake\"].values\n","                auc_k, eer_k, ap_k, thr_k = video_metrics(s, y)\n","                cand = (auc_k, eer_k, ap_k, thr_k, f\"{norm}|{tta}|flip={flip_needed}|top10|f={filt}\", topk)\n","                best = cand if (auc_k > best[0] or (auc_k==best[0] and eer_k < best[1])) else best\n","\n","        # trimmed mean 10%\n","        if \"trim10\" in AGGS:\n","            tdf = grouped.apply(lambda v: float(np.mean(np.sort(v.values)[int(0.1*len(v)): max(int(len(v)-0.1*len(v)),1)])) if len(v)>0 else np.nan).reset_index(name=\"prob_fake\").dropna()\n","            if len(tdf):\n","                y, s = (tdf[\"true_label\"]==\"fake\").astype(int).values, tdf[\"prob_fake\"].values\n","                auc_t, eer_t, ap_t, thr_t = video_metrics(s, y)\n","                cand = (auc_t, eer_t, ap_t, thr_t, f\"{norm}|{tta}|flip={flip_needed}|trim10|f={filt}\", tdf)\n","                best = cand if (auc_t > best[0] or (auc_t==best[0] and eer_t < best[1])) else best\n","\n","# Print ONLY metrics\n","best_auc, best_eer, best_ap, best_thr, best_desc, best_df = best\n","print(f\"AUC={best_auc:.4f} | EER={best_eer:.4f} | AP={best_ap:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TZ6-NCd8TM47","executionInfo":{"status":"ok","timestamp":1755086762931,"user_tz":-120,"elapsed":383329,"user":{"displayName":"Vishnumaya","userId":"01919615312035119785"}},"outputId":"3e0e4daf-bf49-4263-ef9a-bedb628bcd7c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["AUC=0.7536 | EER=0.3431 | AP=0.7700\n"]}]},{"cell_type":"code","source":["# ================= CNN-Aug (EfficientNet-B4) — FULL PER-VIDEO TABLE =================\n","# Mount Drive, load model/weights, score frames, build table, print ALL rows, save CSV.\n","\n","# --- Config (edit paths if needed) ---\n","REAL_FRAMES_DIR = \"/content/drive/My Drive/balanced_frames/real\"\n","FAKE_FRAMES_DIR = \"/content/drive/My Drive/balanced_frames/fake\"\n","WEIGHTS_PATH    = \"/content/drive/My Drive/DeepfakeBench_weights/effnb4_best.pth\"\n","\n","DATASET_NAME  = \"balanced_ffpp\"\n","DETECTOR_NAME = \"CNN-Aug (EffB4)\"\n","SAVE_CSV_PATH = \"/content/drive/My Drive/cnn_aug_per_video_table.csv\"\n","\n","# Inference knobs\n","IMG_SIZE    = 380\n","BATCH_SIZE  = 32\n","NUM_WORKERS = 0            # 0 is most stable in Colab\n","USE_TTA     = True         # average original + hflip\n","RESIZE_MODE = \"short_center\"   # \"short_center\" (preserve aspect) or \"stretch\"\n","USE_IMAGENET_NORM = False      # \"no_norm\" often worked better for your EffNet\n","\n","# -----------------------------------------------------------------------------------\n","import os, glob, re, numpy as np, pandas as pd\n","from PIL import Image\n","import torch, torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","from torchvision.transforms import functional as TF\n","from sklearn.metrics import roc_curve, roc_auc_score, average_precision_score\n","\n","# Mount Drive (ok to mount again)\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=False)\n","\n","# Ensure package\n","try:\n","    from efficientnet_pytorch import EfficientNet\n","except Exception:\n","    import sys, subprocess\n","    subprocess.run([sys.executable,\"-m\",\"pip\",\"install\",\"-q\",\"efficientnet-pytorch==0.7.1\"], check=True)\n","    from efficientnet_pytorch import EfficientNet\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","torch.backends.cudnn.benchmark = True\n","\n","# --- Model (EfficientNet-B4 with 2-class head) ---\n","class CNN_AUG_EffB4(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.backbone = nn.Module()\n","        self.backbone.efficientnet = EfficientNet.from_name('efficientnet-b4')\n","        self.backbone.efficientnet._fc = nn.Identity()\n","        self.backbone.last_layer = nn.Linear(1792, 2)  # [real, fake]\n","    def forward(self, x):\n","        x = self.backbone.efficientnet(x)\n","        x = self.backbone.last_layer(x)\n","        return x\n","\n","assert os.path.isfile(WEIGHTS_PATH), f\"Weights not found: {WEIGHTS_PATH}\"\n","assert os.path.isdir(REAL_FRAMES_DIR) and os.path.isdir(FAKE_FRAMES_DIR), \"Check frame folders.\"\n","\n","model = CNN_AUG_EffB4().to(device)\n","state = torch.load(WEIGHTS_PATH, map_location=\"cpu\")\n","if isinstance(state, dict) and all(isinstance(k,str) for k in state.keys()):\n","    if all(k.startswith(\"module.\") for k in state.keys()):\n","        state = {k.replace(\"module.\",\"\",1): v for k,v in state.items()}\n","model.load_state_dict(state, strict=False)\n","model.eval()\n","softmax = torch.nn.Softmax(dim=1)\n","\n","# --- Data / transforms ---\n","IMG_EXTS = (\".jpg\",\".jpeg\",\".png\",\".bmp\",\".webp\")\n","def is_img(p): return p.lower().endswith(IMG_EXTS)\n","\n","def infer_video_name(path):\n","    stem = os.path.splitext(os.path.basename(path))[0]\n","    m = re.split(r\"_frame\\d+$\", stem)\n","    if len(m) > 1 and m[0]: return m[0]\n","    m2 = re.sub(r\"[_\\-]\\d+$\", \"\", stem)\n","    return m2 if m2 and m2 != stem else stem\n","\n","def build_transform():\n","    if RESIZE_MODE == \"short_center\":\n","        t_base = [transforms.Resize(IMG_SIZE), transforms.CenterCrop(IMG_SIZE)]\n","    else:  # \"stretch\"\n","        t_base = [transforms.Resize((IMG_SIZE, IMG_SIZE))]\n","    t_norm = [] if not USE_IMAGENET_NORM else [transforms.Normalize([0.485,0.456,0.406],\n","                                                                    [0.229,0.224,0.225])]\n","    return transforms.Compose(t_base + [transforms.ToTensor()] + t_norm)\n","\n","class FrameDataset(Dataset):\n","    def __init__(self, folders_labels, transform):\n","        files, labels = [], []\n","        for folder, lbl in folders_labels:\n","            f = sorted([p for p in glob.glob(os.path.join(folder, \"*\")) if is_img(p)])\n","            files += f; labels += [lbl]*len(f)\n","        self.files = files; self.labels = labels; self.t = transform\n","        assert len(self.files) > 0, \"No images found. Check your frame folders.\"\n","    def __len__(self): return len(self.files)\n","    def __getitem__(self, i):\n","        p = self.files[i]\n","        img = Image.open(p).convert(\"RGB\")\n","        return self.t(img), self.labels[i], p, infer_video_name(p)\n","\n","tform = build_transform()\n","ds = FrameDataset([(REAL_FRAMES_DIR,0),(FAKE_FRAMES_DIR,1)], tform)\n","loader = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=False,\n","                    num_workers=NUM_WORKERS, pin_memory=torch.cuda.is_available())\n","\n","# --- Batched inference ---\n","probs, labels, paths, vnames = [], [], [], []\n","with torch.no_grad():\n","    for xb, yb, pb, vb in loader:\n","        xb = xb.to(device, non_blocking=True)\n","        logits = (model(xb) + model(TF.hflip(xb))) / 2 if USE_TTA else model(xb)\n","        p_fake = softmax(logits)[:,1].detach().cpu().numpy()\n","        probs.append(p_fake); labels.append(yb.numpy()); paths += list(pb); vnames += list(vb)\n","\n","probs = np.concatenate(probs); labels = np.concatenate(labels)\n","\n","df = pd.DataFrame({\n","    \"video_name\": vnames,\n","    \"true_label\": np.where(labels==1, \"fake\", \"real\"),\n","    \"prob_fake\": probs,\n","    \"frame_path\": paths\n","})\n","\n","# --- Auto-orient scores (flip if it improves per-video avg AUC) ---\n","vid_avg_tmp = df.groupby([\"video_name\",\"true_label\"])[\"prob_fake\"].mean().reset_index()\n","y_tmp = (vid_avg_tmp[\"true_label\"]==\"fake\").astype(int).values\n","s_tmp = vid_avg_tmp[\"prob_fake\"].values\n","if roc_auc_score(y_tmp, 1 - s_tmp) > roc_auc_score(y_tmp, s_tmp):\n","    df[\"prob_fake\"] = 1 - df[\"prob_fake\"]\n","\n","# --- Thresholds ---\n","# 1) Per-video AVG threshold: EER on per-video averages\n","vid_avg = df.groupby([\"video_name\",\"true_label\"])[\"prob_fake\"].mean().reset_index()\n","y_v = (vid_avg[\"true_label\"]==\"fake\").astype(int).values\n","s_v = vid_avg[\"prob_fake\"].values\n","fpr_v, tpr_v, thr_v = roc_curve(y_v, s_v); fnr_v = 1 - tpr_v\n","i_v = int(np.nanargmin(np.abs(fnr_v - fpr_v)))\n","thr_avg = float(thr_v[i_v])\n","\n","# 2) Frame threshold for majority vote: EER on all frame scores\n","y_f = (df[\"true_label\"]==\"fake\").astype(int).values\n","s_f = df[\"prob_fake\"].values\n","fpr_f, tpr_f, thr_f = roc_curve(y_f, s_f); fnr_f = 1 - tpr_f\n","i_f = int(np.nanargmin(np.abs(fnr_f - fpr_f)))\n","thr_frame = float(thr_f[i_f])\n","\n","# --- Frame-level predictions for counts (majority threshold) ---\n","df[\"frame_pred\"]    = np.where(df[\"prob_fake\"] >= thr_frame, \"fake\", \"real\")\n","df[\"frame_correct\"] = (df[\"frame_pred\"] == df[\"true_label\"]).astype(int)\n","\n","# --- Summarize per video (your requested columns) ---\n","def summarize_video(group):\n","    n = len(group)\n","    n_correct = int(group[\"frame_correct\"].sum())\n","    n_wrong   = int(n - n_correct)\n","    acc = n_correct / n if n>0 else np.nan\n","    avg = float(group[\"prob_fake\"].mean()) if n>0 else np.nan\n","    std = float(group[\"prob_fake\"].std(ddof=0)) if n>1 else 0.0\n","\n","    # AVG-based decision at thr_avg\n","    pred_avg = \"fake\" if avg >= thr_avg else \"real\"\n","    correct_avg = int(pred_avg == group[\"true_label\"].iloc[0])\n","\n","    # Majority decision at thr_frame\n","    majority_ratio = (group[\"frame_pred\"] == \"fake\").mean()\n","    if majority_ratio == 0.5:\n","        pred_maj = pred_avg\n","    else:\n","        pred_maj = \"fake\" if majority_ratio > 0.5 else \"real\"\n","    correct_maj = int(pred_maj == group[\"true_label\"].iloc[0])\n","\n","    return pd.Series({\n","        \"dataset\": DATASET_NAME,\n","        \"detector\": DETECTOR_NAME,\n","        \"video_name\": group[\"video_name\"].iloc[0],\n","        \"true_label\": group[\"true_label\"].iloc[0],\n","        \"n_frames\": n,\n","        \"n_correct_frames\": n_correct,\n","        \"n_wrong_frames\": n_wrong,\n","        \"frame_accuracy\": round(acc, 4),\n","        \"avg_prob_fake\": round(avg, 4),\n","        \"std_prob_fake\": round(std, 4),\n","        \"video_pred_by_avg\": pred_avg,\n","        \"video_correct_by_avg\": correct_avg,\n","        \"video_pred_by_majority\": pred_maj,\n","        \"video_correct_by_majority\": correct_maj\n","    })\n","\n","per_video = df.groupby([\"video_name\",\"true_label\"], as_index=False).apply(summarize_video).reset_index(drop=True)\n","\n","# --- Print ALL rows only (no extra chatter) ---\n","pd.set_option(\"display.max_rows\", None)\n","pd.set_option(\"display.max_colwidth\", None)\n","print(per_video.sort_values([\"true_label\",\"video_name\"]).to_string(index=False))\n","\n","# --- Save to Drive ---\n","per_video.to_csv(SAVE_CSV_PATH, index=False)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"urWy50vY36bT","executionInfo":{"status":"ok","timestamp":1755600049631,"user_tz":-120,"elapsed":519324,"user":{"displayName":"Vishnumaya","userId":"01919615312035119785"}},"outputId":"57321ebc-bc30-4a27-b904-61a429859c34"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","      dataset        detector                            video_name true_label  n_frames  n_correct_frames  n_wrong_frames  frame_accuracy  avg_prob_fake  std_prob_fake video_pred_by_avg  video_correct_by_avg video_pred_by_majority  video_correct_by_majority\n","balanced_ffpp CNN-Aug (EffB4)                               000_003       fake        20                20               0            1.00         0.4766         0.0710              fake                     1                   fake                          1\n","balanced_ffpp CNN-Aug (EffB4)                               010_005       fake        20                19               1            0.95         0.5441         0.1119              fake                     1                   fake                          1\n","balanced_ffpp CNN-Aug (EffB4)                               011_805       fake        20                19               1            0.95         0.6810         0.1288              fake                     1                   fake                          1\n","balanced_ffpp CNN-Aug (EffB4)                               012_026       fake        20                20               0            1.00         0.4608         0.0489              fake                     1                   fake                          1\n","balanced_ffpp CNN-Aug (EffB4)                               013_883       fake        20                20               0            1.00         0.7538         0.0615              fake                     1                   fake                          1\n","balanced_ffpp CNN-Aug (EffB4)                               014_790       fake        20                 7              13            0.35         0.3306         0.1354              real                     0                   real                          0\n","balanced_ffpp CNN-Aug (EffB4)                               015_919       fake        20                20               0            1.00         0.5765         0.1024              fake                     1                   fake                          1\n","balanced_ffpp CNN-Aug (EffB4)                               016_209       fake        20                 4              16            0.20         0.3168         0.0641              real                     0                   real                          0\n","balanced_ffpp CNN-Aug (EffB4)                               017_803       fake        20                20               0            1.00         0.7468         0.1351              fake                     1                   fake                          1\n","balanced_ffpp CNN-Aug (EffB4)                               018_019       fake        20                20               0            1.00         0.6045         0.0981              fake                     1                   fake                          1\n","balanced_ffpp CNN-Aug (EffB4)                               019_018       fake        20                20               0            1.00         0.7926         0.0595              fake                     1                   fake                          1\n","balanced_ffpp CNN-Aug (EffB4)                               020_344       fake        20                 0              20            0.00         0.2730         0.0311              real                     0                   real                          0\n","balanced_ffpp CNN-Aug (EffB4)                               021_312       fake        20                 0              20            0.00         0.3086         0.0354              real                     0                   real                          0\n","balanced_ffpp CNN-Aug (EffB4)                               022_489       fake        20                 0              20            0.00         0.2175         0.0324              real                     0                   real                          0\n","balanced_ffpp CNN-Aug (EffB4)                               023_923       fake        20                 0              20            0.00         0.1217         0.0406              real                     0                   real                          0\n","balanced_ffpp CNN-Aug (EffB4)                               024_073       fake        20                12               8            0.60         0.4308         0.1609              fake                     1                   fake                          1\n","balanced_ffpp CNN-Aug (EffB4)                               025_067       fake        20                 4              16            0.20         0.3119         0.0525              real                     0                   real                          0\n","balanced_ffpp CNN-Aug (EffB4)                               026_012       fake        20                19               1            0.95         0.5026         0.1181              fake                     1                   fake                          1\n","balanced_ffpp CNN-Aug (EffB4)                               027_009       fake        20                12               8            0.60         0.4138         0.0811              fake                     1                   fake                          1\n","balanced_ffpp CNN-Aug (EffB4)                               028_068       fake        20                19               1            0.95         0.5922         0.1431              fake                     1                   fake                          1\n","balanced_ffpp CNN-Aug (EffB4)                               029_048       fake        20                20               0            1.00         0.6615         0.1241              fake                     1                   fake                          1\n","balanced_ffpp CNN-Aug (EffB4)                               030_193       fake        20                20               0            1.00         0.5938         0.1320              fake                     1                   fake                          1\n","balanced_ffpp CNN-Aug (EffB4)                               031_163       fake        20                18               2            0.90         0.5143         0.0938              fake                     1                   fake                          1\n","balanced_ffpp CNN-Aug (EffB4)                               032_944       fake        20                14               6            0.70         0.4272         0.0949              fake                     1                   fake                          1\n","balanced_ffpp CNN-Aug (EffB4)                               033_097       fake        20                 0              20            0.00         0.1902         0.0705              real                     0                   real                          0\n","balanced_ffpp CNN-Aug (EffB4)                               034_590       fake        20                20               0            1.00         0.8061         0.0892              fake                     1                   fake                          1\n","balanced_ffpp CNN-Aug (EffB4)                               035_036       fake        20                20               0            1.00         0.5675         0.0654              fake                     1                   fake                          1\n","balanced_ffpp CNN-Aug (EffB4)                               036_035       fake        20                17               3            0.85         0.4408         0.0692              fake                     1                   fake                          1\n","balanced_ffpp CNN-Aug (EffB4)                               037_072       fake        20                14               6            0.70         0.4224         0.0751              fake                     1                   fake                          1\n","balanced_ffpp CNN-Aug (EffB4)                               038_125       fake        20                 8              12            0.40         0.3620         0.0902              real                     0                   real                          0\n","balanced_ffpp CNN-Aug (EffB4)                               039_058       fake        20                 3              17            0.15         0.2455         0.0805              real                     0                   real                          0\n","balanced_ffpp CNN-Aug (EffB4)                               040_997       fake        20                16               4            0.80         0.4534         0.0855              fake                     1                   fake                          1\n","balanced_ffpp CNN-Aug (EffB4)                               041_063       fake        20                10              10            0.50         0.3594         0.1251              real                     0                   real                          0\n","balanced_ffpp CNN-Aug (EffB4)                               042_084       fake        20                20               0            1.00         0.7224         0.1188              fake                     1                   fake                          1\n","balanced_ffpp CNN-Aug (EffB4)                               043_110       fake        20                 1              19            0.05         0.2799         0.0609              real                     0                   real                          0\n","balanced_ffpp CNN-Aug (EffB4)                               044_945       fake        20                14               6            0.70         0.4615         0.1142              fake                     1                   fake                          1\n","balanced_ffpp CNN-Aug (EffB4)                               045_889       fake        20                 3              17            0.15         0.3515         0.0597              real                     0                   real                          0\n","balanced_ffpp CNN-Aug (EffB4)                               046_904       fake        20                 1              19            0.05         0.2177         0.0549              real                     0                   real                          0\n","balanced_ffpp CNN-Aug (EffB4)                               047_862       fake        20                10              10            0.50         0.3673         0.0878              real                     0                   real                          0\n","balanced_ffpp CNN-Aug (EffB4)                               048_029       fake        20                 1              19            0.05         0.2284         0.0604              real                     0                   real                          0\n","balanced_ffpp CNN-Aug (EffB4)                               049_946       fake        20                20               0            1.00         0.8036         0.0510              fake                     1                   fake                          1\n","balanced_ffpp CNN-Aug (EffB4)                               050_059       fake        20                 3              17            0.15         0.2999         0.1110              real                     0                   real                          0\n","balanced_ffpp CNN-Aug (EffB4)                               051_332       fake        20                17               3            0.85         0.4499         0.0713              fake                     1                   fake                          1\n","balanced_ffpp CNN-Aug (EffB4)                               052_108       fake        20                20               0            1.00         0.6894         0.1020              fake                     1                   fake                          1\n","balanced_ffpp CNN-Aug (EffB4)                               053_095       fake        20                11               9            0.55         0.3723         0.0989              real                     0                   fake                          1\n","balanced_ffpp CNN-Aug (EffB4)                               054_071       fake        20                18               2            0.90         0.5676         0.1073              fake                     1                   fake                          1\n","balanced_ffpp CNN-Aug (EffB4)                               055_147       fake        20                18               2            0.90         0.4620         0.0924              fake                     1                   fake                          1\n","balanced_ffpp CNN-Aug (EffB4)                               056_996       fake        20                 6              14            0.30         0.3424         0.0596              real                     0                   real                          0\n","balanced_ffpp CNN-Aug (EffB4)                               057_070       fake        20                20               0            1.00         0.7693         0.1106              fake                     1                   fake                          1\n","balanced_ffpp CNN-Aug (EffB4)                               058_039       fake        20                16               4            0.80         0.5299         0.1714              fake                     1                   fake                          1\n","balanced_ffpp CNN-Aug (EffB4)                               059_050       fake        20                15               5            0.75         0.4640         0.1240              fake                     1                   fake                          1\n","balanced_ffpp CNN-Aug (EffB4)    04__walking_outside_cafe_disgusted       real        20                20               0            1.00         0.1367         0.0381              real                     1                   real                          1\n","balanced_ffpp CNN-Aug (EffB4)                   05__exit_phone_room       real        20                20               0            1.00         0.1567         0.0616              real                     1                   real                          1\n","balanced_ffpp CNN-Aug (EffB4)                     05__hugging_happy       real        20                12               8            0.60         0.3522         0.0777              real                     1                   real                          1\n","balanced_ffpp CNN-Aug (EffB4)                       05__kitchen_pan       real        20                11               9            0.55         0.3824         0.1397              fake                     0                   real                          1\n","balanced_ffpp CNN-Aug (EffB4)                     05__kitchen_still       real        20                18               2            0.90         0.3318         0.0329              real                     1                   real                          1\n","balanced_ffpp CNN-Aug (EffB4)      05__outside_talking_pan_laughing       real        20                 3              17            0.15         0.4821         0.1390              fake                     0                   fake                          0\n","balanced_ffpp CNN-Aug (EffB4)    05__outside_talking_still_laughing       real        20                18               2            0.90         0.3001         0.0476              real                     1                   real                          1\n","balanced_ffpp CNN-Aug (EffB4)               05__podium_speech_happy       real        20                18               2            0.90         0.3077         0.0549              real                     1                   real                          1\n","balanced_ffpp CNN-Aug (EffB4)              05__talking_against_wall       real        20                20               0            1.00         0.1075         0.0156              real                     1                   real                          1\n","balanced_ffpp CNN-Aug (EffB4)              05__walk_down_hall_angry       real        20                20               0            1.00         0.1799         0.0708              real                     1                   real                          1\n","balanced_ffpp CNN-Aug (EffB4) 05__walking_down_street_outside_angry       real        20                15               5            0.75         0.3065         0.0743              real                     1                   real                          1\n","balanced_ffpp CNN-Aug (EffB4)    05__walking_outside_cafe_disgusted       real        20                20               0            1.00         0.0922         0.0341              real                     1                   real                          1\n","balanced_ffpp CNN-Aug (EffB4)                   06__exit_phone_room       real        20                20               0            1.00         0.1885         0.0569              real                     1                   real                          1\n","balanced_ffpp CNN-Aug (EffB4)                     06__hugging_happy       real        20                 5              15            0.25         0.4630         0.1277              fake                     0                   fake                          0\n","balanced_ffpp CNN-Aug (EffB4)                       06__kitchen_pan       real        20                11               9            0.55         0.3720         0.1313              real                     1                   real                          1\n","balanced_ffpp CNN-Aug (EffB4)                     06__kitchen_still       real        20                11               9            0.55         0.3651         0.0755              real                     1                   real                          1\n","balanced_ffpp CNN-Aug (EffB4)      06__outside_talking_pan_laughing       real        20                 4              16            0.20         0.5220         0.1625              fake                     0                   fake                          0\n","balanced_ffpp CNN-Aug (EffB4)    06__outside_talking_still_laughing       real        20                 7              13            0.35         0.4127         0.0589              fake                     0                   fake                          0\n","balanced_ffpp CNN-Aug (EffB4)               06__podium_speech_happy       real        20                17               3            0.85         0.2704         0.0902              real                     1                   real                          1\n","balanced_ffpp CNN-Aug (EffB4)              06__talking_against_wall       real        20                16               4            0.80         0.2880         0.0674              real                     1                   real                          1\n","balanced_ffpp CNN-Aug (EffB4)               06__talking_angry_couch       real        20                12               8            0.60         0.3305         0.0925              real                     1                   real                          1\n","balanced_ffpp CNN-Aug (EffB4)              06__walk_down_hall_angry       real        20                20               0            1.00         0.1809         0.0481              real                     1                   real                          1\n","balanced_ffpp CNN-Aug (EffB4)     06__walking_and_outside_surprised       real        20                10              10            0.50         0.3590         0.0901              real                     1                   real                          1\n","balanced_ffpp CNN-Aug (EffB4)  06__walking_down_indoor_hall_disgust       real        20                 9              11            0.45         0.4160         0.1214              fake                     0                   fake                          0\n","balanced_ffpp CNN-Aug (EffB4) 06__walking_down_street_outside_angry       real        20                 4              16            0.20         0.4349         0.0481              fake                     0                   fake                          0\n","balanced_ffpp CNN-Aug (EffB4)    06__walking_outside_cafe_disgusted       real        20                20               0            1.00         0.2135         0.0902              real                     1                   real                          1\n","balanced_ffpp CNN-Aug (EffB4)                   07__exit_phone_room       real        20                20               0            1.00         0.2411         0.0578              real                     1                   real                          1\n","balanced_ffpp CNN-Aug (EffB4)                     07__hugging_happy       real        20                 9              11            0.45         0.3972         0.0760              fake                     0                   fake                          0\n","balanced_ffpp CNN-Aug (EffB4)                       07__kitchen_pan       real        20                 1              19            0.05         0.5517         0.1068              fake                     0                   fake                          0\n","balanced_ffpp CNN-Aug (EffB4)                     07__kitchen_still       real        20                 0              20            0.00         0.5035         0.0593              fake                     0                   fake                          0\n","balanced_ffpp CNN-Aug (EffB4)      07__outside_talking_pan_laughing       real        20                 8              12            0.40         0.4598         0.2226              fake                     0                   fake                          0\n","balanced_ffpp CNN-Aug (EffB4)    07__outside_talking_still_laughing       real        20                12               8            0.60         0.3235         0.1058              real                     1                   real                          1\n","balanced_ffpp CNN-Aug (EffB4)               07__podium_speech_happy       real        20                 0              20            0.00         0.5380         0.0366              fake                     0                   fake                          0\n","balanced_ffpp CNN-Aug (EffB4)               07__secret_conversation       real        20                 0              20            0.00         0.5305         0.0352              fake                     0                   fake                          0\n","balanced_ffpp CNN-Aug (EffB4)              07__talking_against_wall       real        20                20               0            1.00         0.1924         0.0480              real                     1                   real                          1\n","balanced_ffpp CNN-Aug (EffB4)               07__talking_angry_couch       real        20                 0              20            0.00         0.5532         0.0526              fake                     0                   fake                          0\n","balanced_ffpp CNN-Aug (EffB4)              07__walk_down_hall_angry       real        20                20               0            1.00         0.2552         0.0476              real                     1                   real                          1\n","balanced_ffpp CNN-Aug (EffB4) 07__walking_down_street_outside_angry       real        20                 7              13            0.35         0.4197         0.0844              fake                     0                   fake                          0\n","balanced_ffpp CNN-Aug (EffB4)    07__walking_outside_cafe_disgusted       real        20                20               0            1.00         0.1666         0.0392              real                     1                   real                          1\n","balanced_ffpp CNN-Aug (EffB4)                   08__exit_phone_room       real        20                20               0            1.00         0.1271         0.0587              real                     1                   real                          1\n","balanced_ffpp CNN-Aug (EffB4)                       08__kitchen_pan       real        20                 4              16            0.20         0.5094         0.1255              fake                     0                   fake                          0\n","balanced_ffpp CNN-Aug (EffB4)                     08__kitchen_still       real        20                 1              19            0.05         0.4971         0.0704              fake                     0                   fake                          0\n","balanced_ffpp CNN-Aug (EffB4)      08__outside_talking_pan_laughing       real        20                11               9            0.55         0.3960         0.2079              fake                     0                   real                          1\n","balanced_ffpp CNN-Aug (EffB4)    08__outside_talking_still_laughing       real        20                20               0            1.00         0.2556         0.0657              real                     1                   real                          1\n","balanced_ffpp CNN-Aug (EffB4)               08__podium_speech_happy       real        20                17               3            0.85         0.2583         0.1012              real                     1                   real                          1\n","balanced_ffpp CNN-Aug (EffB4)              08__talking_against_wall       real        20                20               0            1.00         0.1365         0.0284              real                     1                   real                          1\n","balanced_ffpp CNN-Aug (EffB4)              08__walk_down_hall_angry       real        20                20               0            1.00         0.1947         0.0618              real                     1                   real                          1\n","balanced_ffpp CNN-Aug (EffB4) 08__walking_down_street_outside_angry       real        20                13               7            0.65         0.3316         0.0826              real                     1                   real                          1\n","balanced_ffpp CNN-Aug (EffB4)    08__walking_outside_cafe_disgusted       real        20                20               0            1.00         0.1198         0.0306              real                     1                   real                          1\n","balanced_ffpp CNN-Aug (EffB4)                   09__exit_phone_room       real        20                20               0            1.00         0.1048         0.0475              real                     1                   real                          1\n","balanced_ffpp CNN-Aug (EffB4)                       09__kitchen_pan       real        20                 4              16            0.20         0.4599         0.1290              fake                     0                   fake                          0\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-1910872554.py:192: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  per_video = df.groupby([\"video_name\",\"true_label\"], as_index=False).apply(summarize_video).reset_index(drop=True)\n"]}]},{"cell_type":"code","source":["# Save your per-video table to Google Drive\n","import os, time\n","from google.colab import drive\n","\n","# Make sure Drive is mounted (safe to call again)\n","drive.mount('/content/drive', force_remount=False)\n","\n","# Ensure the table exists\n","assert 'per_video' in globals(), \"Run the previous cell first to create the 'per_video' DataFrame.\"\n","\n","# Choose where/what to save\n","DRIVE_DIR = \"/content/drive/My Drive/deepfake_results\"\n","os.makedirs(DRIVE_DIR, exist_ok=True)\n","\n","FILENAME = \"cnn_aug_per_video_table.csv\"\n","# Or timestamped:\n","# FILENAME = f\"cnn_aug_per_video_table_{time.strftime('%Y%m%d-%H%M%S')}.csv\"\n","\n","SAVE_PATH = os.path.join(DRIVE_DIR, FILENAME)\n","per_video.to_csv(SAVE_PATH, index=False)\n","print(\"Saved to:\", SAVE_PATH)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wZIaSQ547doI","executionInfo":{"status":"ok","timestamp":1755600253362,"user_tz":-120,"elapsed":2330,"user":{"displayName":"Vishnumaya","userId":"01919615312035119785"}},"outputId":"039c1ffd-e357-4686-afb4-5d0469216cf7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Saved to: /content/drive/My Drive/deepfake_results/cnn_aug_per_video_table.csv\n"]}]},{"cell_type":"code","source":["# === Compact per-video table: dataset, detector, video_name, true_label, correctly_predicted (yes/no) ===\n","import numpy as np, pandas as pd\n","from sklearn.metrics import roc_curve\n","\n","# Use the per_video table from the previous step\n","assert 'per_video' in globals(), \"Run the CNN-Aug per-video table cell first to create 'per_video'.\"\n","\n","pv = per_video.copy()\n","\n","# If the table doesn't already have correctness by average, compute it via per-video EER on avg_prob_fake\n","if \"video_correct_by_avg\" not in pv.columns or \"video_pred_by_avg\" not in pv.columns:\n","    y = (pv[\"true_label\"] == \"fake\").astype(int).values\n","    s = pv[\"avg_prob_fake\"].values\n","    fpr, tpr, thr = roc_curve(y, s)\n","    fnr = 1 - tpr\n","    idx = int(np.nanargmin(np.abs(fnr - fpr)))\n","    thr_use = float(thr[idx])\n","    pv[\"video_pred_by_avg\"] = np.where(pv[\"avg_prob_fake\"] >= thr_use, \"fake\", \"real\")\n","    pv[\"video_correct_by_avg\"] = (pv[\"video_pred_by_avg\"] == pv[\"true_label\"]).astype(int)\n","\n","# Build compact table\n","cols = {\n","    \"dataset\": pv[\"dataset\"] if \"dataset\" in pv.columns else \"balanced_ffpp\",\n","    \"detector\": pv[\"detector\"] if \"detector\" in pv.columns else \"CNN-Aug (EffB4)\",\n","    \"video_name\": pv[\"video_name\"],\n","    \"true_label\": pv[\"true_label\"],\n","    \"correctly_predicted\": pv[\"video_correct_by_avg\"].map({1: \"yes\", 0: \"no\"})\n","}\n","out = pd.DataFrame(cols)\n","\n","# Print ONLY the table (all rows)\n","pd.set_option(\"display.max_rows\", None)\n","pd.set_option(\"display.max_colwidth\", None)\n","print(out.sort_values([\"true_label\",\"video_name\"]).to_string(index=False))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OwYmulyA_Xap","executionInfo":{"status":"ok","timestamp":1755601274070,"user_tz":-120,"elapsed":78,"user":{"displayName":"Vishnumaya","userId":"01919615312035119785"}},"outputId":"67dcdcc9-8fec-4682-ed17-8b4ee3f18fb2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["      dataset        detector                            video_name true_label correctly_predicted\n","balanced_ffpp CNN-Aug (EffB4)                               000_003       fake                 yes\n","balanced_ffpp CNN-Aug (EffB4)                               010_005       fake                 yes\n","balanced_ffpp CNN-Aug (EffB4)                               011_805       fake                 yes\n","balanced_ffpp CNN-Aug (EffB4)                               012_026       fake                 yes\n","balanced_ffpp CNN-Aug (EffB4)                               013_883       fake                 yes\n","balanced_ffpp CNN-Aug (EffB4)                               014_790       fake                  no\n","balanced_ffpp CNN-Aug (EffB4)                               015_919       fake                 yes\n","balanced_ffpp CNN-Aug (EffB4)                               016_209       fake                  no\n","balanced_ffpp CNN-Aug (EffB4)                               017_803       fake                 yes\n","balanced_ffpp CNN-Aug (EffB4)                               018_019       fake                 yes\n","balanced_ffpp CNN-Aug (EffB4)                               019_018       fake                 yes\n","balanced_ffpp CNN-Aug (EffB4)                               020_344       fake                  no\n","balanced_ffpp CNN-Aug (EffB4)                               021_312       fake                  no\n","balanced_ffpp CNN-Aug (EffB4)                               022_489       fake                  no\n","balanced_ffpp CNN-Aug (EffB4)                               023_923       fake                  no\n","balanced_ffpp CNN-Aug (EffB4)                               024_073       fake                 yes\n","balanced_ffpp CNN-Aug (EffB4)                               025_067       fake                  no\n","balanced_ffpp CNN-Aug (EffB4)                               026_012       fake                 yes\n","balanced_ffpp CNN-Aug (EffB4)                               027_009       fake                 yes\n","balanced_ffpp CNN-Aug (EffB4)                               028_068       fake                 yes\n","balanced_ffpp CNN-Aug (EffB4)                               029_048       fake                 yes\n","balanced_ffpp CNN-Aug (EffB4)                               030_193       fake                 yes\n","balanced_ffpp CNN-Aug (EffB4)                               031_163       fake                 yes\n","balanced_ffpp CNN-Aug (EffB4)                               032_944       fake                 yes\n","balanced_ffpp CNN-Aug (EffB4)                               033_097       fake                  no\n","balanced_ffpp CNN-Aug (EffB4)                               034_590       fake                 yes\n","balanced_ffpp CNN-Aug (EffB4)                               035_036       fake                 yes\n","balanced_ffpp CNN-Aug (EffB4)                               036_035       fake                 yes\n","balanced_ffpp CNN-Aug (EffB4)                               037_072       fake                 yes\n","balanced_ffpp CNN-Aug (EffB4)                               038_125       fake                  no\n","balanced_ffpp CNN-Aug (EffB4)                               039_058       fake                  no\n","balanced_ffpp CNN-Aug (EffB4)                               040_997       fake                 yes\n","balanced_ffpp CNN-Aug (EffB4)                               041_063       fake                  no\n","balanced_ffpp CNN-Aug (EffB4)                               042_084       fake                 yes\n","balanced_ffpp CNN-Aug (EffB4)                               043_110       fake                  no\n","balanced_ffpp CNN-Aug (EffB4)                               044_945       fake                 yes\n","balanced_ffpp CNN-Aug (EffB4)                               045_889       fake                  no\n","balanced_ffpp CNN-Aug (EffB4)                               046_904       fake                  no\n","balanced_ffpp CNN-Aug (EffB4)                               047_862       fake                  no\n","balanced_ffpp CNN-Aug (EffB4)                               048_029       fake                  no\n","balanced_ffpp CNN-Aug (EffB4)                               049_946       fake                 yes\n","balanced_ffpp CNN-Aug (EffB4)                               050_059       fake                  no\n","balanced_ffpp CNN-Aug (EffB4)                               051_332       fake                 yes\n","balanced_ffpp CNN-Aug (EffB4)                               052_108       fake                 yes\n","balanced_ffpp CNN-Aug (EffB4)                               053_095       fake                  no\n","balanced_ffpp CNN-Aug (EffB4)                               054_071       fake                 yes\n","balanced_ffpp CNN-Aug (EffB4)                               055_147       fake                 yes\n","balanced_ffpp CNN-Aug (EffB4)                               056_996       fake                  no\n","balanced_ffpp CNN-Aug (EffB4)                               057_070       fake                 yes\n","balanced_ffpp CNN-Aug (EffB4)                               058_039       fake                 yes\n","balanced_ffpp CNN-Aug (EffB4)                               059_050       fake                 yes\n","balanced_ffpp CNN-Aug (EffB4)    04__walking_outside_cafe_disgusted       real                 yes\n","balanced_ffpp CNN-Aug (EffB4)                   05__exit_phone_room       real                 yes\n","balanced_ffpp CNN-Aug (EffB4)                     05__hugging_happy       real                 yes\n","balanced_ffpp CNN-Aug (EffB4)                       05__kitchen_pan       real                  no\n","balanced_ffpp CNN-Aug (EffB4)                     05__kitchen_still       real                 yes\n","balanced_ffpp CNN-Aug (EffB4)      05__outside_talking_pan_laughing       real                  no\n","balanced_ffpp CNN-Aug (EffB4)    05__outside_talking_still_laughing       real                 yes\n","balanced_ffpp CNN-Aug (EffB4)               05__podium_speech_happy       real                 yes\n","balanced_ffpp CNN-Aug (EffB4)              05__talking_against_wall       real                 yes\n","balanced_ffpp CNN-Aug (EffB4)              05__walk_down_hall_angry       real                 yes\n","balanced_ffpp CNN-Aug (EffB4) 05__walking_down_street_outside_angry       real                 yes\n","balanced_ffpp CNN-Aug (EffB4)    05__walking_outside_cafe_disgusted       real                 yes\n","balanced_ffpp CNN-Aug (EffB4)                   06__exit_phone_room       real                 yes\n","balanced_ffpp CNN-Aug (EffB4)                     06__hugging_happy       real                  no\n","balanced_ffpp CNN-Aug (EffB4)                       06__kitchen_pan       real                 yes\n","balanced_ffpp CNN-Aug (EffB4)                     06__kitchen_still       real                 yes\n","balanced_ffpp CNN-Aug (EffB4)      06__outside_talking_pan_laughing       real                  no\n","balanced_ffpp CNN-Aug (EffB4)    06__outside_talking_still_laughing       real                  no\n","balanced_ffpp CNN-Aug (EffB4)               06__podium_speech_happy       real                 yes\n","balanced_ffpp CNN-Aug (EffB4)              06__talking_against_wall       real                 yes\n","balanced_ffpp CNN-Aug (EffB4)               06__talking_angry_couch       real                 yes\n","balanced_ffpp CNN-Aug (EffB4)              06__walk_down_hall_angry       real                 yes\n","balanced_ffpp CNN-Aug (EffB4)     06__walking_and_outside_surprised       real                 yes\n","balanced_ffpp CNN-Aug (EffB4)  06__walking_down_indoor_hall_disgust       real                  no\n","balanced_ffpp CNN-Aug (EffB4) 06__walking_down_street_outside_angry       real                  no\n","balanced_ffpp CNN-Aug (EffB4)    06__walking_outside_cafe_disgusted       real                 yes\n","balanced_ffpp CNN-Aug (EffB4)                   07__exit_phone_room       real                 yes\n","balanced_ffpp CNN-Aug (EffB4)                     07__hugging_happy       real                  no\n","balanced_ffpp CNN-Aug (EffB4)                       07__kitchen_pan       real                  no\n","balanced_ffpp CNN-Aug (EffB4)                     07__kitchen_still       real                  no\n","balanced_ffpp CNN-Aug (EffB4)      07__outside_talking_pan_laughing       real                  no\n","balanced_ffpp CNN-Aug (EffB4)    07__outside_talking_still_laughing       real                 yes\n","balanced_ffpp CNN-Aug (EffB4)               07__podium_speech_happy       real                  no\n","balanced_ffpp CNN-Aug (EffB4)               07__secret_conversation       real                  no\n","balanced_ffpp CNN-Aug (EffB4)              07__talking_against_wall       real                 yes\n","balanced_ffpp CNN-Aug (EffB4)               07__talking_angry_couch       real                  no\n","balanced_ffpp CNN-Aug (EffB4)              07__walk_down_hall_angry       real                 yes\n","balanced_ffpp CNN-Aug (EffB4) 07__walking_down_street_outside_angry       real                  no\n","balanced_ffpp CNN-Aug (EffB4)    07__walking_outside_cafe_disgusted       real                 yes\n","balanced_ffpp CNN-Aug (EffB4)                   08__exit_phone_room       real                 yes\n","balanced_ffpp CNN-Aug (EffB4)                       08__kitchen_pan       real                  no\n","balanced_ffpp CNN-Aug (EffB4)                     08__kitchen_still       real                  no\n","balanced_ffpp CNN-Aug (EffB4)      08__outside_talking_pan_laughing       real                  no\n","balanced_ffpp CNN-Aug (EffB4)    08__outside_talking_still_laughing       real                 yes\n","balanced_ffpp CNN-Aug (EffB4)               08__podium_speech_happy       real                 yes\n","balanced_ffpp CNN-Aug (EffB4)              08__talking_against_wall       real                 yes\n","balanced_ffpp CNN-Aug (EffB4)              08__walk_down_hall_angry       real                 yes\n","balanced_ffpp CNN-Aug (EffB4) 08__walking_down_street_outside_angry       real                 yes\n","balanced_ffpp CNN-Aug (EffB4)    08__walking_outside_cafe_disgusted       real                 yes\n","balanced_ffpp CNN-Aug (EffB4)                   09__exit_phone_room       real                 yes\n","balanced_ffpp CNN-Aug (EffB4)                       09__kitchen_pan       real                  no\n"]}]},{"cell_type":"code","source":["# Save the compact table (DataFrame `out`) to Google Drive\n","import os\n","from google.colab import drive\n","\n","# Ensure Drive is mounted and the table exists\n","drive.mount('/content/drive', force_remount=False)\n","assert 'out' in globals(), \"Run the previous cell to create the compact table 'out'.\"\n","\n","SAVE_DIR = \"/content/drive/My Drive/CNN Aug results\"\n","os.makedirs(SAVE_DIR, exist_ok=True)\n","FILENAME = \"cnn_aug_prediction_compact.csv\"  # change name if you like\n","\n","path = os.path.join(SAVE_DIR, FILENAME)\n","out.to_csv(path, index=False)\n","print(\"Saved to:\", path)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mYceVOlO_kKN","executionInfo":{"status":"ok","timestamp":1755601350451,"user_tz":-120,"elapsed":1875,"user":{"displayName":"Vishnumaya","userId":"01919615312035119785"}},"outputId":"7f8856bc-f550-4dd6-ac76-0f66d771b04a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Saved to: /content/drive/My Drive/CNN Aug results/cnn_aug_prediction_compact.csv\n"]}]}]}