{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMf1YHU7pKKAhHi4ZCMsUFk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"1645f5ef35a548bcbcec8ff624180091":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_52f6ead1c52945189bbdb7e87e5025de","IPY_MODEL_98e1d1fd160142c19e2a139c8853c2db","IPY_MODEL_d6321655734d4eda893f13e90c81f591"],"layout":"IPY_MODEL_3e8e5c7f52da4d878d724019e99d24d3"}},"52f6ead1c52945189bbdb7e87e5025de":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4e6c6521d1bc4108b0c33e712c96fa7f","placeholder":"​","style":"IPY_MODEL_343505fb4be34fffbab387c68e29da7f","value":"model.safetensors: 100%"}},"98e1d1fd160142c19e2a139c8853c2db":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_edaed647c45947fcb02356305c00fcad","max":108392156,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6e206ad4ab914a30b34daf1d8eea0b75","value":108392156}},"d6321655734d4eda893f13e90c81f591":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3f417bbcb27a4187b05bb317e448b509","placeholder":"​","style":"IPY_MODEL_12073ddbe71043c08e79bc3cdf59e731","value":" 108M/108M [00:02&lt;00:00, 80.2MB/s]"}},"3e8e5c7f52da4d878d724019e99d24d3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4e6c6521d1bc4108b0c33e712c96fa7f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"343505fb4be34fffbab387c68e29da7f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"edaed647c45947fcb02356305c00fcad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e206ad4ab914a30b34daf1d8eea0b75":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3f417bbcb27a4187b05bb317e448b509":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"12073ddbe71043c08e79bc3cdf59e731":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":468,"referenced_widgets":["1645f5ef35a548bcbcec8ff624180091","52f6ead1c52945189bbdb7e87e5025de","98e1d1fd160142c19e2a139c8853c2db","d6321655734d4eda893f13e90c81f591","3e8e5c7f52da4d878d724019e99d24d3","4e6c6521d1bc4108b0c33e712c96fa7f","343505fb4be34fffbab387c68e29da7f","edaed647c45947fcb02356305c00fcad","6e206ad4ab914a30b34daf1d8eea0b75","3f417bbcb27a4187b05bb317e448b509","12073ddbe71043c08e79bc3cdf59e731"]},"id":"bCdcBQaiKfrf","executionInfo":{"status":"ok","timestamp":1761058677075,"user_tz":-120,"elapsed":248661,"user":{"displayName":"Vishnumaya","userId":"01919615312035119785"}},"outputId":"26066651-b863-49a1-d625-568061c5b6ec"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","DATA_ROOT: /content/drive/MyDrive/frames_cropped_faces_10src\n","WEIGHT_PATH: /content/drive/MyDrive/DeepfakeBench_weights/xception_best.pth\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDevice: cuda\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/108M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1645f5ef35a548bcbcec8ff624180091"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Loaded with strict=False.\n","  missing keys: 422\n","  unexpected keys: 283\n","[WARN] Checkpoint keys didn't match this Xception; keeping ImageNet-pretrained weights.\n","\n","=== Xception (video-level, median) ===\n","AUC: 0.5060\n","EER: 0.4700\n","AP : 0.5230\n","(used 1-p flip: YES)\n","Per-video CSV: /content/drive/MyDrive/Xception_eval_frames_cropped_faces_10src_median.csv\n"]}],"source":["# ===================== Xception eval on frames_cropped_faces_10src (AUC/EER/AP) =====================\n","# - Per-video median aggregation from frames named like <videoName>_frames_01.jpg ...\n","# - Auto \"orientation fix\": uses (1 - p) if it yields better video-level AUC\n","# - Prints AUC, EER, AP\n","\n","# 0) Setup & mounts\n","import os, re, glob, math, json, time, random\n","import numpy as np\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","DRIVE_ROOT = \"/content/drive/MyDrive\" if os.path.exists(\"/content/drive/MyDrive\") else \"/content/drive/My Drive\"\n","DATA_ROOT  = os.path.join(DRIVE_ROOT, \"frames_cropped_faces_10src\")   # <-- your dataset {real,fake}\n","WEIGHT_PATH= os.path.join(DRIVE_ROOT, \"DeepfakeBench_weights\", \"xception_best.pth\")\n","\n","print(\"DATA_ROOT:\", DATA_ROOT)\n","print(\"WEIGHT_PATH:\", WEIGHT_PATH)\n","\n","# 1) Deps\n","!pip -q install timm==0.9.12\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","import torchvision.transforms as T\n","from PIL import Image\n","from sklearn.metrics import roc_auc_score, average_precision_score, roc_curve\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Device:\", device)\n","\n","# 2) Dataset: flat frames -> (img, label, video_key)\n","FRAME_KEY_RE = re.compile(r\"^(.*?)(?:[_-]frames?[_-]?\\d+|[_-]frame[_-]?\\d+)$\", re.IGNORECASE)\n","def get_video_key(basename):\n","    base = os.path.splitext(basename)[0]\n","    m = FRAME_KEY_RE.match(base)\n","    return m.group(1) if m else base.split(\"_\")[0]\n","\n","class FramesDataset(Dataset):\n","    def __init__(self, root):\n","        self.samples = []\n","        for cls, y in ((\"real\",0), (\"fake\",1)):\n","            folder = os.path.join(root, cls)\n","            exts = (\".jpg\",\".jpeg\",\".png\",\".bmp\",\".webp\",\".tif\",\".tiff\",\".JPG\",\".JPEG\",\".PNG\")\n","            paths = [p for p in glob.glob(os.path.join(folder, \"*\")) if os.path.splitext(p)[1] in exts]\n","            for p in paths:\n","                self.samples.append((p, y, get_video_key(os.path.basename(p))))\n","        self.samples.sort(key=lambda x: (x[1], x[2], x[0]))\n","        self.transform = T.Compose([\n","            T.Resize((299, 299)),              # Xception friendly\n","            T.ToTensor(),\n","            T.Normalize(mean=[0.485,0.456,0.406],\n","                        std=[0.229,0.224,0.225])\n","        ])\n","\n","    def __len__(self): return len(self.samples)\n","    def __getitem__(self, idx):\n","        p, y, vkey = self.samples[idx]\n","        # robust read with PIL (handles more formats than cv2 in Colab sometimes)\n","        with Image.open(p) as im:\n","            if im.mode != \"RGB\":\n","                im = im.convert(\"RGB\")\n","            x = self.transform(im)\n","        return x, y, vkey\n","\n","# 3) Model: Xception (timm) + attempt to load your DeepfakeBench weights\n","import timm\n","\n","class XceptionWrapper(nn.Module):\n","    def __init__(self, num_classes=2):\n","        super().__init__()\n","        # timm xception variant; aligns well for evaluation\n","        self.net = timm.create_model(\"xception41\", pretrained=True, num_classes=num_classes)\n","\n","    def forward(self, x):\n","        return self.net(x)\n","\n","    def features(self, x):\n","        # not needed here, but kept to mirror DeepfakeBench API\n","        return self.net.forward_features(x)\n","\n","    def classifier(self, feats):\n","        return self.net.get_classifier()(feats)\n","\n","model = XceptionWrapper(num_classes=2)\n","\n","# try loading DeepfakeBench weights if possible\n","def try_load_deepfakebench_weights(model, path):\n","    if not os.path.isfile(path):\n","        print(f\"[WARN] Weight file not found at: {path} — using ImageNet-pretrained Xception.\")\n","        return False\n","    try:\n","        sd = torch.load(path, map_location=\"cpu\")\n","        # some checkpoints are wrapped; try to unwrap common cases\n","        if isinstance(sd, dict) and \"state_dict\" in sd:\n","            sd = sd[\"state_dict\"]\n","        # strip prefixes like 'module.' or 'model.'\n","        new_sd = {}\n","        for k, v in sd.items():\n","            nk = k\n","            for pref in (\"module.\", \"model.\", \"net.\", \"backbone.\"):\n","                if nk.startswith(pref): nk = nk[len(pref):]\n","            new_sd[nk] = v\n","        # try partial load with non-strict to maximize match\n","        missing, unexpected = model.load_state_dict(new_sd, strict=False)\n","        print(\"Loaded with strict=False.\")\n","        if missing:   print(\"  missing keys:\", len(missing))\n","        if unexpected:print(\"  unexpected keys:\", len(unexpected))\n","        # sanity: if nothing matched, treat as failure\n","        matched = (len(new_sd) > 0) and (len(missing) < len(new_sd))\n","        if not matched:\n","            print(\"[WARN] Checkpoint keys didn't match this Xception; keeping ImageNet-pretrained weights.\")\n","            return False\n","        print(\"[OK] DeepfakeBench weights loaded (best-effort).\")\n","        return True\n","    except Exception as e:\n","        print(f\"[WARN] Failed to load DeepfakeBench weights ({e}). Using ImageNet-pretrained weights.\")\n","        return False\n","\n","_ = try_load_deepfakebench_weights(model, WEIGHT_PATH)\n","model = model.to(device)\n","model.eval()\n","\n","# 4) Inference over frames\n","ds = FramesDataset(DATA_ROOT)\n","if len(ds) == 0:\n","    raise RuntimeError(f\"No images found under {DATA_ROOT}/{{real,fake}}\")\n","\n","loader = DataLoader(ds, batch_size=64, shuffle=False, num_workers=2, pin_memory=True)\n","\n","all_probs = []   # probability of \"fake\" (class=1)\n","all_labels= []\n","all_vkeys = []\n","\n","softmax = nn.Softmax(dim=1)\n","\n","with torch.no_grad():\n","    for xb, yb, vkeys in loader:\n","        xb = xb.to(device, non_blocking=True)\n","        logits = model(xb)\n","        probs = softmax(logits)[:, 1].detach().cpu().numpy()\n","        all_probs.extend(probs.tolist())\n","        all_labels.extend(yb.numpy().tolist())\n","        all_vkeys.extend(list(vkeys))\n","\n","all_probs  = np.asarray(all_probs, dtype=np.float32)\n","all_labels = np.asarray(all_labels, dtype=np.int64)\n","all_vkeys  = np.asarray(all_vkeys)\n","\n","# 5) Per-video aggregation (median), then metrics\n","def aggregate_by_video(vkeys, probs, labels, how=\"median\"):\n","    vids = {}\n","    for v, p, y in zip(vkeys, probs, labels):\n","        if v not in vids: vids[v] = {\"p\": [], \"y\": y}\n","        vids[v][\"p\"].append(float(p))\n","    agg_p, agg_y = [], []\n","    for v, d in vids.items():\n","        arr = np.array(d[\"p\"], dtype=np.float32)\n","        if how == \"median\":\n","            ap = float(np.median(arr))\n","        elif how == \"p90\":\n","            ap = float(np.percentile(arr, 90))\n","        elif how == \"top10\":\n","            ap = float(np.mean(np.sort(arr)[-10:])) if len(arr) >= 10 else float(np.mean(arr))\n","        else:\n","            ap = float(np.mean(arr))\n","        agg_p.append(ap)\n","        agg_y.append(int(d[\"y\"]))\n","    return np.array(agg_p, dtype=np.float32), np.array(agg_y, dtype=np.int64)\n","\n","def metrics_auc_eer_ap(y_true, y_score):\n","    # AUC\n","    auc = roc_auc_score(y_true, y_score)\n","    # AP (average precision)\n","    ap  = average_precision_score(y_true, y_score)\n","    # EER\n","    fpr, tpr, thr = roc_curve(y_true, y_score)\n","    fnr = 1 - tpr\n","    # find threshold where FPR ~= FNR\n","    idx = np.nanargmin(np.abs(fpr - fnr))\n","    eer = (fpr[idx] + fnr[idx]) / 2.0\n","    return float(auc), float(eer), float(ap)\n","\n","vid_probs, vid_labels = aggregate_by_video(all_vkeys, all_probs, all_labels, how=\"median\")\n","\n","# Auto \"orientation fix\": use (1 - p) if that improves AUC\n","auc_orig, eer_orig, ap_orig = metrics_auc_eer_ap(vid_labels, vid_probs)\n","auc_flip, eer_flip, ap_flip = metrics_auc_eer_ap(vid_labels, 1.0 - vid_probs)\n","\n","if auc_flip > auc_orig:\n","    use_probs = 1.0 - vid_probs\n","    auc, eer, ap = auc_flip, eer_flip, ap_flip\n","    flip_used = True\n","else:\n","    use_probs = vid_probs\n","    auc, eer, ap = auc_orig, eer_orig, ap_orig\n","    flip_used = False\n","\n","print(\"\\n=== Xception (video-level, median) ===\")\n","print(f\"AUC: {auc:.4f}\")\n","print(f\"EER: {eer:.4f}\")\n","print(f\"AP : {ap:.4f}\")\n","print(\"(used 1-p flip: %s)\" % (\"YES\" if flip_used else \"NO\"))\n","\n","# 6) (Optional) Save small CSV of per-video results\n","import pandas as pd\n","rows = []\n","for v in sorted(set(all_vkeys.tolist())):\n","    mask = (all_vkeys == v)\n","    y = int(all_labels[mask][0])\n","    p = float(np.median(all_probs[mask]))\n","    rows.append({\"video_name\": v, \"true_label\": y, \"median_prob_fake\": p})\n","res_df = pd.DataFrame(rows)\n","SAVE_CSV_TO = os.path.join(DRIVE_ROOT, \"Xception_eval_frames_cropped_faces_10src_median.csv\")\n","res_df.to_csv(SAVE_CSV_TO, index=False)\n","print(\"Per-video CSV:\", SAVE_CSV_TO)\n","# ================================================================================================\n"]},{"cell_type":"code","source":["# Count \"fakes predicted as real\" (FN) at EER and at 0.5 threshold\n","\n","import os, re, glob\n","import numpy as np\n","from PIL import Image\n","import torch, torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","import torchvision.transforms as T\n","from sklearn.metrics import roc_curve\n","\n","# --- paths (same as before) ---\n","DRIVE_ROOT = \"/content/drive/MyDrive\" if os.path.exists(\"/content/drive/MyDrive\") else \"/content/drive/My Drive\"\n","DATA_ROOT  = os.path.join(DRIVE_ROOT, \"frames_cropped_faces_10src\")\n","WEIGHT_PATH= os.path.join(DRIVE_ROOT, \"DeepfakeBench_weights\", \"xception_best.pth\")\n","\n","# --- dataset/loader (same aggregation logic) ---\n","FRAME_KEY_RE = re.compile(r\"^(.*?)(?:[_-]frames?[_-]?\\d+|[_-]frame[_-]?\\d+)$\", re.IGNORECASE)\n","def get_video_key(basename):\n","    base = os.path.splitext(basename)[0]\n","    m = FRAME_KEY_RE.match(base)\n","    return m.group(1) if m else base.split(\"_\")[0]\n","\n","class FramesDataset(Dataset):\n","    def __init__(self, root):\n","        exts = {\".jpg\",\".jpeg\",\".png\",\".bmp\",\".webp\",\".tif\",\".tiff\",\".JPG\",\".JPEG\",\".PNG\"}\n","        self.samples=[]\n","        for cls,y in ((\"real\",0),(\"fake\",1)):\n","            d = os.path.join(root, cls)\n","            for p in glob.glob(os.path.join(d, \"*\")):\n","                if os.path.splitext(p)[1] in exts:\n","                    self.samples.append((p, y, get_video_key(os.path.basename(p))))\n","        self.samples.sort(key=lambda x:(x[1], x[2], x[0]))\n","        self.tf = T.Compose([\n","            T.Resize((299,299)),\n","            T.ToTensor(),\n","            T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n","        ])\n","    def __len__(self): return len(self.samples)\n","    def __getitem__(self, i):\n","        p,y,v = self.samples[i]\n","        with Image.open(p) as im:\n","            if im.mode!=\"RGB\": im = im.convert(\"RGB\")\n","            x = self.tf(im)\n","        return x, y, v\n","\n","# --- model (quiet) ---\n","import timm, io, contextlib, warnings\n","warnings.filterwarnings(\"ignore\")\n","class XceptionWrapper(nn.Module):\n","    def __init__(self, num_classes=2):\n","        super().__init__()\n","        with contextlib.redirect_stdout(io.StringIO()), contextlib.redirect_stderr(io.StringIO()):\n","            self.net = timm.create_model(\"xception41\", pretrained=True, num_classes=num_classes)\n","    def forward(self,x): return self.net(x)\n","\n","def try_load_weights(model, path):\n","    if not os.path.isfile(path): return\n","    sd = torch.load(path, map_location=\"cpu\")\n","    if isinstance(sd, dict) and \"state_dict\" in sd: sd = sd[\"state_dict\"]\n","    new_sd={}\n","    for k,v in (sd.items() if isinstance(sd, dict) else []):\n","        nk=k\n","        for pref in (\"module.\",\"model.\",\"net.\",\"backbone.\"):\n","            if nk.startswith(pref): nk = nk[len(pref):]\n","        new_sd[nk]=v\n","    try: model.load_state_dict(new_sd, strict=False)\n","    except: pass\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = XceptionWrapper().to(device).eval()\n","try_load_weights(model, WEIGHT_PATH)\n","softmax = nn.Softmax(dim=1)\n","\n","# --- inference over frames ---\n","ds = FramesDataset(DATA_ROOT)\n","loader = DataLoader(ds, batch_size=64, shuffle=False, num_workers=2, pin_memory=True)\n","\n","all_p, all_y, all_v = [], [], []\n","with torch.no_grad():\n","    for xb, yb, vks in loader:\n","        xb = xb.to(device, non_blocking=True)\n","        prob_fake = softmax(model(xb))[:,1].detach().cpu().numpy()\n","        all_p.extend(prob_fake.tolist())\n","        all_y.extend(yb.numpy().tolist())\n","        all_v.extend(list(vks))\n","\n","all_p = np.asarray(all_p, dtype=np.float32)\n","all_y = np.asarray(all_y, dtype=np.int64)\n","all_v = np.asarray(all_v)\n","\n","# --- per-video (median) ---\n","vid_scores = {}\n","for v, p, y in zip(all_v, all_p, all_y):\n","    if v not in vid_scores: vid_scores[v] = {\"p\": [], \"y\": y}\n","    vid_scores[v][\"p\"].append(float(p))\n","vkeys = sorted(vid_scores.keys())\n","vid_p = np.array([np.median(vid_scores[v][\"p\"]) for v in vkeys], dtype=np.float32)\n","vid_y = np.array([vid_scores[v][\"y\"] for v in vkeys], dtype=np.int64)\n","\n","# --- counts at EER and at 0.5 ---\n","fpr, tpr, thr = roc_curve(vid_y, vid_p)\n","fnr = 1 - tpr\n","eer_idx = int(np.nanargmin(np.abs(fpr - fnr)))\n","eer_thr = float(thr[eer_idx])\n","\n","def counts_at(th):\n","    yhat = (vid_p >= th).astype(int)  # 1=fake\n","    tp = int(((vid_y==1) & (yhat==1)).sum())  # fake->fake\n","    tn = int(((vid_y==0) & (yhat==0)).sum())  # real->real\n","    fp = int(((vid_y==0) & (yhat==1)).sum())  # real->fake\n","    fn = int(((vid_y==1) & (yhat==0)).sum())  # fake->real  <<< what you asked\n","    return tp, tn, fp, fn\n","\n","tp_e, tn_e, fp_e, fn_e = counts_at(eer_thr)\n","tp_5, tn_5, fp_5, fn_5 = counts_at(0.5)\n","\n","print(f\"Fakes predicted as real @EER: {fn_e}\")\n","print(f\"Fakes predicted as real @0.5: {fn_5}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2kp2S-JjOX1o","executionInfo":{"status":"ok","timestamp":1760957170166,"user_tz":-120,"elapsed":29966,"user":{"displayName":"Vishnumaya","userId":"01919615312035119785"}},"outputId":"9b673415-c524-42bb-83d1-44771cfd9fbb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Fakes predicted as real @EER: 25\n","Fakes predicted as real @0.5: 4\n"]}]},{"cell_type":"code","source":["# Force-save the LARGE TABLE that was printed above (schema-matched)\n","\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=False)\n","\n","import os, time, pandas as pd\n","\n","SAVE_DIR = \"/content/drive/MyDrive/xception results 10 src\"\n","os.makedirs(SAVE_DIR, exist_ok=True)\n","\n","# Expected LARGE-table columns (must all be present)\n","REQUIRED = {\n","    \"dataset\",\"detector\",\"video_name\",\"true_label\",\n","    \"n_frames\",\"n_correct_frames\",\"n_wrong_frames\",\"frame_accuracy\",\n","    \"avg_prob_fake\",\"std_prob_fake\",\n","    \"video_pred_by_avg\",\"video_correct_by_avg\",\n","    \"video_pred_by_majority\",\"video_correct_by_majority\",\n","}\n","\n","# Find DataFrames in memory that match the LARGE-table schema\n","candidates = []\n","for name, val in globals().items():\n","    if isinstance(val, pd.DataFrame):\n","        cols = set(map(str, val.columns))\n","        if REQUIRED.issubset(cols):\n","            candidates.append((name, val))\n","\n","if not candidates:\n","    raise RuntimeError(\n","        \"Could not find a DataFrame with LARGE-table columns in memory. \"\n","        \"Make sure you ran the large-table cell just before this.\"\n","    )\n","\n","# Prefer a 100-row table; else pick the one with the most rows\n","best_name, best_df = max(\n","    candidates,\n","    key=lambda nv: (abs(len(nv[1]) - 100) < 1e-9, len(nv[1]))  # True>False, then row count\n",")\n","\n","ts = time.strftime(\"%Y%m%d-%H%M%S\")\n","csv_path = os.path.join(SAVE_DIR, f\"xception_large_table_10src_{ts}.csv\")\n","best_df.to_csv(csv_path, index=False)\n","\n","print(f\"Saved LARGE table from variable: {best_name}  (rows={len(best_df)})\")\n","print(\"→\", csv_path)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":228},"id":"Ws7WuNQoiaAN","executionInfo":{"status":"error","timestamp":1760962394244,"user_tz":-120,"elapsed":2565,"user":{"displayName":"Vishnumaya","userId":"01919615312035119785"}},"outputId":"9e13daac-6a1a-48db-ca99-a037166fbd4f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"dictionary changed size during iteration","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2594649978.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Find DataFrames in memory that match the LARGE-table schema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mcandidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: dictionary changed size during iteration"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=False)\n","\n","import os\n","SAVE_DIR = \"/content/drive/MyDrive/xception results 10 src\"\n","os.makedirs(SAVE_DIR, exist_ok=True)\n","\n","CSV_PATH = os.path.join(SAVE_DIR, \"xception_large_table_10src.csv\")\n","df.to_csv(CSV_PATH, index=False)\n","\n","print(\"Saved to:\", CSV_PATH)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"vtrTbJkOVRs0","executionInfo":{"status":"error","timestamp":1760960812849,"user_tz":-120,"elapsed":11207,"user":{"displayName":"Vishnumaya","userId":"01919615312035119785"}},"outputId":"08e40212-7c3d-4453-95ba-88c2ed17a781"},"execution_count":null,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"Mountpoint must not already contain files","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3727153936.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mSAVE_DIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/xception results 10 src\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    193\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mountpoint must not be a symlink'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mountpoint must not already contain files'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mountpoint must either be a directory or not exist'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Mountpoint must not already contain files"]}]},{"cell_type":"code","source":["# Build SMALL table from whatever columns your large table `df` has.\n","\n","import numpy as np, pandas as pd\n","from sklearn.metrics import roc_curve\n","\n","# 1) normalize column names (strip spaces, lowercase)\n","orig_cols = list(df.columns)\n","norm_map = {c: c.strip().lower() for c in df.columns}\n","df_norm = df.rename(columns=norm_map)\n","\n","def c(name):  # helper to fetch a column by normalized key\n","    key = name.lower()\n","    for orig, norm in norm_map.items():\n","        if norm == key:\n","            return orig\n","    return None\n","\n","col_true     = c(\"true_label\")\n","col_predavg  = c(\"video_pred_by_avg\")\n","col_correct  = c(\"video_correct_by_avg\")\n","col_avgprob  = c(\"avg_prob_fake\")\n","\n","if col_true is None:\n","    raise KeyError(\"true_label column not found in df. Available: \" + \", \".join(orig_cols))\n","\n","# Map labels to ints if they are strings\n","y_series = df[col_true]\n","if y_series.dtype == object:\n","    # accept \"real\"/\"fake\" (case-insensitive) or 0/1 as strings\n","    y = y_series.str.lower().map({\"real\":0, \"fake\":1})\n","    if y.isna().any():\n","        # try to coerce to int\n","        y = pd.to_numeric(y_series, errors=\"coerce\")\n","else:\n","    y = y_series\n","\n","if y.isna().any():\n","    raise ValueError(\"Could not interpret true_label as 0/1 or real/fake.\")\n","\n","# Strategy A: use existing correctness column\n","if col_correct is not None:\n","    small_df = df[[c(\"dataset\"), c(\"detector\"), c(\"video_name\"), col_true]].copy()\n","    small_df[\"correctly_predicted\"] = np.where(df[col_correct].astype(int)==1, \"yes\", \"no\")\n","\n","# Strategy B: use existing predictions to compute correctness\n","elif col_predavg is not None:\n","    # video_pred_by_avg may be strings (\"real\"/\"fake\") or ints (0/1)\n","    pred_col = df[col_predavg]\n","    if pred_col.dtype == object:\n","        pred = pred_col.str.lower().map({\"real\":0, \"fake\":1})\n","    else:\n","        pred = pred_col.astype(int)\n","    if pred.isna().any():\n","        raise ValueError(\"Could not interpret video_pred_by_avg as 0/1 or real/fake.\")\n","    correct = (pred.values == y.values).astype(int)\n","\n","    small_df = df[[c(\"dataset\"), c(\"detector\"), c(\"video_name\"), col_true]].copy()\n","    small_df[\"correctly_predicted\"] = np.where(correct==1, \"yes\", \"no\")\n","\n","# Strategy C (fallback): recompute Youden-J threshold from avg_prob_fake + true_label\n","else:\n","    if col_avgprob is None:\n","        raise KeyError(\n","            \"Neither video_correct_by_avg nor video_pred_by_avg nor avg_prob_fake present.\\n\"\n","            \"Available columns: \" + \", \".join(orig_cols)\n","        )\n","    scores = df[col_avgprob].astype(float).values\n","    yy     = y.values.astype(int)\n","    fpr, tpr, thr = roc_curve(yy, scores)\n","    j = tpr - fpr\n","    thr_youden = float(thr[np.nanargmax(j)])\n","    pred = (scores >= thr_youden).astype(int)\n","    correct = (pred == yy).astype(int)\n","\n","    small_df = df[[c(\"dataset\"), c(\"detector\"), c(\"video_name\"), col_true]].copy()\n","    small_df[\"correctly_predicted\"] = np.where(correct==1, \"yes\", \"no\")\n","\n","# Pretty print all rows\n","pd.set_option(\"display.max_rows\", 500)\n","pd.set_option(\"display.max_columns\", None)\n","pd.set_option(\"display.width\", 0)\n","print(small_df.to_string(index=False))\n","\n","# OPTIONAL: save to Drive\n","# save_dir = \"/content/drive/MyDrive/xception results 10 src\"\n","# os.makedirs(save_dir, exist_ok=True)\n","# small_df.to_csv(os.path.join(save_dir, \"xception_small_table_10src.csv\"), index=False)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":228},"id":"gWIhfoCnWxY2","executionInfo":{"status":"error","timestamp":1760959341874,"user_tz":-120,"elapsed":473,"user":{"displayName":"Vishnumaya","userId":"01919615312035119785"}},"outputId":"51cd5f2b-f580-4be0-b2a5-729de493e52d"},"execution_count":null,"outputs":[{"output_type":"error","ename":"KeyError","evalue":"'Neither video_correct_by_avg nor video_pred_by_avg nor avg_prob_fake present.\\nAvailable columns: dataset, detector, video_name, true_label, correctly_predicted'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1255225514.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcol_avgprob\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         raise KeyError(\n\u001b[0m\u001b[1;32m     64\u001b[0m             \u001b[0;34m\"Neither video_correct_by_avg nor video_pred_by_avg nor avg_prob_fake present.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;34m\"Available columns: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\", \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'Neither video_correct_by_avg nor video_pred_by_avg nor avg_prob_fake present.\\nAvailable columns: dataset, detector, video_name, true_label, correctly_predicted'"]}]},{"cell_type":"code","source":["# Save the CURRENT df (small table) to Drive\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=False)\n","\n","import os\n","SAVE_DIR = \"/content/drive/MyDrive/xception results 10 src\"\n","os.makedirs(SAVE_DIR, exist_ok=True)\n","\n","small_df = df[['dataset','detector','video_name','true_label','correctly_predicted']].copy()\n","out_path = os.path.join(SAVE_DIR, \"xception_small_table_10src.csv\")\n","small_df.to_csv(out_path, index=False)\n","print(\"Saved:\", out_path)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SeYU2OK3XK_U","executionInfo":{"status":"ok","timestamp":1760959447761,"user_tz":-120,"elapsed":1865,"user":{"displayName":"Vishnumaya","userId":"01919615312035119785"}},"outputId":"c13a820d-ad78-42d4-b5f5-1ee34342d9c5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Saved: /content/drive/MyDrive/xception results 10 src/xception_small_table_10src.csv\n"]}]},{"cell_type":"code","source":["# === Count FAKE FRAMES predicted as REAL (frame-level) @EER and @0.5 ===\n","import os, re, glob, io, contextlib, warnings, sys, subprocess\n","warnings.filterwarnings(\"ignore\")\n","\n","# Quiet install timm\n","subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"timm==0.9.12\"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n","\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=False)\n","\n","import numpy as np\n","import torch, torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","import torchvision.transforms as T\n","from PIL import Image\n","from sklearn.metrics import roc_curve, roc_auc_score\n","import timm\n","\n","# --- Paths ---\n","DRIVE_ROOT = \"/content/drive/MyDrive\" if os.path.exists(\"/content/drive/MyDrive\") else \"/content/drive/My Drive\"\n","DATA_ROOT  = os.path.join(DRIVE_ROOT, \"frames_cropped_faces_10src\")  # {real,fake}\n","WEIGHT_PATH= os.path.join(DRIVE_ROOT, \"DeepfakeBench_weights\", \"xception_best.pth\")\n","\n","# --- Dataset (frame-level) ---\n","class FramesDataset(Dataset):\n","    def __init__(self, root):\n","        exts = {\".jpg\",\".jpeg\",\".png\",\".bmp\",\".webp\",\".tif\",\".tiff\",\".JPG\",\".JPEG\",\".PNG\"}\n","        self.samples=[]\n","        for cls,y in ((\"real\",0),(\"fake\",1)):\n","            d = os.path.join(root, cls)\n","            if not os.path.isdir(d): continue\n","            for p in glob.glob(os.path.join(d, \"*\")):\n","                if os.path.splitext(p)[1] in exts:\n","                    self.samples.append((p, y))\n","        self.samples.sort(key=lambda x: (x[1], x[0]))\n","        self.tf = T.Compose([\n","            T.Resize((299,299)),\n","            T.ToTensor(),\n","            T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n","        ])\n","    def __len__(self): return len(self.samples)\n","    def __getitem__(self, i):\n","        p,y = self.samples[i]\n","        with Image.open(p) as im:\n","            if im.mode!=\"RGB\": im = im.convert(\"RGB\")\n","            x = self.tf(im)\n","        return x, y\n","\n","# --- Model ---\n","class XceptionWrapper(nn.Module):\n","    def __init__(self, num_classes=2):\n","        super().__init__()\n","        with contextlib.redirect_stdout(io.StringIO()), contextlib.redirect_stderr(io.StringIO()):\n","            self.net = timm.create_model(\"xception41\", pretrained=True, num_classes=num_classes)\n","    def forward(self,x): return self.net(x)\n","\n","def try_load_weights(model, path):\n","    if not os.path.isfile(path): return\n","    sd = torch.load(path, map_location=\"cpu\")\n","    if isinstance(sd, dict) and \"state_dict\" in sd: sd = sd[\"state_dict\"]\n","    new_sd={}\n","    for k,v in (sd.items() if isinstance(sd, dict) else []):\n","        nk=k\n","        for pref in (\"module.\",\"model.\",\"net.\",\"backbone.\"):\n","            if nk.startswith(pref): nk = nk[len(pref):]\n","        new_sd[nk]=v\n","    try: model.load_state_dict(new_sd, strict=False)\n","    except: pass\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = XceptionWrapper().to(device).eval()\n","try_load_weights(model, WEIGHT_PATH)\n","softmax = nn.Softmax(dim=1)\n","\n","# --- Inference (frame-level probs) ---\n","ds = FramesDataset(DATA_ROOT)\n","loader = DataLoader(ds, batch_size=64, shuffle=False, num_workers=2, pin_memory=True)\n","\n","frame_probs, frame_labels = [], []\n","with torch.no_grad():\n","    for xb, yb in loader:\n","        xb = xb.to(device, non_blocking=True)\n","        p = softmax(model(xb))[:,1].detach().cpu().numpy()\n","        frame_probs.extend(p.tolist())\n","        frame_labels.extend(yb.numpy().tolist())\n","\n","frame_probs  = np.asarray(frame_probs, dtype=np.float32)\n","frame_labels = np.asarray(frame_labels, dtype=np.int64)\n","\n","# --- Optional orientation fix at FRAME level (choose p or 1-p by AUC) ---\n","auc_p  = roc_auc_score(frame_labels, frame_probs)\n","auc_1p = roc_auc_score(frame_labels, 1.0 - frame_probs)\n","if auc_1p > auc_p:\n","    frame_probs = 1.0 - frame_probs\n","\n","# --- EER threshold (frame-level) ---\n","fpr, tpr, thr = roc_curve(frame_labels, frame_probs)\n","fnr = 1 - tpr\n","eer_idx = int(np.nanargmin(np.abs(fpr - fnr)))\n","thr_eer = float(thr[eer_idx])\n","\n","# --- Counts on FAKE frames only (label==1): predicted as REAL ---\n","y = frame_labels\n","p = frame_probs\n","fake_mask = (y == 1)\n","\n","# @EER\n","pred_eer = (p >= thr_eer).astype(int)  # 1=fake\n","fake_as_real_eer = int(((pred_eer == 0) & fake_mask).sum())\n","\n","# @0.5\n","pred_05 = (p >= 0.5).astype(int)\n","fake_as_real_05 = int(((pred_05 == 0) & fake_mask).sum())\n","\n","total_fake_frames = int(fake_mask.sum())\n","\n","print(f\"Fake frames predicted as REAL @EER: {fake_as_real_eer} / {total_fake_frames}\")\n","print(f\"Fake frames predicted as REAL @0.5: {fake_as_real_05} / {total_fake_frames}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ni_4UkAAYxNe","executionInfo":{"status":"ok","timestamp":1760959895647,"user_tz":-120,"elapsed":30698,"user":{"displayName":"Vishnumaya","userId":"01919615312035119785"}},"outputId":"61fdd7be-b215-460e-cc50-770e6218d40b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Fake frames predicted as REAL @EER: 485 / 1000\n","Fake frames predicted as REAL @0.5: 57 / 1000\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"c_uzY-0ock-K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ================= LARGE TABLE (labels as \"real\"/\"fake\") =================\n","# Columns:\n","# dataset, detector, video_name, true_label, n_frames, n_correct_frames, n_wrong_frames,\n","# frame_accuracy, avg_prob_fake, std_prob_fake, video_pred_by_avg, video_correct_by_avg,\n","# video_pred_by_majority, video_correct_by_majority\n","\n","import os, re, glob, io, contextlib, warnings, math\n","warnings.filterwarnings(\"ignore\")\n","\n","# Quiet install\n","import sys, subprocess\n","subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"timm==0.9.12\"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n","\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=False)\n","\n","import numpy as np\n","import pandas as pd\n","import torch, torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","import torchvision.transforms as T\n","from PIL import Image\n","from sklearn.metrics import roc_curve, roc_auc_score\n","\n","# ---- Paths ----\n","DRIVE_ROOT  = \"/content/drive/MyDrive\" if os.path.exists(\"/content/drive/MyDrive\") else \"/content/drive/My Drive\"\n","DATASET    = \"frames_cropped_faces_10src\"\n","DATA_ROOT  = os.path.join(DRIVE_ROOT, DATASET)              # {real,fake}\n","WEIGHT_PATH= os.path.join(DRIVE_ROOT, \"DeepfakeBench_weights\", \"xception_best.pth\")\n","DETECTOR   = \"Xception\"\n","\n","# ---- Dataset loader (per-frame), video key from filename prefix before _frames_XX ----\n","FRAME_KEY_RE = re.compile(r\"^(.*?)(?:[_-]frames?[_-]?\\d+|[_-]frame[_-]?\\d+)$\", re.IGNORECASE)\n","def get_video_key(basename):\n","    base = os.path.splitext(basename)[0]\n","    m = FRAME_KEY_RE.match(base)\n","    return m.group(1) if m else base.split(\"_\")[0]\n","\n","class FramesDS(Dataset):\n","    def __init__(self, root):\n","        exts = {\".jpg\",\".jpeg\",\".png\",\".bmp\",\".webp\",\".tif\",\".tiff\",\".JPG\",\".JPEG\",\".PNG\"}\n","        self.samples=[]\n","        for cls,y in ((\"real\",0),(\"fake\",1)):\n","            d = os.path.join(root, cls)\n","            for p in glob.glob(os.path.join(d, \"*\")):\n","                if os.path.splitext(p)[1] in exts:\n","                    self.samples.append((p, y, get_video_key(os.path.basename(p))))\n","        self.samples.sort(key=lambda x:(x[1], x[2], x[0]))\n","        self.tf = T.Compose([\n","            T.Resize((299,299)),\n","            T.ToTensor(),\n","            T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n","        ])\n","    def __len__(self): return len(self.samples)\n","    def __getitem__(self, i):\n","        p,y,v = self.samples[i]\n","        with Image.open(p) as im:\n","            if im.mode!=\"RGB\": im = im.convert(\"RGB\")\n","            x = self.tf(im)\n","        return x, y, v\n","\n","# ---- Model (timm Xception) + best-effort weight load ----\n","import timm\n","class XceptionWrapper(nn.Module):\n","    def __init__(self, num_classes=2):\n","        super().__init__()\n","        with contextlib.redirect_stdout(io.StringIO()), contextlib.redirect_stderr(io.StringIO()):\n","            self.net = timm.create_model(\"xception41\", pretrained=True, num_classes=num_classes)\n","    def forward(self,x): return self.net(x)\n","\n","def try_load_weights(model, path):\n","    if not os.path.isfile(path): return\n","    sd = torch.load(path, map_location=\"cpu\")\n","    if isinstance(sd, dict) and \"state_dict\" in sd: sd = sd[\"state_dict\"]\n","    new_sd={}\n","    for k,v in (sd.items() if isinstance(sd, dict) else []):\n","        nk=k\n","        for pref in (\"module.\",\"model.\",\"net.\",\"backbone.\"):\n","            if nk.startswith(pref): nk = nk[len(pref):]\n","        new_sd[nk]=v\n","    try: model.load_state_dict(new_sd, strict=False)\n","    except: pass\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = XceptionWrapper().to(device).eval()\n","try_load_weights(model, WEIGHT_PATH)\n","softmax = nn.Softmax(dim=1)\n","\n","# ---- Inference over frames ----\n","ds = FramesDS(DATA_ROOT)\n","loader = DataLoader(ds, batch_size=64, shuffle=False, num_workers=2, pin_memory=True)\n","\n","frame_probs, frame_labels, frame_vkeys = [], [], []\n","\n","with torch.no_grad():\n","    for xb, yb, vks in loader:\n","        xb = xb.to(device, non_blocking=True)\n","        prob_fake = softmax(model(xb))[:,1].detach().cpu().numpy()\n","        frame_probs.extend(prob_fake.tolist())\n","        frame_labels.extend(yb.numpy().tolist())\n","        frame_vkeys.extend(list(vks))\n","\n","frame_probs  = np.asarray(frame_probs, dtype=np.float32)\n","frame_labels = np.asarray(frame_labels, dtype=np.int64)\n","frame_vkeys  = np.asarray(frame_vkeys)\n","\n","# ---- Auto orientation flip: choose p or (1-p) maximizing VIDEO-level AUC ----\n","def agg_by_video(vkeys, probs, labels, fn=\"median\"):\n","    vids = {}\n","    for v,p,y in zip(vkeys, probs, labels):\n","        if v not in vids: vids[v]={\"p\":[], \"y\":y}\n","        vids[v][\"p\"].append(float(p))\n","    names = sorted(vids.keys())\n","    P = np.array([np.median(vids[n][\"p\"]) if fn==\"median\" else np.mean(vids[n][\"p\"]) for n in names], dtype=np.float32)\n","    Y = np.array([vids[n][\"y\"] for n in names], dtype=np.int64)\n","    return names, P, Y\n","\n","_, P_med, Y_vid = agg_by_video(frame_vkeys, frame_probs, frame_labels, \"median\")\n","auc_p  = roc_auc_score(Y_vid, P_med)\n","auc_1p = roc_auc_score(Y_vid, 1.0 - P_med)\n","flip = auc_1p > auc_p\n","if flip:\n","    frame_probs = 1.0 - frame_probs\n","    _, P_med, Y_vid = agg_by_video(frame_vkeys, frame_probs, frame_labels, \"median\")\n","\n","# ---- Thresholds (Youden-J):\n","def youden_threshold(y_true, y_score):\n","    fpr, tpr, thr = roc_curve(y_true, y_score)\n","    j = tpr - fpr\n","    return float(thr[np.nanargmax(j)])\n","\n","# Frame-level threshold on ALL frames\n","from sklearn.metrics import roc_curve\n","thr_frame = youden_threshold(frame_labels, frame_probs)\n","\n","# Video-average threshold on per-video average probs\n","names, P_avg, Y = agg_by_video(frame_vkeys, frame_probs, frame_labels, \"mean\")\n","thr_video_avg = youden_threshold(Y, P_avg)\n","\n","# ---- Build per-video rows (labels/preds as strings \"real\"/\"fake\") ----\n","def lab2str(y): return \"real\" if int(y)==0 else \"fake\"\n","\n","rows = []\n","video_dict = {}\n","for v,p,y in zip(frame_vkeys, frame_probs, frame_labels):\n","    if v not in video_dict: video_dict[v] = {\"probs\": [], \"label\": int(y)}\n","    video_dict[v][\"probs\"].append(float(p))\n","\n","for v in sorted(video_dict.keys()):\n","    probs = np.array(video_dict[v][\"probs\"], dtype=np.float32)\n","    y_int  = int(video_dict[v][\"label\"])      # 0/1\n","    y_str  = lab2str(y_int)                   # \"real\"/\"fake\"\n","    n_frames = probs.size\n","\n","    # frame-level predictions\n","    yhat_frames = (probs >= thr_frame).astype(int)\n","    n_correct_frames = int((yhat_frames == y_int).sum())\n","    n_wrong_frames   = int(n_frames - n_correct_frames)\n","    frame_accuracy   = n_correct_frames / float(n_frames) if n_frames > 0 else 0.0\n","\n","    avg_prob_fake = float(np.mean(probs))\n","    std_prob_fake = float(np.std(probs))\n","\n","    # video_pred_by_avg (string)\n","    pred_avg_int = int(avg_prob_fake >= thr_video_avg)\n","    pred_avg_str = lab2str(pred_avg_int)\n","    video_correct_by_avg = int(pred_avg_int == y_int)\n","\n","    # video_pred_by_majority (string)\n","    pred_maj_int = int((yhat_frames.sum() >= math.ceil(n_frames/2)))\n","    pred_maj_str = lab2str(pred_maj_int)\n","    video_correct_by_majority = int(pred_maj_int == y_int)\n","\n","    rows.append({\n","        \"dataset\": DATASET,\n","        \"detector\": DETECTOR,\n","        \"video_name\": v,\n","        \"true_label\": y_str,\n","        \"n_frames\": n_frames,\n","        \"n_correct_frames\": n_correct_frames,\n","        \"n_wrong_frames\": n_wrong_frames,\n","        \"frame_accuracy\": round(frame_accuracy, 4),\n","        \"avg_prob_fake\": round(avg_prob_fake, 6),\n","        \"std_prob_fake\": round(std_prob_fake, 6),\n","        \"video_pred_by_avg\": pred_avg_str,\n","        \"video_correct_by_avg\": video_correct_by_avg,\n","        \"video_pred_by_majority\": pred_maj_str,\n","        \"video_correct_by_majority\": video_correct_by_majority,\n","    })\n","\n","df = pd.DataFrame(rows)\n","\n","# Ensure full display (100 rows) with no column breaks\n","pd.set_option(\"display.max_rows\", 200)\n","pd.set_option(\"display.max_columns\", None)\n","pd.set_option(\"display.width\", 0)\n","\n","print(df.to_string(index=False))\n","# ======================================================================\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1760962440168,"user_tz":-120,"elapsed":30065,"user":{"displayName":"Vishnumaya","userId":"01919615312035119785"}},"outputId":"bef4c1da-d0cb-4cc2-877a-027c6f8aa7d7","id":"z8ZPTFSlclVa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","                   dataset detector video_name true_label  n_frames  n_correct_frames  n_wrong_frames  frame_accuracy  avg_prob_fake  std_prob_fake video_pred_by_avg  video_correct_by_avg video_pred_by_majority  video_correct_by_majority\n","frames_cropped_faces_10src Xception       10_1       fake        20                 1              19            0.05       0.412100       0.023638              fake                     1                   real                          0\n","frames_cropped_faces_10src Xception      10_10       fake        20                 6              14            0.30       0.443847       0.034106              fake                     1                   real                          0\n","frames_cropped_faces_10src Xception      10_11       fake        20                20               0            1.00       0.591035       0.043206              fake                     1                   fake                          1\n","frames_cropped_faces_10src Xception      10_12       fake        20                18               2            0.90       0.502442       0.040950              fake                     1                   fake                          1\n","frames_cropped_faces_10src Xception      10_13       fake        20                 0              20            0.00       0.398158       0.014095              real                     0                   real                          0\n","frames_cropped_faces_10src Xception      10_14       fake        20                 2              18            0.10       0.418206       0.020501              fake                     1                   real                          0\n","frames_cropped_faces_10src Xception      10_15       fake        20                20               0            1.00       0.547134       0.055068              fake                     1                   fake                          1\n","frames_cropped_faces_10src Xception      10_16       fake        20                 3              17            0.15       0.434540       0.014432              fake                     1                   real                          0\n","frames_cropped_faces_10src Xception      10_17       fake        20                 0              20            0.00       0.389863       0.022128              real                     0                   real                          0\n","frames_cropped_faces_10src Xception      10_18       fake        20                 3              17            0.15       0.420996       0.024792              fake                     1                   real                          0\n","frames_cropped_faces_10src Xception      10_19       fake        20                 8              12            0.40       0.452883       0.024722              fake                     1                   real                          0\n","frames_cropped_faces_10src Xception       10_2       fake        20                19               1            0.95       0.510008       0.062474              fake                     1                   fake                          1\n","frames_cropped_faces_10src Xception      10_20       fake        20                18               2            0.90       0.510640       0.035598              fake                     1                   fake                          1\n","frames_cropped_faces_10src Xception      10_21       fake        20                 3              17            0.15       0.415146       0.035903              fake                     1                   real                          0\n","frames_cropped_faces_10src Xception      10_22       fake        20                20               0            1.00       0.543835       0.035771              fake                     1                   fake                          1\n","frames_cropped_faces_10src Xception      10_23       fake        20                 0              20            0.00       0.402820       0.018562              real                     0                   real                          0\n","frames_cropped_faces_10src Xception      10_24       fake        20                20               0            1.00       0.664942       0.040109              fake                     1                   fake                          1\n","frames_cropped_faces_10src Xception      10_25       fake        20                16               4            0.80       0.474607       0.023315              fake                     1                   fake                          1\n","frames_cropped_faces_10src Xception      10_26       fake        20                18               2            0.90       0.474843       0.022901              fake                     1                   fake                          1\n","frames_cropped_faces_10src Xception      10_27       fake        20                 1              19            0.05       0.417232       0.022990              fake                     1                   real                          0\n","frames_cropped_faces_10src Xception      10_28       fake        20                16               4            0.80       0.556211       0.123718              fake                     1                   fake                          1\n","frames_cropped_faces_10src Xception      10_29       fake        20                 8              12            0.40       0.445237       0.026627              fake                     1                   real                          0\n","frames_cropped_faces_10src Xception       10_3       fake        20                20               0            1.00       0.535053       0.047842              fake                     1                   fake                          1\n","frames_cropped_faces_10src Xception      10_30       fake        20                20               0            1.00       0.543763       0.052082              fake                     1                   fake                          1\n","frames_cropped_faces_10src Xception      10_31       fake        20                10              10            0.50       0.451717       0.057492              fake                     1                   fake                          1\n","frames_cropped_faces_10src Xception      10_32       fake        20                10              10            0.50       0.432536       0.044411              fake                     1                   fake                          1\n","frames_cropped_faces_10src Xception      10_33       fake        20                 2              18            0.10       0.424374       0.025443              fake                     1                   real                          0\n","frames_cropped_faces_10src Xception      10_34       fake        20                 1              19            0.05       0.410937       0.024060              fake                     1                   real                          0\n","frames_cropped_faces_10src Xception      10_35       fake        20                14               6            0.70       0.463912       0.021718              fake                     1                   fake                          1\n","frames_cropped_faces_10src Xception      10_36       fake        20                 6              14            0.30       0.435962       0.022911              fake                     1                   real                          0\n","frames_cropped_faces_10src Xception      10_37       fake        20                14               6            0.70       0.477183       0.034216              fake                     1                   fake                          1\n","frames_cropped_faces_10src Xception      10_38       fake        20                20               0            1.00       0.516250       0.035042              fake                     1                   fake                          1\n","frames_cropped_faces_10src Xception      10_39       fake        20                18               2            0.90       0.502796       0.040859              fake                     1                   fake                          1\n","frames_cropped_faces_10src Xception       10_4       fake        20                 5              15            0.25       0.464931       0.130870              fake                     1                   real                          0\n","frames_cropped_faces_10src Xception      10_40       fake        20                15               5            0.75       0.465664       0.045577              fake                     1                   fake                          1\n","frames_cropped_faces_10src Xception      10_41       fake        20                17               3            0.85       0.508512       0.057078              fake                     1                   fake                          1\n","frames_cropped_faces_10src Xception      10_42       fake        20                 1              19            0.05       0.422995       0.018422              fake                     1                   real                          0\n","frames_cropped_faces_10src Xception      10_43       fake        20                 0              20            0.00       0.408068       0.021446              real                     0                   real                          0\n","frames_cropped_faces_10src Xception      10_44       fake        20                20               0            1.00       0.529815       0.027298              fake                     1                   fake                          1\n","frames_cropped_faces_10src Xception      10_45       fake        20                18               2            0.90       0.499520       0.050920              fake                     1                   fake                          1\n","frames_cropped_faces_10src Xception      10_46       fake        20                13               7            0.65       0.475540       0.040410              fake                     1                   fake                          1\n","frames_cropped_faces_10src Xception      10_47       fake        20                 6              14            0.30       0.430386       0.031079              fake                     1                   real                          0\n","frames_cropped_faces_10src Xception      10_48       fake        20                12               8            0.60       0.455604       0.022568              fake                     1                   fake                          1\n","frames_cropped_faces_10src Xception      10_49       fake        20                18               2            0.90       0.476446       0.024174              fake                     1                   fake                          1\n","frames_cropped_faces_10src Xception       10_5       fake        20                 6              14            0.30       0.434571       0.035989              fake                     1                   real                          0\n","frames_cropped_faces_10src Xception      10_50       fake        20                 0              20            0.00       0.410144       0.019116              real                     0                   real                          0\n","frames_cropped_faces_10src Xception       10_6       fake        20                 0              20            0.00       0.425272       0.017495              fake                     1                   real                          0\n","frames_cropped_faces_10src Xception       10_7       fake        20                 2              18            0.10       0.426221       0.028514              fake                     1                   real                          0\n","frames_cropped_faces_10src Xception       10_8       fake        20                15               5            0.75       0.471630       0.022123              fake                     1                   fake                          1\n","frames_cropped_faces_10src Xception       10_9       fake        20                19               1            0.95       0.509183       0.043787              fake                     1                   fake                          1\n","frames_cropped_faces_10src Xception        Ali       real        20                20               0            1.00       0.393091       0.025242              real                     1                   real                          1\n","frames_cropped_faces_10src Xception  Elizebeth       real        20                19               1            0.95       0.409780       0.017755              real                     1                   real                          1\n","frames_cropped_faces_10src Xception     Ganesh       real        20                 2              18            0.10       0.483155       0.027551              fake                     0                   fake                          0\n","frames_cropped_faces_10src Xception        aji       real        20                 0              20            0.00       0.570302       0.050699              fake                     0                   fake                          0\n","frames_cropped_faces_10src Xception      akbar       real        20                11               9            0.55       0.460810       0.037113              fake                     0                   real                          1\n","frames_cropped_faces_10src Xception      akhil       real        20                20               0            1.00       0.398394       0.011901              real                     1                   real                          1\n","frames_cropped_faces_10src Xception     akshay       real        20                19               1            0.95       0.422192       0.020444              fake                     0                   real                          1\n","frames_cropped_faces_10src Xception       alib       real        20                 0              20            0.00       0.587469       0.053645              fake                     0                   fake                          0\n","frames_cropped_faces_10src Xception      ameen       real        20                 5              15            0.25       0.522793       0.102100              fake                     0                   fake                          0\n","frames_cropped_faces_10src Xception       ammu       real        20                13               7            0.65       0.440687       0.028069              fake                     0                   real                          1\n","frames_cropped_faces_10src Xception     anandu       real        20                 6              14            0.30       0.474361       0.028873              fake                     0                   fake                          0\n","frames_cropped_faces_10src Xception      anish       real        20                18               2            0.90       0.431484       0.021492              fake                     0                   real                          1\n","frames_cropped_faces_10src Xception       ansu       real        20                 1              19            0.05       0.507011       0.027319              fake                     0                   fake                          0\n","frames_cropped_faces_10src Xception     arnold       real        20                17               3            0.85       0.423951       0.028610              fake                     0                   real                          1\n","frames_cropped_faces_10src Xception     assif_       real        20                 0              20            0.00       0.542325       0.030204              fake                     0                   fake                          0\n","frames_cropped_faces_10src Xception    baptist       real        20                 0              20            0.00       0.547930       0.049619              fake                     0                   fake                          0\n","frames_cropped_faces_10src Xception   binisha_       real        20                16               4            0.80       0.423804       0.027742              fake                     0                   real                          1\n","frames_cropped_faces_10src Xception    chettai       real        20                20               0            1.00       0.407099       0.014759              real                     1                   real                          1\n","frames_cropped_faces_10src Xception      chris       real        20                15               5            0.75       0.435617       0.024419              fake                     0                   real                          1\n","frames_cropped_faces_10src Xception  christian       real        20                17               3            0.85       0.409391       0.046635              real                     1                   real                          1\n","frames_cropped_faces_10src Xception        col       real        20                 8              12            0.40       0.450878       0.020019              fake                     0                   fake                          0\n","frames_cropped_faces_10src Xception    darwish       real        20                 0              20            0.00       0.524226       0.025710              fake                     0                   fake                          0\n","frames_cropped_faces_10src Xception      deeps       real        20                18               2            0.90       0.421776       0.025152              fake                     0                   real                          1\n","frames_cropped_faces_10src Xception      denna       real        20                16               4            0.80       0.435543       0.031362              fake                     0                   real                          1\n","frames_cropped_faces_10src Xception    fathima       real        20                 4              16            0.20       0.494111       0.043740              fake                     0                   fake                          0\n","frames_cropped_faces_10src Xception     jelvin       real        20                12               8            0.60       0.448816       0.059008              fake                     0                   real                          1\n","frames_cropped_faces_10src Xception   jennifer       real        20                17               3            0.85       0.437437       0.011677              fake                     0                   real                          1\n","frames_cropped_faces_10src Xception      jissa       real        20                12               8            0.60       0.452296       0.022849              fake                     0                   real                          1\n","frames_cropped_faces_10src Xception      kevin       real        20                16               4            0.80       0.426830       0.031447              fake                     0                   real                          1\n","frames_cropped_faces_10src Xception       lena       real        20                19               1            0.95       0.410892       0.019188              fake                     0                   real                          1\n","frames_cropped_faces_10src Xception       liya       real        20                20               0            1.00       0.406983       0.012454              real                     1                   real                          1\n","frames_cropped_faces_10src Xception    liyamom       real        20                15               5            0.75       0.431113       0.028381              fake                     0                   real                          1\n","frames_cropped_faces_10src Xception       malu       real        20                 0              20            0.00       0.651335       0.054268              fake                     0                   fake                          0\n","frames_cropped_faces_10src Xception      nevin       real        20                 2              18            0.10       0.478025       0.020756              fake                     0                   fake                          0\n","frames_cropped_faces_10src Xception   niranjan       real        20                16               4            0.80       0.429354       0.029968              fake                     0                   real                          1\n","frames_cropped_faces_10src Xception    praveen       real        20                20               0            1.00       0.408956       0.016297              real                     1                   real                          1\n","frames_cropped_faces_10src Xception    pushpan       real        20                11               9            0.55       0.446923       0.021883              fake                     0                   real                          1\n","frames_cropped_faces_10src Xception      rahul       real        20                 0              20            0.00       0.542279       0.051336              fake                     0                   fake                          0\n","frames_cropped_faces_10src Xception       raju       real        20                 3              17            0.15       0.494080       0.035705              fake                     0                   fake                          0\n","frames_cropped_faces_10src Xception      rasee       real        20                 4              16            0.20       0.473778       0.024735              fake                     0                   fake                          0\n","frames_cropped_faces_10src Xception     roshan       real        20                 5              15            0.25       0.508729       0.070521              fake                     0                   fake                          0\n","frames_cropped_faces_10src Xception     sachin       real        20                20               0            1.00       0.407339       0.017947              real                     1                   real                          1\n","frames_cropped_faces_10src Xception      salim       real        20                 2              18            0.10       0.519579       0.040117              fake                     0                   fake                          0\n","frames_cropped_faces_10src Xception     seethu       real        20                16               4            0.80       0.422351       0.060875              fake                     0                   real                          1\n","frames_cropped_faces_10src Xception     shanty       real        20                 7              13            0.35       0.479337       0.050454              fake                     0                   fake                          0\n","frames_cropped_faces_10src Xception       subu       real        20                18               2            0.90       0.405735       0.031903              real                     1                   real                          1\n","frames_cropped_faces_10src Xception      teggy       real        20                 9              11            0.45       0.458811       0.038920              fake                     0                   fake                          0\n","frames_cropped_faces_10src Xception     thomas       real        20                 7              13            0.35       0.501919       0.068748              fake                     0                   fake                          0\n","frames_cropped_faces_10src Xception      umesh       real        20                 0              20            0.00       0.492613       0.015878              fake                     0                   fake                          0\n","frames_cropped_faces_10src Xception        yad       real        20                 0              20            0.00       0.545070       0.040119              fake                     0                   fake                          0\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=False)\n","\n","import os, time\n","SAVE_DIR = \"/content/drive/MyDrive/xception results 10 src\"\n","os.makedirs(SAVE_DIR, exist_ok=True)\n","\n","CSV_PATH = os.path.join(SAVE_DIR, f\"xception_large_table_10src_{time.strftime('%Y%m%d-%H%M%S')}.csv\")\n","df.to_csv(CSV_PATH, index=False)\n","print(\"Saved:\", CSV_PATH)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p3SgT1BFirEb","executionInfo":{"status":"ok","timestamp":1760962515758,"user_tz":-120,"elapsed":2255,"user":{"displayName":"Vishnumaya","userId":"01919615312035119785"}},"outputId":"9738f091-0268-455e-92b7-8d40bba5618d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Saved: /content/drive/MyDrive/xception results 10 src/xception_large_table_10src_20251020-121515.csv\n"]}]}]}