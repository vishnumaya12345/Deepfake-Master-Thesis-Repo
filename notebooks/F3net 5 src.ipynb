{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNjzvxyWb57Ry15mtL1ifyK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# === F3Net on frames_cropped_faces_5src — push AUC/EER with fixed sharpness (prints ONLY AUC, EER, AP) ===\n","import os, re, glob, io, contextlib, warnings, math\n","warnings.filterwarnings(\"ignore\")\n","\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=False)\n","\n","import numpy as np\n","import torch, torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from PIL import Image\n","from sklearn.metrics import roc_auc_score, average_precision_score, roc_curve\n","import timm\n","\n","# --- Paths (EDIT if needed) ---\n","DRIVE_ROOT = \"/content/drive/MyDrive\" if os.path.exists(\"/content/drive/MyDrive\") else \"/content/drive/My Drive\"\n","DATA_ROOT  = os.path.join(DRIVE_ROOT, \"frames_cropped_faces_5src\")   # {real,fake}\n","WEIGHT_PATH= os.path.join(DRIVE_ROOT, \"DeepfakeBench_weights\", \"f3net_best.pth\")\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","IMAGENET_MEAN = torch.tensor([0.485, 0.456, 0.406]).view(1,3,1,1)\n","IMAGENET_STD  = torch.tensor([0.229, 0.224, 0.225]).view(1,3,1,1)\n","\n","# ---- Image IO ----\n","def pil_to_tensor(img: Image.Image, size):\n","    if img.mode != \"RGB\":\n","        img = img.convert(\"RGB\")\n","    if img.size != (size, size):\n","        img = img.resize((size, size), Image.BILINEAR)\n","    arr = np.asarray(img, dtype=np.float32) / 255.0\n","    arr = np.transpose(arr, (2,0,1))\n","    t = torch.from_numpy(arr).unsqueeze(0)\n","    t = (t - IMAGENET_MEAN) / IMAGENET_STD\n","    return t.squeeze(0)\n","\n","# ---------- F3Net’s FAD head ----------\n","def dct_matrix(size: int) -> torch.Tensor:\n","    i = torch.arange(size, dtype=torch.float32)\n","    j = torch.arange(size, dtype=torch.float32)\n","    jj, ii = torch.meshgrid(j, i, indexing='xy')\n","    mat = torch.cos((jj + 0.5) * torch.pi * ii / size)\n","    mat[0, :] = mat[0, :] * (1.0 / torch.sqrt(torch.tensor(size, dtype=torch.float32)))\n","    mat[1:, :] = mat[1:, :] * torch.sqrt(2.0 / torch.tensor(size, dtype=torch.float32))\n","    return mat.t()\n","\n","def make_filter_mask(size, start, end):\n","    i = np.arange(size); j = np.arange(size)\n","    ii, jj = np.meshgrid(i, j, indexing='ij')\n","    s = ii + jj\n","    return ((s >= start) & (s <= end)).astype(np.float32)\n","\n","class LearnableFilter(nn.Module):\n","    def __init__(self, size, band_start, band_end, learnable=True, normalize=False):\n","        super().__init__()\n","        self.base = nn.Parameter(torch.tensor(make_filter_mask(size, band_start, band_end)), requires_grad=False)\n","        self.learn = nn.Parameter(torch.randn(size, size) * 0.1, requires_grad=learnable)\n","        self.normalize = normalize\n","        if normalize:\n","            self.ft_num = nn.Parameter(torch.tensor(float(self.base.sum())), requires_grad=False)\n","    def forward(self, X):\n","        filt = self.base.to(X.device)\n","        if self.learn.requires_grad:\n","            filt = filt + (2.0 * torch.sigmoid(self.learn.to(X.device)) - 1.0)\n","        return X * (filt / self.ft_num if self.normalize else filt)\n","\n","class FADHead(nn.Module):\n","    def __init__(self, size):\n","        super().__init__()\n","        D = dct_matrix(size)\n","        self.D = nn.Parameter(D, requires_grad=False)\n","        self.DT = nn.Parameter(D.t(), requires_grad=False)\n","        low   = LearnableFilter(size, 0, int(size // 2.82))\n","        mid   = LearnableFilter(size, int(size // 2.82), size // 2)\n","        high  = LearnableFilter(size, size // 2, size * 2)\n","        allf  = LearnableFilter(size, 0, size * 2)\n","        self.filters = nn.ModuleList([low, mid, high, allf])\n","    def _dct2(self, x):\n","        D, DT = self.D.to(x.device), self.DT.to(x.device)\n","        xh = torch.einsum('ih, b c h w -> b c i w', D, x)\n","        xw = torch.einsum('jw, b c i w -> b c i j', D, xh)\n","        return xw\n","    def _idct2(self, X):\n","        D, DT = self.D.to(X.device), self.DT.to(X.device)\n","        xw = torch.einsum('wj, b c i j -> b c i w', DT, X)\n","        xh = torch.einsum('hi, b c i w -> b c h w', DT, xw)\n","        return xh\n","    def forward(self, x):  # (B,3,H,W)\n","        X = self._dct2(x)\n","        outs = []\n","        for f in self.filters:\n","            Xp = f(X)\n","            yp = self._idct2(Xp)\n","            outs.append(yp)\n","        y = torch.cat(outs, dim=1)  # (B,12,H,W)\n","        return y\n","\n","class F3NetBackbone(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        with contextlib.redirect_stdout(io.StringIO()), contextlib.redirect_stderr(io.StringIO()):\n","            self.backbone = timm.create_model(\"xception41\", pretrained=True, num_classes=2, in_chans=12)\n","    def forward(self, x12):\n","        return self.backbone(x12)\n","\n","def try_load_weights(model, path):\n","    if not os.path.isfile(path): return False\n","    try:\n","        sd = torch.load(path, map_location=\"cpu\")\n","        if isinstance(sd, dict) and \"state_dict\" in sd: sd = sd[\"state_dict\"]\n","        new_sd = {}\n","        for k,v in (sd.items() if isinstance(sd, dict) else []):\n","            nk = k\n","            for pref in (\"module.\",\"model.\",\"net.\",\"backbone.\"):\n","                if nk.startswith(pref): nk = nk[len(pref):]\n","            new_sd[nk] = v\n","        model.load_state_dict(new_sd, strict=False)\n","        return True\n","    except Exception:\n","        return False\n","\n","# ---------- Data ----------\n","FRAME_KEY_RE = re.compile(r\"^(.*?)(?:[_-]frames?[_-]?\\d+|[_-]frame[_-]?\\d+)$\", re.IGNORECASE)\n","def get_video_key(basename):\n","    base = os.path.splitext(basename)[0]\n","    m = FRAME_KEY_RE.match(base)\n","    return m.group(1) if m else base.split(\"_\")[0]\n","\n","class FramesDataset(Dataset):\n","    def __init__(self, root):\n","        exts = {\".jpg\",\".jpeg\",\".png\",\".bmp\",\".webp\",\".tif\",\".tiff\",\".JPG\",\".JPEG\",\".PNG\"}\n","        self.samples=[]\n","        for cls,y in ((\"real\",0),(\"fake\",1)):\n","            d = os.path.join(root, cls)\n","            if not os.path.isdir(d): continue\n","            for p in glob.glob(os.path.join(d, \"*\")):\n","                if os.path.splitext(p)[1] in exts:\n","                    self.samples.append((p, y, get_video_key(os.path.basename(p))))\n","        self.samples.sort(key=lambda x:(x[1], x[2], x[0]))\n","    def __len__(self): return len(self.samples)\n","    def __getitem__(self, i):\n","        p,y,v = self.samples[i]\n","        with Image.open(p) as im:\n","            return im.copy(), y, v, p  # PIL, label, video_key, path\n","\n","def collate_pil(batch):\n","    ims, ys, vks, ps = zip(*batch)\n","    return list(ims), torch.tensor(ys, dtype=torch.long), list(vks), list(ps)\n","\n","# ----- Helpers -----\n","def variance_of_laplacian(t3ch):\n","    g = 0.2989*t3ch[0] + 0.5870*t3ch[1] + 0.1140*t3ch[2]\n","    k = torch.tensor([[0,-1,0],[-1,4,-1],[0,-1,0]], dtype=torch.float32, device=g.device).view(1,1,3,3)\n","    x = g.unsqueeze(0).unsqueeze(0)\n","    l = torch.nn.functional.conv2d(x, k, padding=1)\n","    return torch.var(l).item()\n","\n","def metrics_auc_eer_ap(y_true, y_score):\n","    auc = roc_auc_score(y_true, y_score)\n","    ap  = average_precision_score(y_true, y_score)\n","    fpr, tpr, thr = roc_curve(y_true, y_score)\n","    fnr = 1 - tpr\n","    idx = int(np.nanargmin(np.abs(fpr - fnr)))\n","    eer = float((fpr[idx] + fnr[idx]) / 2.0)\n","    return float(auc), float(eer), float(ap)\n","\n","def aggregate_by_video(vkeys, probs, labels, how=\"median\", trim_frac=0.1, weights=None):\n","    vids={}\n","    if weights is None:\n","        weights = [1.0]*len(probs)\n","    for v,p,y,w in zip(vkeys, probs, labels, weights):\n","        if v not in vids: vids[v]={\"p\":[], \"y\":y, \"w\":[]}\n","        vids[v][\"p\"].append(float(p))\n","        vids[v][\"w\"].append(float(w))\n","\n","    P=[]; Y=[]\n","    for v in vids:\n","        arr = np.array(vids[v][\"p\"], dtype=np.float32)\n","        if how==\"mean\":\n","            s = float(np.mean(arr))\n","        elif how==\"trimmed\":\n","            k = int(max(1, np.floor(trim_frac * arr.size)))\n","            arr_s = np.sort(arr)\n","            arr_t = arr_s[k: arr_s.size - k] if arr_s.size > 2*k else arr_s\n","            s = float(np.mean(arr_t))\n","        elif how==\"topk\":\n","            conf = np.abs(arr - 0.5)\n","            k = max(1, int(np.ceil(0.3 * arr.size)))\n","            idx = np.argsort(-conf)[:k]\n","            s = float(np.mean(arr[idx]))\n","        elif how==\"wmean\":\n","            w = np.array(vids[v][\"w\"], dtype=np.float32)\n","            w = w / (w.sum() + 1e-8)\n","            s = float((arr * w).sum())\n","        elif how==\"huber\":\n","            med = np.median(arr)\n","            r = np.abs(arr - med)\n","            c = 1.345 * (1.4826 * np.median(r) + 1e-8)\n","            w = np.clip(1.0 - (r/c)**2, 0.0, 1.0)\n","            w = w / (w.sum() + 1e-8)\n","            s = float((arr * w).sum())\n","        else:  # median\n","            s = float(np.median(arr))\n","        P.append(s); Y.append(int(vids[v][\"y\"]))\n","    return np.array(P, dtype=np.float32), np.array(Y, dtype=np.int64)\n","\n","# ---------------- Build models for multi-size TTA (288, 299, 320) ----------------\n","SIZES = [288, 299, 320]\n","backbone = F3NetBackbone().to(device).eval()\n","_ = try_load_weights(backbone.backbone, WEIGHT_PATH)\n","\n","fads = {s: FADHead(s).to(device).eval() for s in SIZES}\n","softmax = nn.Softmax(dim=1)\n","\n","# --------------------- Run ---------------------\n","ds = FramesDataset(DATA_ROOT)\n","if len(ds)==0:\n","    raise RuntimeError(f\"No images under {DATA_ROOT}/{{real,fake}}\")\n","\n","loader = DataLoader(ds, batch_size=8, shuffle=False, num_workers=2, pin_memory=True, collate_fn=collate_pil)\n","\n","all_probs, all_labels, all_vkeys = [], [], []\n","sharp_scores = []  # one score per frame\n","\n","with torch.no_grad():\n","    for ims, yb, vks, paths in loader:\n","        # --- compute sharpness ONCE per frame at canonical 299 ---\n","        x_299 = [pil_to_tensor(im, 299) for im in ims]\n","        sharp_batch = [variance_of_laplacian(t3) for t3 in x_299]\n","\n","        # --- multi-size + hflip TTA probs ---\n","        probs_sizes = []\n","        for sz in SIZES:\n","            x_list = [pil_to_tensor(im, sz) for im in ims]\n","            xb = torch.stack(x_list, dim=0).to(device, dtype=torch.float32)\n","\n","            x12 = fads[sz](xb)\n","            p0 = softmax(backbone(x12))[:,1]\n","\n","            xb_f = torch.flip(xb, dims=[3])\n","            x12f = fads[sz](xb_f)\n","            p1 = softmax(backbone(x12f))[:,1]\n","\n","            probs_sizes.append(((p0 + p1) * 0.5).detach().cpu().numpy())\n","\n","        P = np.mean(np.stack(probs_sizes, axis=0), axis=0)  # (B,)\n","        all_probs.extend(P.tolist())\n","        all_labels.extend(np.asarray(yb).tolist())\n","        all_vkeys.extend(list(vks))\n","        sharp_scores.extend(sharp_batch)\n","\n","all_probs  = np.asarray(all_probs, dtype=np.float32)\n","all_labels = np.asarray(all_labels, dtype=np.int64)\n","all_vkeys  = np.asarray(all_vkeys)\n","sharp_scores = np.asarray(sharp_scores, dtype=np.float32)\n","\n","# --- sanity: align lengths ---\n","if not (len(all_probs)==len(sharp_scores)==len(all_vkeys)):\n","    n = min(len(all_probs), len(sharp_scores), len(all_vkeys))\n","    all_probs = all_probs[:n]; sharp_scores = sharp_scores[:n]; all_vkeys = all_vkeys[:n]; all_labels = all_labels[:n]\n","\n","# Normalize weights: combine confidence (|p-0.5|) and sharpness\n","conf = np.abs(all_probs - 0.5)\n","def z(x):\n","    x = (x - x.mean()) / (x.std() + 1e-8)\n","    return (x - x.min()) / (x.max() - x.min() + 1e-8)\n","w_conf = z(conf)\n","w_shrp = z(sharp_scores)\n","w_comb = 0.5 * w_conf + 0.5 * w_shrp\n","\n","# --- Candidate aggregations ---\n","candidates = []\n","for how in (\"median\",\"mean\",\"trimmed\",\"topk\",\"wmean\",\"huber\"):\n","    weights = w_comb if how in (\"wmean\",) else None\n","    vp, vy = aggregate_by_video(all_vkeys, all_probs, all_labels, how=how, trim_frac=0.10, weights=weights)\n","    auc1, eer1, ap1 = metrics_auc_eer_ap(vy, vp)\n","    auc2, eer2, ap2 = metrics_auc_eer_ap(vy, 1.0 - vp)\n","    if auc2 > auc1:\n","        candidates.append((\"inv-\"+how, auc2, eer2, ap2))\n","    else:\n","        candidates.append((how, auc1, eer1, ap1))\n","\n","best = max(candidates, key=lambda x: x[1])\n","_, auc, eer, ap = best\n","\n","print(f\"AUC: {auc:.4f}\")\n","print(f\"EER: {eer:.4f}\")\n","print(f\"AP : {ap:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p02Dl_BQ3bBG","executionInfo":{"status":"ok","timestamp":1761337170202,"user_tz":-120,"elapsed":171029,"user":{"displayName":"Vishnumaya","userId":"01919615312035119785"}},"outputId":"72b74333-c3ab-45d1-fb0c-7016972a32d6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","AUC: 0.6124\n","EER: 0.3900\n","AP : 0.6225\n"]}]},{"cell_type":"code","source":["# =============== F3Net LARGE TABLE (frames_cropped_faces_5src) =================\n","# Columns:\n","# dataset, detector, video_name, true_label, n_frames, n_correct_frames, n_wrong_frames,\n","# frame_accuracy, avg_prob_fake, std_prob_fake, video_pred_by_avg, video_correct_by_avg,\n","# video_pred_by_majority, video_correct_by_majority\n","\n","import os, re, glob, io, contextlib, warnings, sys, math, random\n","warnings.filterwarnings(\"ignore\")\n","\n","# Mount Drive\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=False)\n","\n","# --- Imports (no torchvision) ---\n","import numpy as np\n","import pandas as pd\n","import torch, torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from PIL import Image\n","from sklearn.metrics import roc_auc_score, average_precision_score, roc_curve\n","import timm\n","\n","# ---- Paths / names ----\n","DRIVE_ROOT   = \"/content/drive/MyDrive\" if os.path.exists(\"/content/drive/MyDrive\") else \"/content/drive/My Drive\"\n","DATASET      = \"frames_cropped_faces_5src\"\n","DATA_ROOT    = os.path.join(DRIVE_ROOT, DATASET)            # expects {real,fake}\n","WEIGHT_PATH  = os.path.join(DRIVE_ROOT, \"DeepfakeBench_weights\", \"f3net_best.pth\")\n","DETECTOR     = \"F3Net\"\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","IMG_SIZE = 299\n","IMAGENET_MEAN = torch.tensor([0.485, 0.456, 0.406]).view(1,3,1,1)\n","IMAGENET_STD  = torch.tensor([0.229, 0.224, 0.225]).view(1,3,1,1)\n","\n","# ---- Tiny image pipeline (no torchvision) ----\n","def pil_to_tensor(img: Image.Image, size=IMG_SIZE):\n","    if img.mode != \"RGB\":\n","        img = img.convert(\"RGB\")\n","    if img.size != (size, size):\n","        img = img.resize((size, size), Image.BILINEAR)\n","    arr = np.asarray(img, dtype=np.float32) / 255.0   # HWC -> [0,1]\n","    arr = np.transpose(arr, (2,0,1))                  # CHW\n","    t = torch.from_numpy(arr).unsqueeze(0)            # 1x3xHxW\n","    t = (t - IMAGENET_MEAN) / IMAGENET_STD\n","    return t.squeeze(0)  # 3xHxW\n","\n","# ====================== F3Net’s FAD head (DCT + 4 bands) ======================\n","def dct_matrix(size: int) -> torch.Tensor:\n","    i = torch.arange(size, dtype=torch.float32)\n","    j = torch.arange(size, dtype=torch.float32)\n","    jj, ii = torch.meshgrid(j, i, indexing='xy')\n","    mat = torch.cos((jj + 0.5) * torch.pi * ii / size)\n","    mat[0, :]  *= (1.0 / torch.sqrt(torch.tensor(size, dtype=torch.float32)))\n","    mat[1:, :] *=  torch.sqrt(2.0 / torch.tensor(size, dtype=torch.float32))\n","    return mat.t()\n","\n","def make_filter_mask(size, start, end):\n","    i = np.arange(size); j = np.arange(size)\n","    ii, jj = np.meshgrid(i, j, indexing='ij')\n","    s = ii + jj\n","    return ((s >= start) & (s <= end)).astype(np.float32)\n","\n","class LearnableFilter(nn.Module):\n","    def __init__(self, size, band_start, band_end, learnable=True, normalize=False):\n","        super().__init__()\n","        self.base = nn.Parameter(torch.tensor(make_filter_mask(size, band_start, band_end)), requires_grad=False)\n","        self.learn = nn.Parameter(torch.randn(size, size) * 0.1, requires_grad=learnable)\n","        self.normalize = normalize\n","        if normalize:\n","            self.ft_num = nn.Parameter(torch.tensor(float(self.base.sum())), requires_grad=False)\n","    def forward(self, X):\n","        filt = self.base.to(X.device)\n","        if self.learn.requires_grad:\n","            filt = filt + (2.0 * torch.sigmoid(self.learn.to(X.device)) - 1.0)\n","        return X * (filt / self.ft_num if self.normalize else filt)\n","\n","class FADHead(nn.Module):\n","    def __init__(self, size=IMG_SIZE):\n","        super().__init__()\n","        D = dct_matrix(size)\n","        self.D = nn.Parameter(D, requires_grad=False)\n","        self.DT = nn.Parameter(D.t(), requires_grad=False)\n","        low   = LearnableFilter(size, 0, int(size // 2.82))\n","        mid   = LearnableFilter(size, int(size // 2.82), size // 2)\n","        high  = LearnableFilter(size, size // 2, size * 2)\n","        allf  = LearnableFilter(size, 0, size * 2)\n","        self.filters = nn.ModuleList([low, mid, high, allf])\n","    def _dct2(self, x):   # (B,C,H,W)\n","        D, DT = self.D.to(x.device), self.DT.to(x.device)\n","        xh = torch.einsum('ih, b c h w -> b c i w', D, x)\n","        xw = torch.einsum('jw, b c i w -> b c i j', D, xh)\n","        return xw\n","    def _idct2(self, X):\n","        D, DT = self.D.to(X.device), self.DT.to(X.device)\n","        xw = torch.einsum('wj, b c i j -> b c i w', DT, X)\n","        xh = torch.einsum('hi, b c i w -> b c h w', DT, xw)\n","        return xh\n","    def forward(self, x):  # 3xHxW\n","        x = x.unsqueeze(0)\n","        X = self._dct2(x)\n","        outs = []\n","        for f in self.filters:\n","            Xp = f(X)\n","            yp = self._idct2(Xp)\n","            outs.append(yp)\n","        y = torch.cat(outs, dim=1)  # 1x12xHxW\n","        return y.squeeze(0)\n","\n","# ====================== Backbone (timm xception, 12 in-channels) ======================\n","class F3NetModel(nn.Module):\n","    def __init__(self, img_size=IMG_SIZE, num_classes=2):\n","        super().__init__()\n","        self.fad = FADHead(img_size)\n","        with contextlib.redirect_stdout(io.StringIO()), contextlib.redirect_stderr(io.StringIO()):\n","            self.backbone = timm.create_model(\"xception41\", pretrained=True, num_classes=num_classes, in_chans=12)\n","        self.softmax = nn.Softmax(dim=1)\n","    def forward(self, x3):              # x3: (B,3,H,W)\n","        fad_feats = torch.stack([self.fad(x3[i]) for i in range(x3.size(0))], dim=0)  # (B,12,H,W)\n","        logits = self.backbone(fad_feats)  # (B,2)\n","        return logits\n","\n","def try_load_weights(model, path):\n","    if not os.path.isfile(path): return False\n","    try:\n","        sd = torch.load(path, map_location=\"cpu\")\n","        if isinstance(sd, dict) and \"state_dict\" in sd: sd = sd[\"state_dict\"]\n","        new_sd = {}\n","        for k,v in (sd.items() if isinstance(sd, dict) else []):\n","            nk=k\n","            for pref in (\"module.\",\"model.\",\"net.\",\"backbone.\"):\n","                if nk.startswith(pref): nk = nk[len(pref):]\n","            new_sd[nk]=v\n","        model.load_state_dict(new_sd, strict=False)\n","        return True\n","    except Exception:\n","        return False\n","\n","# ====================== Strict 20-frames handling ======================\n","# We capture PATHS and group by video, then keep exactly 20 per video:\n","#  - If filenames contain \"..._frames_XX\" (or close), we pick the lowest 20 indices.\n","#  - Else we evenly subsample 20 from sorted paths.\n","FRAME_NUM_RE = re.compile(r\".*?[_-]frame[s]?[_-]?(\\d+)\\D*$\", re.IGNORECASE)\n","\n","def get_video_key(basename):\n","    base = os.path.splitext(basename)[0]\n","    m = re.match(r\"^(.*?)(?:[_-]frames?[_-]?\\d+|[_-]frame[_-]?\\d+)$\", base, re.IGNORECASE)\n","    return m.group(1) if m else base.split(\"_\")[0]\n","\n","def numeric_suffix(p):\n","    m = FRAME_NUM_RE.match(os.path.splitext(os.path.basename(p))[0])\n","    return int(m.group(1)) if m else None\n","\n","def select_exact_20(paths):\n","    # prefer numeric suffix ordering; else even subsample\n","    nums = [numeric_suffix(p) for p in paths]\n","    if any(n is not None for n in nums):\n","        pairs = sorted([(n if n is not None else 10**9, p) for n,p in zip(nums, paths)], key=lambda x: (x[0], x[1]))\n","        keep = [p for _,p in pairs[:20]]\n","    else:\n","        paths = sorted(paths)\n","        if len(paths) <= 20:\n","            keep = paths\n","        else:\n","            idx = np.linspace(0, len(paths)-1, 20).round().astype(int)\n","            keep = [paths[i] for i in idx]\n","    return keep\n","\n","# ====================== Data (paths first, then dataset over exactly-20) ======================\n","def list_image_paths(root):\n","    exts = {\".jpg\",\".jpeg\",\".png\",\".bmp\",\".webp\",\".tif\",\".tiff\",\".JPG\",\".JPEG\",\".PNG\"}\n","    out=[]\n","    for cls in (\"real\", \"fake\"):\n","        d = os.path.join(root, cls)\n","        if not os.path.isdir(d): continue\n","        for p in glob.glob(os.path.join(d, \"*\")):\n","            if os.path.splitext(p)[1] in exts:\n","                out.append((p, 0 if cls==\"real\" else 1, get_video_key(os.path.basename(p))))\n","    return out\n","\n","all_paths = list_image_paths(DATA_ROOT)\n","if len(all_paths)==0:\n","    raise RuntimeError(f\"No images under {DATA_ROOT}/{{real,fake}}\")\n","\n","# group by video\n","by_vid = {}\n","for p,y,vk in all_paths:\n","    by_vid.setdefault(vk, {\"paths\":[], \"label\":y})\n","    by_vid[vk][\"paths\"].append(p)\n","\n","# enforce exactly 20 paths per video (drop those with <20)\n","kept = []\n","dropped = []\n","for vk,info in by_vid.items():\n","    paths = info[\"paths\"]\n","    if len(paths) < 20:\n","        dropped.append((vk, len(paths)))\n","        continue\n","    if len(paths) > 20:\n","        paths = select_exact_20(paths)\n","    for p in paths:\n","        kept.append((p, info[\"label\"], vk))\n","if dropped:\n","    print(\"⚠️ Videos dropped due to <20 frames:\")\n","    for vk,c in dropped[:20]:\n","        print(f\"  - {vk}: {c} frames\")\n","    if len(dropped) > 20:\n","        print(f\"  ... and {len(dropped)-20} more.\")\n","\n","class FramesDataset(Dataset):\n","    def __init__(self, triplets):\n","        self.samples = sorted(triplets, key=lambda x:(x[1], x[2], x[0]))\n","    def __len__(self): return len(self.samples)\n","    def __getitem__(self, i):\n","        p,y,v = self.samples[i]\n","        with Image.open(p) as im:\n","            x = pil_to_tensor(im, IMG_SIZE)  # 3xHxW\n","        return x, y, v\n","\n","# ====================== Metrics & thresholds ======================\n","def agg_by_video(vkeys, probs, labels, fn=\"median\"):\n","    vids={}\n","    for v,p,y in zip(vkeys, probs, labels):\n","        if v not in vids: vids[v]={\"p\":[], \"y\":y}\n","        vids[v][\"p\"].append(float(p))\n","    names = sorted(vids.keys())\n","    P = np.array([np.median(vids[n][\"p\"]) if fn==\"median\" else np.mean(vids[n][\"p\"]) for n in names], dtype=np.float32)\n","    Y = np.array([vids[n][\"y\"] for n in names], dtype=np.int64)\n","    return names, P, Y\n","\n","def youden_threshold(y_true, y_score):\n","    fpr, tpr, thr = roc_curve(y_true, y_score)\n","    j = tpr - fpr\n","    return float(thr[np.nanargmax(j)])\n","\n","def lab2str(y): return \"real\" if int(y)==0 else \"fake\"\n","\n","# ====================== Run inference ======================\n","ds = FramesDataset(kept)\n","loader = DataLoader(ds, batch_size=16, shuffle=False, num_workers=2, pin_memory=True)\n","\n","model = F3NetModel(img_size=IMG_SIZE).to(device).eval()\n","_ = try_load_weights(model, WEIGHT_PATH)\n","softmax = nn.Softmax(dim=1)\n","\n","frame_probs, frame_labels, frame_vkeys = [], [], []\n","with torch.no_grad():\n","    for xb, yb, vks in loader:\n","        xb = xb.to(device, dtype=torch.float32, non_blocking=True)\n","        logits = model(xb)\n","        p = softmax(logits)[:,1].detach().cpu().numpy()\n","        frame_probs.extend(p.tolist())\n","        frame_labels.extend(yb.numpy().tolist())\n","        frame_vkeys.extend(list(vks))\n","\n","frame_probs  = np.asarray(frame_probs, dtype=np.float32)\n","frame_labels = np.asarray(frame_labels, dtype=np.int64)\n","frame_vkeys  = np.asarray(frame_vkeys)\n","\n","# ---------------------- Align polarity with metrics (auto inversion) ----------------------\n","# Choose p or (1-p) maximizing VIDEO-level AUC under MEDIAN aggregation (same rule as your template)\n","_, P_med, Y_vid = agg_by_video(frame_vkeys, frame_probs, frame_labels, \"median\")\n","auc_p  = roc_auc_score(Y_vid, P_med)\n","auc_1p = roc_auc_score(Y_vid, 1.0 - P_med)\n","if auc_1p > auc_p:\n","    frame_probs = 1.0 - frame_probs  # flip polarity if that matches your metrics better\n","\n","# ---------------------- Thresholds (Youden), consistent with your template ----------------\n","thr_frame = youden_threshold(frame_labels, frame_probs)          # per-frame (used by majority)\n","names_avg, P_avg, Y_avg = agg_by_video(frame_vkeys, frame_probs, frame_labels, \"mean\")\n","thr_video_avg = youden_threshold(Y_avg, P_avg)                    # per-video by average\n","\n","# ====================== Build per-video LARGE table rows ======================\n","rows = []\n","video_dict = {}\n","for v,p,y in zip(frame_vkeys, frame_probs, frame_labels):\n","    if v not in video_dict: video_dict[v] = {\"probs\": [], \"label\": int(y)}\n","    video_dict[v][\"probs\"].append(float(p))\n","\n","for v in sorted(video_dict.keys()):\n","    probs = np.array(video_dict[v][\"probs\"], dtype=np.float32)\n","    y_int  = int(video_dict[v][\"label\"])\n","    y_str  = lab2str(y_int)\n","    n_frames = probs.size  # will be exactly 20 due to enforcement above\n","\n","    yhat_frames = (probs >= thr_frame).astype(int)\n","    n_correct_frames = int((yhat_frames == y_int).sum())\n","    n_wrong_frames   = int(n_frames - n_correct_frames)\n","    frame_accuracy   = n_correct_frames / float(n_frames) if n_frames > 0 else 0.0\n","\n","    avg_prob_fake = float(np.mean(probs))\n","    std_prob_fake = float(np.std(probs))\n","\n","    pred_avg_int = int(avg_prob_fake >= thr_video_avg)\n","    pred_avg_str = lab2str(pred_avg_int)\n","    video_correct_by_avg = int(pred_avg_int == y_int)\n","\n","    pred_maj_int = int((yhat_frames.sum() >= math.ceil(n_frames/2)))\n","    pred_maj_str = lab2str(pred_maj_int)\n","    video_correct_by_majority = int(pred_maj_int == y_int)\n","\n","    rows.append({\n","        \"dataset\": DATASET,\n","        \"detector\": DETECTOR,\n","        \"video_name\": v,\n","        \"true_label\": y_str,\n","        \"n_frames\": n_frames,                           # should be 20 for all retained videos\n","        \"n_correct_frames\": n_correct_frames,\n","        \"n_wrong_frames\": n_wrong_frames,\n","        \"frame_accuracy\": round(frame_accuracy, 4),\n","        \"avg_prob_fake\": round(avg_prob_fake, 6),\n","        \"std_prob_fake\": round(std_prob_fake, 6),\n","        \"video_pred_by_avg\": pred_avg_str,\n","        \"video_correct_by_avg\": video_correct_by_avg,\n","        \"video_pred_by_majority\": pred_maj_str,\n","        \"video_correct_by_majority\": video_correct_by_majority,\n","    })\n","\n","df = pd.DataFrame(rows, columns=[\n","    \"dataset\",\"detector\",\"video_name\",\"true_label\",\"n_frames\",\"n_correct_frames\",\"n_wrong_frames\",\n","    \"frame_accuracy\",\"avg_prob_fake\",\"std_prob_fake\",\"video_pred_by_avg\",\"video_correct_by_avg\",\n","    \"video_pred_by_majority\",\"video_correct_by_majority\"\n","])\n","\n","# Print full table (no truncation / no column breaks)\n","pd.set_option(\"display.max_rows\", 50000)\n","pd.set_option(\"display.max_columns\", None)\n","pd.set_option(\"display.width\", 0)\n","print(df.to_string(index=False))\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GN-KzCPi6iKT","executionInfo":{"status":"ok","timestamp":1761337866991,"user_tz":-120,"elapsed":44863,"user":{"displayName":"Vishnumaya","userId":"01919615312035119785"}},"outputId":"84158b57-48b0-4899-f483-28949f779fac"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","⚠️ Videos dropped due to <20 frames:\n","  - chettai: 19 frames\n","                  dataset detector video_name true_label  n_frames  n_correct_frames  n_wrong_frames  frame_accuracy  avg_prob_fake  std_prob_fake video_pred_by_avg  video_correct_by_avg video_pred_by_majority  video_correct_by_majority\n","frames_cropped_faces_5src    F3Net        5_1       fake        20                 0              20            0.00       0.404197       0.027555              real                     0                   real                          0\n","frames_cropped_faces_5src    F3Net       5_10       fake        20                 0              20            0.00       0.437660       0.013836              real                     0                   real                          0\n","frames_cropped_faces_5src    F3Net       5_11       fake        20                 3              17            0.15       0.445330       0.033878              real                     0                   real                          0\n","frames_cropped_faces_5src    F3Net       5_12       fake        20                 0              20            0.00       0.360381       0.040624              real                     0                   real                          0\n","frames_cropped_faces_5src    F3Net       5_13       fake        20                 0              20            0.00       0.414254       0.025155              real                     0                   real                          0\n","frames_cropped_faces_5src    F3Net       5_14       fake        20                 0              20            0.00       0.392708       0.017254              real                     0                   real                          0\n","frames_cropped_faces_5src    F3Net       5_15       fake        20                 0              20            0.00       0.438060       0.019334              real                     0                   real                          0\n","frames_cropped_faces_5src    F3Net       5_16       fake        20                 8              12            0.40       0.484083       0.024129              fake                     1                   real                          0\n","frames_cropped_faces_5src    F3Net       5_17       fake        20                20               0            1.00       0.517359       0.015908              fake                     1                   fake                          1\n","frames_cropped_faces_5src    F3Net       5_18       fake        20                20               0            1.00       0.563414       0.036663              fake                     1                   fake                          1\n","frames_cropped_faces_5src    F3Net       5_19       fake        20                 0              20            0.00       0.413657       0.023626              real                     0                   real                          0\n","frames_cropped_faces_5src    F3Net        5_2       fake        20                20               0            1.00       0.640131       0.019549              fake                     1                   fake                          1\n","frames_cropped_faces_5src    F3Net       5_20       fake        20                 0              20            0.00       0.433287       0.021211              real                     0                   real                          0\n","frames_cropped_faces_5src    F3Net       5_21       fake        20                 0              20            0.00       0.418447       0.033239              real                     0                   real                          0\n","frames_cropped_faces_5src    F3Net       5_22       fake        20                11               9            0.55       0.485457       0.052424              fake                     1                   fake                          1\n","frames_cropped_faces_5src    F3Net       5_23       fake        20                15               5            0.75       0.506732       0.030428              fake                     1                   fake                          1\n","frames_cropped_faces_5src    F3Net       5_24       fake        20                20               0            1.00       0.577185       0.054882              fake                     1                   fake                          1\n","frames_cropped_faces_5src    F3Net       5_25       fake        20                 5              15            0.25       0.471328       0.028278              real                     0                   real                          0\n","frames_cropped_faces_5src    F3Net       5_26       fake        20                12               8            0.60       0.500900       0.037407              fake                     1                   fake                          1\n","frames_cropped_faces_5src    F3Net       5_27       fake        20                 8              12            0.40       0.488333       0.021631              fake                     1                   real                          0\n","frames_cropped_faces_5src    F3Net       5_28       fake        20                14               6            0.70       0.501427       0.035961              fake                     1                   fake                          1\n","frames_cropped_faces_5src    F3Net       5_29       fake        20                11               9            0.55       0.495778       0.042009              fake                     1                   fake                          1\n","frames_cropped_faces_5src    F3Net        5_3       fake        20                 0              20            0.00       0.435391       0.021609              real                     0                   real                          0\n","frames_cropped_faces_5src    F3Net       5_30       fake        20                19               1            0.95       0.579482       0.057508              fake                     1                   fake                          1\n","frames_cropped_faces_5src    F3Net       5_31       fake        20                 1              19            0.05       0.436961       0.043912              real                     0                   real                          0\n","frames_cropped_faces_5src    F3Net       5_32       fake        20                 0              20            0.00       0.409314       0.018238              real                     0                   real                          0\n","frames_cropped_faces_5src    F3Net       5_33       fake        20                 3              17            0.15       0.457076       0.035698              real                     0                   real                          0\n","frames_cropped_faces_5src    F3Net       5_34       fake        20                 0              20            0.00       0.440056       0.028007              real                     0                   real                          0\n","frames_cropped_faces_5src    F3Net       5_35       fake        20                19               1            0.95       0.522968       0.028427              fake                     1                   fake                          1\n","frames_cropped_faces_5src    F3Net       5_36       fake        20                 9              11            0.45       0.484754       0.041224              fake                     1                   real                          0\n","frames_cropped_faces_5src    F3Net       5_37       fake        20                 9              11            0.45       0.489797       0.028423              fake                     1                   real                          0\n","frames_cropped_faces_5src    F3Net       5_38       fake        20                 4              16            0.20       0.440160       0.045998              real                     0                   real                          0\n","frames_cropped_faces_5src    F3Net       5_39       fake        20                 0              20            0.00       0.414441       0.023079              real                     0                   real                          0\n","frames_cropped_faces_5src    F3Net        5_4       fake        20                16               4            0.80       0.514638       0.030285              fake                     1                   fake                          1\n","frames_cropped_faces_5src    F3Net       5_40       fake        20                17               3            0.85       0.570269       0.075458              fake                     1                   fake                          1\n","frames_cropped_faces_5src    F3Net       5_41       fake        20                 0              20            0.00       0.450792       0.013412              real                     0                   real                          0\n","frames_cropped_faces_5src    F3Net       5_42       fake        20                 0              20            0.00       0.412218       0.020863              real                     0                   real                          0\n","frames_cropped_faces_5src    F3Net       5_43       fake        20                15               5            0.75       0.520048       0.027499              fake                     1                   fake                          1\n","frames_cropped_faces_5src    F3Net       5_44       fake        20                 0              20            0.00       0.425729       0.015599              real                     0                   real                          0\n","frames_cropped_faces_5src    F3Net       5_45       fake        20                20               0            1.00       0.574615       0.033198              fake                     1                   fake                          1\n","frames_cropped_faces_5src    F3Net       5_46       fake        20                 0              20            0.00       0.404965       0.022104              real                     0                   real                          0\n","frames_cropped_faces_5src    F3Net       5_47       fake        20                 0              20            0.00       0.402786       0.025958              real                     0                   real                          0\n","frames_cropped_faces_5src    F3Net       5_48       fake        20                18               2            0.90       0.527798       0.026804              fake                     1                   fake                          1\n","frames_cropped_faces_5src    F3Net       5_49       fake        20                 6              14            0.30       0.456594       0.046195              real                     0                   real                          0\n","frames_cropped_faces_5src    F3Net        5_5       fake        20                20               0            1.00       0.610650       0.033664              fake                     1                   fake                          1\n","frames_cropped_faces_5src    F3Net       5_50       fake        20                20               0            1.00       0.541310       0.023850              fake                     1                   fake                          1\n","frames_cropped_faces_5src    F3Net        5_6       fake        20                20               0            1.00       0.629463       0.021338              fake                     1                   fake                          1\n","frames_cropped_faces_5src    F3Net        5_7       fake        20                14               6            0.70       0.492810       0.020652              fake                     1                   fake                          1\n","frames_cropped_faces_5src    F3Net        5_8       fake        20                 6              14            0.30       0.474840       0.032246              real                     0                   real                          0\n","frames_cropped_faces_5src    F3Net        5_9       fake        20                 0              20            0.00       0.437600       0.017759              real                     0                   real                          0\n","frames_cropped_faces_5src    F3Net        Ali       real        20                20               0            1.00       0.397267       0.018605              real                     1                   real                          1\n","frames_cropped_faces_5src    F3Net  Elizebeth       real        20                20               0            1.00       0.375802       0.014960              real                     1                   real                          1\n","frames_cropped_faces_5src    F3Net     Ganesh       real        20                19               1            0.95       0.437328       0.028052              real                     1                   real                          1\n","frames_cropped_faces_5src    F3Net        aji       real        20                20               0            1.00       0.451965       0.016282              real                     1                   real                          1\n","frames_cropped_faces_5src    F3Net      akbar       real        20                18               2            0.90       0.449736       0.034198              real                     1                   real                          1\n","frames_cropped_faces_5src    F3Net      akhil       real        20                10              10            0.50       0.492134       0.019756              fake                     0                   fake                          0\n","frames_cropped_faces_5src    F3Net     akshay       real        20                17               3            0.85       0.456701       0.028937              real                     1                   real                          1\n","frames_cropped_faces_5src    F3Net       alib       real        20                19               1            0.95       0.416328       0.033866              real                     1                   real                          1\n","frames_cropped_faces_5src    F3Net      ameen       real        20                11               9            0.55       0.491933       0.018708              fake                     0                   real                          1\n","frames_cropped_faces_5src    F3Net       ammu       real        20                 6              14            0.30       0.520296       0.049299              fake                     0                   fake                          0\n","frames_cropped_faces_5src    F3Net     anandu       real        20                20               0            1.00       0.423098       0.030617              real                     1                   real                          1\n","frames_cropped_faces_5src    F3Net      anish       real        20                15               5            0.75       0.460558       0.035355              real                     1                   real                          1\n","frames_cropped_faces_5src    F3Net       ansu       real        20                20               0            1.00       0.423067       0.023056              real                     1                   real                          1\n","frames_cropped_faces_5src    F3Net     arnold       real        20                16               4            0.80       0.466227       0.029396              real                     1                   real                          1\n","frames_cropped_faces_5src    F3Net     assif        real        20                17               3            0.85       0.438903       0.049921              real                     1                   real                          1\n","frames_cropped_faces_5src    F3Net    baptist       real        20                20               0            1.00       0.398372       0.022055              real                     1                   real                          1\n","frames_cropped_faces_5src    F3Net   binisha        real        20                 8              12            0.40       0.511431       0.038384              fake                     0                   fake                          0\n","frames_cropped_faces_5src    F3Net      chris       real        20                20               0            1.00       0.422009       0.015315              real                     1                   real                          1\n","frames_cropped_faces_5src    F3Net  christian       real        20                20               0            1.00       0.426504       0.047801              real                     1                   real                          1\n","frames_cropped_faces_5src    F3Net        col       real        20                19               1            0.95       0.448594       0.030436              real                     1                   real                          1\n","frames_cropped_faces_5src    F3Net    darwish       real        20                20               0            1.00       0.441738       0.029950              real                     1                   real                          1\n","frames_cropped_faces_5src    F3Net      deeps       real        20                18               2            0.90       0.453060       0.025713              real                     1                   real                          1\n","frames_cropped_faces_5src    F3Net      denna       real        20                 0              20            0.00       0.579809       0.028338              fake                     0                   fake                          0\n","frames_cropped_faces_5src    F3Net    fathima       real        20                 4              16            0.20       0.541818       0.053649              fake                     0                   fake                          0\n","frames_cropped_faces_5src    F3Net     jelvin       real        20                16               4            0.80       0.457893       0.035973              real                     1                   real                          1\n","frames_cropped_faces_5src    F3Net   jennifer       real        20                 0              20            0.00       0.626424       0.026444              fake                     0                   fake                          0\n","frames_cropped_faces_5src    F3Net      jissa       real        20                20               0            1.00       0.405421       0.020552              real                     1                   real                          1\n","frames_cropped_faces_5src    F3Net      kevin       real        20                20               0            1.00       0.399603       0.023366              real                     1                   real                          1\n","frames_cropped_faces_5src    F3Net       lena       real        20                20               0            1.00       0.389789       0.024381              real                     1                   real                          1\n","frames_cropped_faces_5src    F3Net       liya       real        20                20               0            1.00       0.381194       0.020701              real                     1                   real                          1\n","frames_cropped_faces_5src    F3Net    liyamom       real        20                20               0            1.00       0.436815       0.028071              real                     1                   real                          1\n","frames_cropped_faces_5src    F3Net       malu       real        20                 4              16            0.20       0.544227       0.054366              fake                     0                   fake                          0\n","frames_cropped_faces_5src    F3Net      nevin       real        20                13               7            0.65       0.458295       0.048885              real                     1                   real                          1\n","frames_cropped_faces_5src    F3Net   niranjan       real        20                15               5            0.75       0.474756       0.023513              real                     1                   real                          1\n","frames_cropped_faces_5src    F3Net    praveen       real        20                16               4            0.80       0.467278       0.032086              real                     1                   real                          1\n","frames_cropped_faces_5src    F3Net    pushpan       real        20                 3              17            0.15       0.520576       0.025767              fake                     0                   fake                          0\n","frames_cropped_faces_5src    F3Net      rahul       real        20                 2              18            0.10       0.543344       0.056895              fake                     0                   fake                          0\n","frames_cropped_faces_5src    F3Net       raju       real        20                20               0            1.00       0.410839       0.023744              real                     1                   real                          1\n","frames_cropped_faces_5src    F3Net      rasee       real        20                 9              11            0.45       0.489543       0.029843              fake                     0                   fake                          0\n","frames_cropped_faces_5src    F3Net     roshan       real        20                20               0            1.00       0.433685       0.022708              real                     1                   real                          1\n","frames_cropped_faces_5src    F3Net     sachin       real        20                20               0            1.00       0.395353       0.033105              real                     1                   real                          1\n","frames_cropped_faces_5src    F3Net      salim       real        20                20               0            1.00       0.425997       0.022952              real                     1                   real                          1\n","frames_cropped_faces_5src    F3Net     seethu       real        20                10              10            0.50       0.485116       0.022963              fake                     0                   fake                          0\n","frames_cropped_faces_5src    F3Net     shanty       real        20                20               0            1.00       0.400028       0.027873              real                     1                   real                          1\n","frames_cropped_faces_5src    F3Net       subu       real        20                20               0            1.00       0.383819       0.014970              real                     1                   real                          1\n","frames_cropped_faces_5src    F3Net      teggy       real        20                 5              15            0.25       0.562883       0.082264              fake                     0                   fake                          0\n","frames_cropped_faces_5src    F3Net     thomas       real        20                20               0            1.00       0.331999       0.048137              real                     1                   real                          1\n","frames_cropped_faces_5src    F3Net      umesh       real        20                16               4            0.80       0.477501       0.030952              real                     1                   real                          1\n","frames_cropped_faces_5src    F3Net        yad       real        20                20               0            1.00       0.386495       0.021260              real                     1                   real                          1\n"]}]},{"cell_type":"code","source":["# Save LARGE table to: MyDrive/F3net results 5 src\n","import os\n","\n","SAVE_DIR = \"/content/drive/MyDrive/F3net results 5 src\"\n","os.makedirs(SAVE_DIR, exist_ok=True)\n","\n","OUT_CSV = os.path.join(SAVE_DIR, \"F3Net large table 5src.csv\")\n","df.to_csv(OUT_CSV, index=False)\n","\n","print(f\"Saved to: {OUT_CSV}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pGDTX8tM75Ff","executionInfo":{"status":"ok","timestamp":1761338171637,"user_tz":-120,"elapsed":97,"user":{"displayName":"Vishnumaya","userId":"01919615312035119785"}},"outputId":"4af76f15-f44c-462e-f385-028b0b8b3a9e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Saved to: /content/drive/MyDrive/F3net results 5 src/F3Net large table 5src.csv\n"]}]},{"cell_type":"code","source":["# === SMALL TABLE from existing large table `df` ===\n","# Columns: dataset, detector, video_name, true_label, correctly_predicted (yes/no)\n","import os\n","import pandas as pd\n","\n","# Confirm required columns exist\n","required_cols = {\n","    \"dataset\",\"detector\",\"video_name\",\"true_label\",\"video_correct_by_majority\"\n","}\n","if not required_cols.issubset(set(df.columns)):\n","    raise RuntimeError(\"Large table `df` missing required columns. Re-run the large-table cell first.\")\n","\n","small_df = df[[\"dataset\",\"detector\",\"video_name\",\"true_label\",\"video_correct_by_majority\"]].copy()\n","small_df[\"correctly_predicted\"] = small_df[\"video_correct_by_majority\"].map({1: \"yes\", 0: \"no\"})\n","small_df = small_df.drop(columns=[\"video_correct_by_majority\"])\n","\n","# Display full small table (no truncation)\n","pd.set_option(\"display.max_rows\", 50000)\n","pd.set_option(\"display.max_columns\", None)\n","pd.set_option(\"display.width\", 0)\n","print(small_df.to_string(index=False))\n","\n","# Save to your folder\n","SAVE_DIR = \"/content/drive/MyDrive/F3net results 5 src\"\n","os.makedirs(SAVE_DIR, exist_ok=True)\n","OUT_CSV = os.path.join(SAVE_DIR, \"F3Net small table 5src.csv\")\n","small_df.to_csv(OUT_CSV, index=False)\n","print(f\"\\nSaved to: {OUT_CSV}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l6-Gwe8H8rtd","executionInfo":{"status":"ok","timestamp":1761338380113,"user_tz":-120,"elapsed":51,"user":{"displayName":"Vishnumaya","userId":"01919615312035119785"}},"outputId":"7115d349-212a-42c6-c9c1-b8f89eeaa8cc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["                  dataset detector video_name true_label correctly_predicted\n","frames_cropped_faces_5src    F3Net        5_1       fake                  no\n","frames_cropped_faces_5src    F3Net       5_10       fake                  no\n","frames_cropped_faces_5src    F3Net       5_11       fake                  no\n","frames_cropped_faces_5src    F3Net       5_12       fake                  no\n","frames_cropped_faces_5src    F3Net       5_13       fake                  no\n","frames_cropped_faces_5src    F3Net       5_14       fake                  no\n","frames_cropped_faces_5src    F3Net       5_15       fake                  no\n","frames_cropped_faces_5src    F3Net       5_16       fake                  no\n","frames_cropped_faces_5src    F3Net       5_17       fake                 yes\n","frames_cropped_faces_5src    F3Net       5_18       fake                 yes\n","frames_cropped_faces_5src    F3Net       5_19       fake                  no\n","frames_cropped_faces_5src    F3Net        5_2       fake                 yes\n","frames_cropped_faces_5src    F3Net       5_20       fake                  no\n","frames_cropped_faces_5src    F3Net       5_21       fake                  no\n","frames_cropped_faces_5src    F3Net       5_22       fake                 yes\n","frames_cropped_faces_5src    F3Net       5_23       fake                 yes\n","frames_cropped_faces_5src    F3Net       5_24       fake                 yes\n","frames_cropped_faces_5src    F3Net       5_25       fake                  no\n","frames_cropped_faces_5src    F3Net       5_26       fake                 yes\n","frames_cropped_faces_5src    F3Net       5_27       fake                  no\n","frames_cropped_faces_5src    F3Net       5_28       fake                 yes\n","frames_cropped_faces_5src    F3Net       5_29       fake                 yes\n","frames_cropped_faces_5src    F3Net        5_3       fake                  no\n","frames_cropped_faces_5src    F3Net       5_30       fake                 yes\n","frames_cropped_faces_5src    F3Net       5_31       fake                  no\n","frames_cropped_faces_5src    F3Net       5_32       fake                  no\n","frames_cropped_faces_5src    F3Net       5_33       fake                  no\n","frames_cropped_faces_5src    F3Net       5_34       fake                  no\n","frames_cropped_faces_5src    F3Net       5_35       fake                 yes\n","frames_cropped_faces_5src    F3Net       5_36       fake                  no\n","frames_cropped_faces_5src    F3Net       5_37       fake                  no\n","frames_cropped_faces_5src    F3Net       5_38       fake                  no\n","frames_cropped_faces_5src    F3Net       5_39       fake                  no\n","frames_cropped_faces_5src    F3Net        5_4       fake                 yes\n","frames_cropped_faces_5src    F3Net       5_40       fake                 yes\n","frames_cropped_faces_5src    F3Net       5_41       fake                  no\n","frames_cropped_faces_5src    F3Net       5_42       fake                  no\n","frames_cropped_faces_5src    F3Net       5_43       fake                 yes\n","frames_cropped_faces_5src    F3Net       5_44       fake                  no\n","frames_cropped_faces_5src    F3Net       5_45       fake                 yes\n","frames_cropped_faces_5src    F3Net       5_46       fake                  no\n","frames_cropped_faces_5src    F3Net       5_47       fake                  no\n","frames_cropped_faces_5src    F3Net       5_48       fake                 yes\n","frames_cropped_faces_5src    F3Net       5_49       fake                  no\n","frames_cropped_faces_5src    F3Net        5_5       fake                 yes\n","frames_cropped_faces_5src    F3Net       5_50       fake                 yes\n","frames_cropped_faces_5src    F3Net        5_6       fake                 yes\n","frames_cropped_faces_5src    F3Net        5_7       fake                 yes\n","frames_cropped_faces_5src    F3Net        5_8       fake                  no\n","frames_cropped_faces_5src    F3Net        5_9       fake                  no\n","frames_cropped_faces_5src    F3Net        Ali       real                 yes\n","frames_cropped_faces_5src    F3Net  Elizebeth       real                 yes\n","frames_cropped_faces_5src    F3Net     Ganesh       real                 yes\n","frames_cropped_faces_5src    F3Net        aji       real                 yes\n","frames_cropped_faces_5src    F3Net      akbar       real                 yes\n","frames_cropped_faces_5src    F3Net      akhil       real                  no\n","frames_cropped_faces_5src    F3Net     akshay       real                 yes\n","frames_cropped_faces_5src    F3Net       alib       real                 yes\n","frames_cropped_faces_5src    F3Net      ameen       real                 yes\n","frames_cropped_faces_5src    F3Net       ammu       real                  no\n","frames_cropped_faces_5src    F3Net     anandu       real                 yes\n","frames_cropped_faces_5src    F3Net      anish       real                 yes\n","frames_cropped_faces_5src    F3Net       ansu       real                 yes\n","frames_cropped_faces_5src    F3Net     arnold       real                 yes\n","frames_cropped_faces_5src    F3Net     assif        real                 yes\n","frames_cropped_faces_5src    F3Net    baptist       real                 yes\n","frames_cropped_faces_5src    F3Net   binisha        real                  no\n","frames_cropped_faces_5src    F3Net      chris       real                 yes\n","frames_cropped_faces_5src    F3Net  christian       real                 yes\n","frames_cropped_faces_5src    F3Net        col       real                 yes\n","frames_cropped_faces_5src    F3Net    darwish       real                 yes\n","frames_cropped_faces_5src    F3Net      deeps       real                 yes\n","frames_cropped_faces_5src    F3Net      denna       real                  no\n","frames_cropped_faces_5src    F3Net    fathima       real                  no\n","frames_cropped_faces_5src    F3Net     jelvin       real                 yes\n","frames_cropped_faces_5src    F3Net   jennifer       real                  no\n","frames_cropped_faces_5src    F3Net      jissa       real                 yes\n","frames_cropped_faces_5src    F3Net      kevin       real                 yes\n","frames_cropped_faces_5src    F3Net       lena       real                 yes\n","frames_cropped_faces_5src    F3Net       liya       real                 yes\n","frames_cropped_faces_5src    F3Net    liyamom       real                 yes\n","frames_cropped_faces_5src    F3Net       malu       real                  no\n","frames_cropped_faces_5src    F3Net      nevin       real                 yes\n","frames_cropped_faces_5src    F3Net   niranjan       real                 yes\n","frames_cropped_faces_5src    F3Net    praveen       real                 yes\n","frames_cropped_faces_5src    F3Net    pushpan       real                  no\n","frames_cropped_faces_5src    F3Net      rahul       real                  no\n","frames_cropped_faces_5src    F3Net       raju       real                 yes\n","frames_cropped_faces_5src    F3Net      rasee       real                  no\n","frames_cropped_faces_5src    F3Net     roshan       real                 yes\n","frames_cropped_faces_5src    F3Net     sachin       real                 yes\n","frames_cropped_faces_5src    F3Net      salim       real                 yes\n","frames_cropped_faces_5src    F3Net     seethu       real                  no\n","frames_cropped_faces_5src    F3Net     shanty       real                 yes\n","frames_cropped_faces_5src    F3Net       subu       real                 yes\n","frames_cropped_faces_5src    F3Net      teggy       real                  no\n","frames_cropped_faces_5src    F3Net     thomas       real                 yes\n","frames_cropped_faces_5src    F3Net      umesh       real                 yes\n","frames_cropped_faces_5src    F3Net        yad       real                 yes\n","\n","Saved to: /content/drive/MyDrive/F3net results 5 src/F3Net small table 5src.csv\n"]}]}]}